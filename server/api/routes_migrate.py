# api/routes_migrate.py

import logging
import uuid
import re
import os
import time
import queue
import threading
import copy
import requests
import urllib.parse
import json
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from flask import Blueprint, jsonify, request, Response, stream_with_context
from bs4 import BeautifulSoup
from utils import (
    upload_data_title,
    upload_data_screenshot,
    upload_data_movie_info,
    add_torrent_to_downloader,
    extract_tags_from_mediainfo,
    extract_origin_from_description,
    extract_resolution_from_mediainfo,
)
from utils.downloader_selector import select_best_downloader
from core.migrator import TorrentMigrator

# å¯¼å…¥ç§å­å‚æ•°æ¨¡å‹
from models.seed_parameter import SeedParameter

# --- [æ–°å¢] å¯¼å…¥ config_manager ---
# ç¡®ä¿èƒ½å¤Ÿè®¿é—®åˆ°å…¨å±€çš„ config_manager å®ä¾‹
from config import config_manager, GLOBAL_MAPPINGS

# --- [æ–°å¢] å¯¼å…¥æ—¥å¿—æµç®¡ç†å™¨ ---
from utils import log_streamer

# --- [æ–°å¢] å¯¼å…¥SSEç®¡ç†å™¨ ---
from utils.sse_manager import sse_manager

migrate_bp = Blueprint("migrate_api", __name__, url_prefix="/api")

MIGRATION_CACHE = {}
MIGRATION_CACHE_LOCK = threading.Lock()
MIGRATION_TORRENT_FILE_LOCKS = {}

INACTIVE_TORRENT_STATES = ("æœªåšç§", "å·²æš‚åœ", "å·²åœæ­¢", "é”™è¯¯", "ç­‰å¾…", "é˜Ÿåˆ—")


def get_seed_hash(db_manager, torrent_id, site_name):
    """æ ¹æ®torrent_id/site_nameè·å–hash"""
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)
        ph = db_manager.get_placeholder()
        query = (
            f"SELECT hash FROM seed_parameters WHERE torrent_id = {ph} AND site_name = {ph} "
            f"ORDER BY updated_at DESC LIMIT 1"
        )
        cursor.execute(query, (torrent_id, site_name))
        row = cursor.fetchone()
        cursor.close()
        conn.close()
        if not row:
            return None
        return row["hash"] if isinstance(row, dict) else row[0]
    except Exception as e:
        logging.warning(f"è·å–hashå¤±è´¥: {e}")
        return None


def get_seed_name(db_manager, torrent_id, site_name):
    """æ ¹æ®torrent_id/site_nameè·å–name"""
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)
        ph = db_manager.get_placeholder()
        query = (
            f"SELECT name FROM seed_parameters WHERE torrent_id = {ph} AND site_name = {ph} "
            f"ORDER BY updated_at DESC LIMIT 1"
        )
        cursor.execute(query, (torrent_id, site_name))
        row = cursor.fetchone()
        cursor.close()
        conn.close()
        if not row:
            return None
        return row["name"] if isinstance(row, dict) else row[0]
    except Exception as e:
        logging.warning(f"è·å–nameå¤±è´¥: {e}")
        return None


def get_current_torrent_info(db_manager, torrent_name):
    """æ ¹æ®ç§å­åç§°è·å–å½“å‰ç§å­çš„ä¿å­˜è·¯å¾„/ä¸‹è½½å™¨IDï¼ˆä¼˜å…ˆæ´»è·ƒçŠ¶æ€ï¼Œä¼˜å…ˆuse_proxy=trueï¼‰"""
    if not torrent_name:
        return None

    try:

        def _to_timestamp(value) -> float:
            if not value:
                return 0
            try:
                ts = getattr(value, "timestamp", None)
                if callable(ts):
                    return float(ts())
            except Exception:
                pass
            if isinstance(value, (int, float)):
                return float(value)
            if isinstance(value, str):
                text = value.strip()
                if not text:
                    return 0
                try:
                    return float(text)
                except ValueError:
                    pass
                # sqlite å¸¸è§ï¼š'YYYY-MM-DD HH:MM:SS' æˆ– ISO8601ï¼ˆå¯èƒ½å¸¦ Zï¼‰
                normalized = text.replace("Z", "+00:00")
                for parser in (datetime.fromisoformat,):
                    try:
                        return float(parser(normalized).timestamp())
                    except Exception:
                        pass
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M:%S.%f"):
                    try:
                        return float(datetime.strptime(text, fmt).timestamp())
                    except Exception:
                        pass
            return 0

        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)
        ph = db_manager.get_placeholder()

        # æŸ¥è¯¢æ‰€æœ‰ç›¸åŒåç§°çš„ç§å­è®°å½•
        query = f"""
            SELECT save_path, downloader_id, name, state, last_seen
            FROM torrents
            WHERE name = {ph}
        """
        cursor.execute(query, (torrent_name,))
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        if not rows:
            return None

        # å°†ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨
        torrent_list = []
        for row in rows:
            if isinstance(row, dict):
                last_seen = row.get("last_seen")
                torrent_list.append(
                    {
                        "save_path": row.get("save_path"),
                        "downloader_id": row.get("downloader_id"),
                        "name": row.get("name"),
                        "state": row.get("state"),
                        "last_seen": _to_timestamp(last_seen),
                    }
                )
            else:
                last_seen = row[4]
                torrent_list.append(
                    {
                        "save_path": row[0],
                        "downloader_id": row[1],
                        "name": row[2],
                        "state": row[3],
                        "last_seen": _to_timestamp(last_seen),
                    }
                )

        # è·å–æ‰€æœ‰ä¸‹è½½å™¨ID
        downloader_ids = list(
            set(t.get("downloader_id") for t in torrent_list if t.get("downloader_id"))
        )

        # ä½¿ç”¨å·¥å…·å‡½æ•°é€‰æ‹©æœ€ä½³ä¸‹è½½å™¨
        best_downloader_id = select_best_downloader(
            downloader_ids=downloader_ids,
            config_manager=config_manager,
            torrent_list=torrent_list,
            inactive_torrent_states=INACTIVE_TORRENT_STATES,
        )

        # æ‰¾åˆ°ä½¿ç”¨æœ€ä½³ä¸‹è½½å™¨çš„ç§å­è®°å½•
        first_torrent = next(
            (t for t in torrent_list if t.get("downloader_id") == best_downloader_id),
            torrent_list[0],
        )

        return {
            "save_path": first_torrent.get("save_path"),
            "downloader_id": first_torrent.get("downloader_id"),
            "name": first_torrent.get("name"),
        }
    except Exception as e:
        logging.warning(f"è·å–å½“å‰ç§å­ä¿¡æ¯å¤±è´¥: {e}")
        print(f"[get_current_torrent_info] å¼‚å¸¸: {e}")
        return None


# ===================================================================
#                          è½¬ç§è®¾ç½® API (æ–°æ•´åˆ)
# ===================================================================


@migrate_bp.route("/settings/cross_seed", methods=["GET"])
def get_cross_seed_settings():
    """è·å–è½¬ç§ç›¸å…³çš„è®¾ç½®ã€‚"""
    try:
        config = config_manager.get()
        # ä½¿ç”¨ .get() æä¾›é»˜è®¤å€¼ï¼Œé˜²æ­¢é…ç½®æ–‡ä»¶æŸåæ—¶å‡ºé”™
        cross_seed_config = config.get("cross_seed", {}) or {}
        cross_seed_config.setdefault("image_hoster", "pixhost")
        cross_seed_config.setdefault("default_downloader", "")
        cross_seed_config.setdefault("publish_batch_concurrency_mode", "cpu")
        cross_seed_config.setdefault("publish_batch_concurrency_manual", 5)
        return jsonify(cross_seed_config)
    except Exception as e:
        logging.error(f"è·å–è½¬ç§è®¾ç½®å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


@migrate_bp.route("/settings/cross_seed", methods=["POST"])
def save_cross_seed_settings():
    """ä¿å­˜è½¬ç§ç›¸å…³çš„è®¾ç½®ã€‚"""
    try:
        new_settings = request.json
        if not isinstance(new_settings, dict):
            return jsonify({"error": "æ— æ•ˆçš„è®¾ç½®æ•°æ®æ ¼å¼ã€‚"}), 400

        current_config = config_manager.get()
        existing_settings = current_config.get("cross_seed", {}) or {}

        # åˆå¹¶æ›´æ–°ï¼Œé¿å…å‰ç«¯åªæäº¤éƒ¨åˆ†å­—æ®µæ—¶è¦†ç›–æ‰å…¶ä»– cross_seed é…ç½®
        merged_settings = existing_settings.copy()
        merged_settings.update(new_settings)

        if not merged_settings.get("image_hoster"):
            merged_settings["image_hoster"] = existing_settings.get("image_hoster") or "pixhost"

        # è§„èŒƒåŒ–å¹¶å‘é…ç½®å­—æ®µï¼ˆä»…åšåŸºæœ¬æ ¡éªŒï¼Œæœ€ç»ˆå¹¶å‘ä»ä¼šåœ¨ä»»åŠ¡å¯åŠ¨æ—¶æŒ‰ä¸Šé™è£å‰ªï¼‰
        mode = merged_settings.get("publish_batch_concurrency_mode", "cpu")
        if mode not in ("cpu", "manual", "all"):
            mode = "cpu"
        merged_settings["publish_batch_concurrency_mode"] = mode

        manual_value = merged_settings.get("publish_batch_concurrency_manual", 5)
        try:
            manual_value = int(manual_value)
        except Exception:
            manual_value = 5
        merged_settings["publish_batch_concurrency_manual"] = max(1, manual_value)

        # æ›´æ–°é…ç½®ä¸­çš„ cross_seed éƒ¨åˆ†
        current_config["cross_seed"] = merged_settings

        if config_manager.save(current_config):
            return jsonify({"message": "è½¬ç§è®¾ç½®å·²æˆåŠŸä¿å­˜ï¼"})
        else:
            return jsonify({"error": "æ— æ³•å°†è®¾ç½®å†™å…¥é…ç½®æ–‡ä»¶ã€‚"}), 500

    except Exception as e:
        logging.error(f"ä¿å­˜è½¬ç§è®¾ç½®å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


@migrate_bp.route("/settings/cross_seed/publish_concurrency_info", methods=["GET"])
def get_publish_concurrency_info():
    """è·å–æœåŠ¡å™¨ CPU çº¿ç¨‹æ•°åŠæ¨èå¹¶å‘ï¼Œç”¨äºå‰ç«¯å±•ç¤ºå¹¶å‘ç­–ç•¥ã€‚"""
    try:
        cpu_threads = os.cpu_count() or 0
        cpu_threads = int(cpu_threads) if cpu_threads else 1
        suggested = cpu_threads * 2

        # ä¸æ‰¹é‡å‘å¸ƒæ¥å£çš„å¹¶å‘ä¸Šé™ä¿æŒä¸€è‡´ï¼ˆå‰ç«¯å±•ç¤ºç”¨ï¼‰
        max_concurrency = BATCH_PUBLISH_MAX_CONCURRENCY
        effective_suggested = max(1, min(max_concurrency, suggested))

        return jsonify(
            {
                "success": True,
                "cpu_threads": cpu_threads,
                "suggested_concurrency": suggested,
                "effective_suggested_concurrency": effective_suggested,
                "max_concurrency": max_concurrency,
                "default_concurrency": BATCH_PUBLISH_DEFAULT_CONCURRENCY,
            }
        )
    except Exception as e:
        logging.error(f"è·å–å¹¶å‘ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500


# ===================================================================
#                         åŸæœ‰è¿ç§» API
# ===================================================================


# æ–°å¢ï¼šä»æ•°æ®åº“è¯»å–ç§å­ä¿¡æ¯çš„APIæ¥å£
@migrate_bp.route("/migrate/get_db_seed_info", methods=["GET"])
def get_db_seed_info():
    """ä»æ•°æ®åº“è¯»å–ç§å­ä¿¡æ¯ç”¨äºå±•ç¤º"""
    try:
        torrent_id = request.args.get("torrent_id")
        site_name = request.args.get("site_name")
        task_id = request.args.get("task_id")  # æ¥æ”¶å‰ç«¯ä¼ æ¥çš„task_id

        if not torrent_id or not site_name:
            return (
                jsonify({"success": False, "message": "é”™è¯¯ï¼štorrent_idå’Œsite_nameå‚æ•°ä¸èƒ½ä¸ºç©º"}),
                400,
            )

        # å¦‚æœå‰ç«¯æä¾›äº†task_idï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ç”Ÿæˆæ–°çš„
        if task_id:
            log_streamer.create_stream(task_id)
            log_streamer.emit_log(
                task_id, "æ•°æ®åº“æŸ¥è¯¢", "æ­£åœ¨ä»æ•°æ®åº“è¯»å–ç§å­ä¿¡æ¯...", "processing"
            )

        db_manager = migrate_bp.db_manager

        # ä»æ•°æ®åº“è¯»å–
        try:
            # åˆå§‹åŒ–ç§å­å‚æ•°æ¨¡å‹
            from models.seed_parameter import SeedParameter

            seed_param_model = SeedParameter(db_manager)

            parameters = seed_param_model.get_parameters(torrent_id, site_name)

            if parameters:
                logging.info(f"æˆåŠŸä»æ•°æ®åº“è¯»å–ç§å­ä¿¡æ¯: {torrent_id} from {site_name}")

                # ä»torrentsè¡¥å……å½“å‰ä¿å­˜è·¯å¾„/ä¸‹è½½å™¨ID
                torrent_name = parameters.get("name")
                torrent_info = get_current_torrent_info(db_manager, torrent_name)
                if torrent_info:
                    parameters["save_path"] = torrent_info.get("save_path") or ""
                    parameters["downloader_id"] = torrent_info.get("downloader_id")

                # ç”Ÿæˆåå‘æ˜ å°„è¡¨ï¼ˆä»æ ‡å‡†é”®åˆ°ä¸­æ–‡æ˜¾ç¤ºåç§°çš„æ˜ å°„ï¼‰
                reverse_mappings = generate_reverse_mappings()

                # ç”Ÿæˆtask_idå¹¶å­˜å…¥ç¼“å­˜ï¼Œä»¥ä¾¿å‘å¸ƒæ—¶ä½¿ç”¨
                cache_task_id = str(uuid.uuid4())

                # è·å–ç«™ç‚¹ä¿¡æ¯
                source_info = db_manager.get_site_by_nickname(site_name)
                if not source_info:
                    # å¦‚æœé€šè¿‡æ˜µç§°æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡è‹±æ–‡ç«™ç‚¹åæŸ¥æ‰¾
                    try:
                        conn = db_manager._get_connection()
                        cursor = db_manager._get_cursor(conn)
                        cursor.execute("SELECT * FROM sites WHERE site = ?", (site_name,))
                        source_info = cursor.fetchone()
                        if source_info:
                            source_info = dict(source_info)
                    except Exception as e:
                        logging.warning(f"è·å–ç«™ç‚¹ä¿¡æ¯å¤±è´¥: {e}")

                # å°†æ•°æ®å­˜å…¥ç¼“å­˜ï¼Œä»¥ä¾¿å‘å¸ƒæ—¶ä½¿ç”¨
                with MIGRATION_CACHE_LOCK:
                    MIGRATION_CACHE[cache_task_id] = {
                        "source_info": source_info,
                        "original_torrent_path": None,  # å°†åœ¨å‘å¸ƒæ—¶é‡æ–°è·å–
                        "torrent_dir": None,  # å°†åœ¨å‘å¸ƒæ—¶é‡æ–°ç¡®å®š
                        "source_site_name": site_name,
                        "source_torrent_id": torrent_id,
                        "requires_torrent_download": True,  # éœ€è¦ä¸‹è½½ç§å­æ–‡ä»¶
                    }

                if task_id:
                    # æ ‡è®°æ•°æ®åº“æŸ¥è¯¢æ­¥éª¤å®Œæˆ
                    log_streamer.emit_log(task_id, "æ•°æ®åº“æŸ¥è¯¢", "æ•°æ®åº“è¯»å–å®Œæˆ", "success")
                    # å‘é€å®Œæˆæ­¥éª¤
                    log_streamer.emit_log(task_id, "å®Œæˆ", "æ•°æ®åŠ è½½å®Œæˆ", "success")
                    # å…³é—­æ—¥å¿—æµ
                    log_streamer.close_stream(task_id)

                return jsonify(
                    {
                        "success": True,
                        "data": parameters,
                        "source": "database",
                        "task_id": cache_task_id,  # è¿”å›cache_task_idç»™å‰ç«¯
                        "reverse_mappings": reverse_mappings,
                    }
                )
            else:
                logging.info(
                    f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç§å­ä¿¡æ¯: {torrent_id} from {site_name}ï¼Œå°†ä»æºç«™ç‚¹æŠ“å–"
                )

                # æ ‡è®°æ•°æ®åº“æŸ¥è¯¢ä¸ºå¤±è´¥ï¼Œå‡†å¤‡ä»æºç«™ç‚¹æŠ“å–
                if task_id:
                    log_streamer.emit_log(task_id, "æ•°æ®åº“æŸ¥è¯¢", "æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç¼“å­˜", "error")

                return (
                    jsonify(
                        {
                            "success": False,
                            "message": "æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç§å­ä¿¡æ¯",
                            "should_fetch": True,  # æ ‡è®°éœ€è¦ä»æºç«™ç‚¹æŠ“å–
                            "task_id": task_id,  # è¿”å›task_idä»¥ä¾¿å‰ç«¯ç»§ç»­ä½¿ç”¨åŒä¸€ä¸ªæ—¥å¿—æµ
                        }
                    ),
                    202,
                )  # ä½¿ç”¨202çŠ¶æ€ç è¡¨ç¤º"å·²æ¥å—ï¼Œä½†éœ€è¦ç»§ç»­å¤„ç†"

        except Exception as e:
            logging.error(f"ä»æ•°æ®åº“è¯»å–ç§å­ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            return jsonify({"success": False, "message": f"æ•°æ®åº“è¯»å–å¤±è´¥: {str(e)}"}), 500

    except Exception as e:
        logging.error(f"get_db_seed_infoå‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


def generate_reverse_mappings():
    """ç”Ÿæˆä»æ ‡å‡†é”®åˆ°ä¸­æ–‡æ˜¾ç¤ºåç§°çš„åå‘æ˜ å°„"""
    try:
        # è¯»å–å…¨å±€æ˜ å°„é…ç½®
        import yaml
        import os

        # é¦–å…ˆå°è¯•ä»global_mappings.yamlè¯»å–
        global_mappings = {}

        if os.path.exists(GLOBAL_MAPPINGS):
            try:
                with open(GLOBAL_MAPPINGS, "r", encoding="utf-8") as f:
                    config_data = yaml.safe_load(f)
                    global_mappings = config_data.get("global_standard_keys", {})
                logging.info(
                    f"æˆåŠŸä»global_mappings.yamlè¯»å–é…ç½®ï¼ŒåŒ…å«{len(global_mappings)}ä¸ªç±»åˆ«"
                )
            except Exception as e:
                logging.warning(f"è¯»å–global_mappings.yamlå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®")

        # å¦‚æœYAMLæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä»é…ç½®ç®¡ç†å™¨è·å–
        if not global_mappings:
            config = config_manager.get()
            global_mappings = config.get("global_standard_keys", {})

        reverse_mappings = {
            "type": {},
            "medium": {},
            "video_codec": {},
            "audio_codec": {},
            "resolution": {},
            "source": {},
            "team": {},
            "tags": {},
        }

        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆåå‘æ˜ å°„
        categories_mapping = {
            "type": global_mappings.get("type", {}),
            "medium": global_mappings.get("medium", {}),
            "video_codec": global_mappings.get("video_codec", {}),
            "audio_codec": global_mappings.get("audio_codec", {}),
            "resolution": global_mappings.get("resolution", {}),
            "source": global_mappings.get("source", {}),
            "team": global_mappings.get("team", {}),
            "tags": global_mappings.get("tag", {}),  # æ³¨æ„è¿™é‡ŒYAMLä¸­æ˜¯'tag'è€Œä¸æ˜¯'tags'
        }

        # åˆ›å»ºåå‘æ˜ å°„ï¼šä»æ ‡å‡†å€¼åˆ°ä¸­æ–‡åç§°
        for category, mappings in categories_mapping.items():
            if category == "tags":
                # æ ‡ç­¾ç‰¹æ®Šå¤„ç†ï¼Œæå–ä¸­æ–‡åä½œä¸ºé”®ï¼Œæ ‡å‡†å€¼ä½œä¸ºå€¼
                for chinese_name, standard_value in mappings.items():
                    if standard_value:  # è¿‡æ»¤æ‰nullå€¼
                        reverse_mappings["tags"][standard_value] = chinese_name
            else:
                # å…¶ä»–ç±»åˆ«æ­£å¸¸å¤„ç†
                for chinese_name, standard_value in mappings.items():
                    if standard_value and standard_value not in reverse_mappings[category]:
                        reverse_mappings[category][standard_value] = chinese_name

        # åªåœ¨å¿…è¦æ—¶æ·»åŠ å›ºå®šæ˜ å°„é¡¹ä½œä¸ºåå¤‡ï¼Œé¿å…è¦†ç›–YAMLé…ç½®
        add_fallback_mappings(reverse_mappings)

        logging.info(f"æˆåŠŸç”Ÿæˆåå‘æ˜ å°„è¡¨: { {k: len(v) for k, v in reverse_mappings.items()} }")
        return reverse_mappings

    except Exception as e:
        logging.error(f"ç”Ÿæˆåå‘æ˜ å°„è¡¨å¤±è´¥: {e}", exc_info=True)
        # è¿”å›ç©ºçš„åå‘æ˜ å°„è¡¨ä½œä¸ºåå¤‡
        return {
            "type": {},
            "medium": {},
            "video_codec": {},
            "audio_codec": {},
            "resolution": {},
            "source": {},
            "team": {},
            "tags": {},
        }


def add_fallback_mappings(reverse_mappings):
    """æ·»åŠ åå¤‡æ˜ å°„é¡¹ï¼Œä»…åœ¨YAMLé…ç½®ç¼ºå¤±æ—¶ä½¿ç”¨"""

    # æ£€æŸ¥å„ä¸ªç±»åˆ«æ˜¯å¦ä¸ºç©ºï¼Œå¦‚æœä¸ºç©ºåˆ™æ·»åŠ åŸºç¡€æ˜ å°„
    if not reverse_mappings["type"]:
        logging.warning("typeæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["type"].update(
            {
                "category.movie": "ç”µå½±",
                "category.tv_series": "å‰§é›†",
                "category.animation": "åŠ¨ç”»",
                "category.documentaries": "çºªå½•ç‰‡",
                "category.music": "éŸ³ä¹",
                "category.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["medium"]:
        logging.warning("mediumæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["medium"].update(
            {
                "medium.bluray": "Blu-ray",
                "medium.uhd_bluray": "UHD Blu-ray",
                "medium.remux": "Remux",
                "medium.encode": "Encode",
                "medium.webdl": "WEB-DL",
                "medium.webrip": "WebRip",
                "medium.hdtv": "HDTV",
                "medium.dvd": "DVD",
                "medium.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["video_codec"]:
        logging.warning("video_codecæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["video_codec"].update(
            {
                "video.h264": "H.264/AVC",
                "video.h265": "H.265/HEVC",
                "video.x265": "x265",
                "video.vc1": "VC-1",
                "video.mpeg2": "MPEG-2",
                "video.av1": "AV1",
                "video.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["audio_codec"]:
        logging.warning("audio_codecæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["audio_codec"].update(
            {
                "audio.flac": "FLAC",
                "audio.dts": "DTS",
                "audio.dts_hd_ma": "DTS-HD MA",
                "audio.dtsx": "DTS:X",
                "audio.truehd": "TrueHD",
                "audio.truehd_atmos": "TrueHD Atmos",
                "audio.ac3": "AC-3",
                "audio.ddp": "E-AC-3",
                "audio.aac": "AAC",
                "audio.mp3": "MP3",
                "audio.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["resolution"]:
        logging.warning("resolutionæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["resolution"].update(
            {
                "resolution.r8k": "8K",
                "resolution.r4k": "4K",
                "resolution.r2160p": "2160p",
                "resolution.r1080p": "1080p",
                "resolution.r1080i": "1080i",
                "resolution.r720p": "720p",
                "resolution.r480p": "480p",
                "resolution.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["source"]:
        logging.warning("sourceæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["source"].update(
            {
                "source.china": "ä¸­å›½",
                "source.hongkong": "é¦™æ¸¯",
                "source.taiwan": "å°æ¹¾",
                "source.western": "ç¾å›½",
                "source.uk": "è‹±å›½",
                "source.japan": "æ—¥æœ¬",
                "source.korea": "éŸ©å›½",
                "source.other": "å…¶ä»–",
            }
        )

    if not reverse_mappings["team"]:
        logging.warning("teamæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["team"].update({"team.other": "å…¶ä»–"})

    if not reverse_mappings["tags"]:
        logging.warning("tagsæ˜ å°„ä¸ºç©ºï¼Œæ·»åŠ åŸºç¡€åå¤‡æ˜ å°„")
        reverse_mappings["tags"].update(
            {
                "tag.DIY": "DIY",
                "tag.ä¸­å­—": "ä¸­å­—",
                "tag.HDR": "HDR",
            }
        )


@migrate_bp.route("/migrate/download_torrent_only", methods=["POST"])
def download_torrent_only():
    """ä»…ä¸‹è½½ç§å­æ–‡ä»¶ï¼Œä¸è¿›è¡Œæ•°æ®è§£ææˆ–å­˜å‚¨"""
    migrator = None
    try:
        data = request.json
        torrent_id = data.get("torrent_id")
        site_name = data.get("site_name")

        if not all([torrent_id, site_name]):
            return (
                jsonify(
                    {"success": False, "message": "é”™è¯¯ï¼šç¼ºå°‘å¿…è¦å‚æ•°ï¼ˆtorrent_idã€site_nameï¼‰"}
                ),
                400,
            )

        db_manager = migrate_bp.db_manager

        # è·å–ç«™ç‚¹ä¿¡æ¯
        source_info = db_manager.get_site_by_nickname(site_name)
        if not source_info or not source_info.get("cookie"):
            return (
                jsonify({"success": False, "message": f"é”™è¯¯ï¼šæºç«™ç‚¹ '{site_name}' é…ç½®ä¸å®Œæ•´ã€‚"}),
                404,
            )

        # è·å–è‹±æ–‡ç«™ç‚¹åï¼ˆç”¨äºæ–‡ä»¶åå‰ç¼€ï¼‰
        site_code = source_info.get("site", site_name.lower())

        # ä½¿ç”¨ç»Ÿä¸€çš„ç§å­ç›®å½•
        from config import TEMP_DIR
        import os
        import urllib.parse
        import re

        torrent_dir = os.path.join(TEMP_DIR, "torrents")
        os.makedirs(torrent_dir, exist_ok=True)

        # åˆ›å»ºTorrentMigratorå®ä¾‹ä»…ç”¨äºä¸‹è½½ç§å­æ–‡ä»¶
        migrator = TorrentMigrator(
            source_site_info=source_info,
            target_site_info=None,
            search_term=torrent_id,
            config_manager=config_manager,
            db_manager=db_manager,
        )

        # ä¸‹è½½ç§å­æ–‡ä»¶ï¼ˆè¿”å›çš„æ˜¯åŸå§‹æ–‡ä»¶åï¼‰
        torrent_path = migrator._download_torrent_file(torrent_id, torrent_dir)

        if torrent_path and os.path.exists(torrent_path):
            # è·å–åŸå§‹æ–‡ä»¶å
            original_filename = os.path.basename(torrent_path)

            # æ·»åŠ ç«™ç‚¹-ID-å‰ç¼€ï¼Œä¸prepare_review_dataä¿æŒä¸€è‡´
            prefixed_filename = f"{site_code}-{torrent_id}-{original_filename}"
            prefixed_torrent_path = os.path.join(torrent_dir, prefixed_filename)

            # é‡å‘½åæ–‡ä»¶
            try:
                os.rename(torrent_path, prefixed_torrent_path)
                logging.info(f"ç§å­æ–‡ä»¶å·²é‡å‘½å: {original_filename} -> {prefixed_filename}")

                return jsonify(
                    {
                        "success": True,
                        "torrent_path": prefixed_torrent_path,
                        "torrent_dir": torrent_dir,
                        "message": "ç§å­æ–‡ä»¶ä¸‹è½½æˆåŠŸ",
                    }
                )
            except Exception as rename_error:
                logging.error(f"é‡å‘½åç§å­æ–‡ä»¶å¤±è´¥: {rename_error}")
                # å¦‚æœé‡å‘½åå¤±è´¥ï¼Œä»ç„¶è¿”å›åŸå§‹è·¯å¾„
                return jsonify(
                    {
                        "success": True,
                        "torrent_path": torrent_path,
                        "torrent_dir": torrent_dir,
                        "message": "ç§å­æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼ˆæœªæ·»åŠ å‰ç¼€ï¼‰",
                    }
                )
        else:
            return jsonify({"success": False, "message": "ç§å­æ–‡ä»¶ä¸‹è½½å¤±è´¥"}), 500

    except Exception as e:
        logging.error(f"download_torrent_only å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500
    finally:
        if migrator:
            # ä¸‹è½½ä¸“ç”¨ï¼šä¿ç•™å·²ä¸‹è½½çš„ç§å­æ–‡ä»¶ï¼Œä½†é‡Šæ”¾ loguru sink
            migrator.cleanup(remove_temp_files=False)


# æ–°å¢ï¼šä¸“é—¨è´Ÿè´£æ•°æ®æŠ“å–å’Œå­˜å‚¨çš„APIæ¥å£
@migrate_bp.route("/migrate/fetch_and_store", methods=["POST"])
def migrate_fetch_and_store():
    """ä¸“é—¨è´Ÿè´£ç§å­ä¿¡æ¯æŠ“å–å’Œå­˜å‚¨ï¼Œä¸è¿”å›é¢„è§ˆæ•°æ®"""
    db_manager = migrate_bp.db_manager
    data = request.json
    migrator = None
    source_site_name, search_term, save_path, torrent_name, downloader_id = (
        data.get("sourceSite"),
        data.get("searchTerm"),
        data.get("savePath", ""),
        data.get("torrentName"),
        data.get("downloaderId"),
    )

    if not all([source_site_name, search_term]):
        return jsonify({"success": False, "message": "é”™è¯¯ï¼šæºç«™ç‚¹å’Œæœç´¢è¯ä¸èƒ½ä¸ºç©ºã€‚"}), 400

    # æ¥æ”¶å‰ç«¯ä¼ æ¥çš„task_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆæ–°çš„
    task_id = data.get("task_id")
    if not task_id:
        task_id = str(uuid.uuid4())

    # åˆ›å»ºæˆ–è·å–æ—¥å¿—æµ
    log_streamer.create_stream(task_id)
    log_streamer.emit_log(task_id, "å¼€å§‹æŠ“å–", "æ­£åœ¨ä»æºç«™ç‚¹æŠ“å–ç§å­ä¿¡æ¯...", "processing")

    try:
        # è·å–ç«™ç‚¹ä¿¡æ¯å¹¶è·å–è‹±æ–‡ç«™ç‚¹å
        source_info = db_manager.get_site_by_nickname(source_site_name)

        if not source_info or not source_info.get("cookie"):
            return (
                jsonify(
                    {
                        "success": False,
                        "message": f"é”™è¯¯ï¼šæºç«™ç‚¹ '{source_site_name}' é…ç½®ä¸å®Œæ•´ã€‚",
                    }
                ),
                404,
            )

        source_role = source_info.get("migration", 0)

        if source_role not in [1, 3]:
            return (
                jsonify(
                    {
                        "success": False,
                        "message": f"é”™è¯¯ï¼šç«™ç‚¹ '{source_site_name}' ä¸å…è®¸ä½œä¸ºæºç«™ç‚¹è¿›è¡Œè¿ç§»ã€‚",
                    }
                ),
                403,
            )

        # è·å–è‹±æ–‡ç«™ç‚¹åä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
        english_site_name = source_info.get("site", source_site_name.lower())

        # åˆå§‹åŒ– Migrator æ—¶ä¸ä¼ å…¥ç›®æ ‡ç«™ç‚¹ä¿¡æ¯
        migrator = TorrentMigrator(
            source_site_info=source_info,
            target_site_info=None,
            search_term=search_term,
            save_path=save_path,
            torrent_name=torrent_name,
            config_manager=config_manager,
            db_manager=db_manager,
            downloader_id=downloader_id,
            task_id=task_id,
        )  # ä¼ é€’task_id

        result = migrator.prepare_review_data()

        if "review_data" in result:
            new_task_id = str(uuid.uuid4())
            # åªç¼“å­˜å¿…è¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç§å­ç›®å½•è·¯å¾„ç”¨äºå‘å¸ƒæ—¶æŸ¥æ‰¾ç§å­æ–‡ä»¶
            with MIGRATION_CACHE_LOCK:
                MIGRATION_CACHE[new_task_id] = {
                    "source_info": source_info,
                    "original_torrent_path": result["original_torrent_path"],
                    "torrent_dir": result["torrent_dir"],  # ä¿å­˜ç§å­ç›®å½•è·¯å¾„
                    "source_site_name": english_site_name,  # ä½¿ç”¨è‹±æ–‡ç«™ç‚¹åä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
                    "source_torrent_id": search_term,
                }

            logging.info(
                f"ç§å­ä¿¡æ¯æŠ“å–å¹¶å­˜å‚¨æˆåŠŸ: {search_term} from {source_site_name} ({english_site_name})"
            )

            # æ ‡è®°"å¼€å§‹æŠ“å–"æ­¥éª¤ä¸ºæˆåŠŸ
            log_streamer.emit_log(task_id, "å¼€å§‹æŠ“å–", "ç§å­ä¿¡æ¯æŠ“å–å®Œæˆ", "success")
            # å…³é—­æ—¥å¿—æµ
            log_streamer.close_stream(task_id)

            return jsonify(
                {
                    "success": True,
                    "task_id": new_task_id,
                    "message": "ç§å­ä¿¡æ¯å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“",
                    "logs": result["logs"],
                }
            )
        else:
            # æŠ“å–å¤±è´¥ï¼Œæ ‡è®°ä¸ºé”™è¯¯
            log_streamer.emit_log(task_id, "å¼€å§‹æŠ“å–", result.get("logs", "æŠ“å–å¤±è´¥"), "error")
            log_streamer.close_stream(task_id)

            return jsonify({"success": False, "message": result.get("logs", "æœªçŸ¥é”™è¯¯")})
    except Exception as e:
        logging.error(f"migrate_fetch_and_store å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"}), 500
    finally:
        if migrator:
            # è¯¥æ¥å£éœ€è¦ä¿ç•™ original_torrent_path ä¾›åç»­å‘å¸ƒå¤ç”¨
            migrator.cleanup(remove_temp_files=False)


# æ–°å¢ï¼šæ›´æ–°æ•°æ®åº“ç§å­å‚æ•°å¹¶é‡æ–°æ ‡å‡†åŒ–çš„APIæ¥å£
@migrate_bp.route("/migrate/update_db_seed_info", methods=["POST"])
def update_db_seed_info():
    """æ›´æ–°æ•°æ®åº“ä¸­çš„å‚æ•°å¹¶é‡æ–°æ ‡å‡†åŒ–"""
    try:
        data = request.json
        torrent_name = data.get("torrent_name")
        torrent_id = data.get("torrent_id")
        site_name = data.get("site_name")
        updated_parameters = data.get("updated_parameters")

        db_manager = migrate_bp.db_manager

        try:
            # æ›´æ–°æ•°æ®åº“
            from models.seed_parameter import SeedParameter

            seed_param_model = SeedParameter(db_manager)

            logging.info(f"å¼€å§‹æ›´æ–°ç§å­å‚æ•°: {torrent_id} from {site_name} ({site_name})")

            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æä¾›äº†ä¿®æ”¹çš„æ ‡å‡†å‚æ•°
            user_standardized_params = updated_parameters.get("standardized_params", {})

            if user_standardized_params:
                # ç”¨æˆ·å·²ç»ä¿®æ”¹äº†æ ‡å‡†å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·çš„ä¿®æ”¹
                logging.info("ä½¿ç”¨ç”¨æˆ·ä¿®æ”¹çš„æ ‡å‡†å‚æ•°")
                standardized_params = user_standardized_params
            else:
                # ç”¨æˆ·æ²¡æœ‰ä¿®æ”¹æ ‡å‡†å‚æ•°ï¼Œé‡æ–°è¿›è¡Œå‚æ•°æ ‡å‡†åŒ–
                logging.info("ç”¨æˆ·æœªä¿®æ”¹æ ‡å‡†å‚æ•°ï¼Œé‡æ–°è¿›è¡Œè‡ªåŠ¨æ ‡å‡†åŒ–")
                # é‡æ–°è¿›è¡Œå‚æ•°æ ‡å‡†åŒ–ï¼ˆæ¨¡æ‹ŸParameterMapperçš„å¤„ç†ï¼‰
                # éœ€è¦æ„é€ extracted_dataæ ¼å¼ç”¨äºæ˜ å°„
                extracted_data = {
                    "title": updated_parameters.get("title", ""),
                    "subtitle": updated_parameters.get("subtitle", ""),
                    "imdb_link": updated_parameters.get("imdb_link", ""),
                    "douban_link": updated_parameters.get("douban_link", ""),
                    "tmdb_link": updated_parameters.get("tmdb_link", ""),
                    "intro": {
                        "statement": updated_parameters.get("statement", ""),
                        "poster": updated_parameters.get("poster", ""),
                        "body": updated_parameters.get("body", ""),
                        "screenshots": updated_parameters.get("screenshots", ""),
                        "imdb_link": updated_parameters.get("imdb_link", ""),
                        "douban_link": updated_parameters.get("douban_link", ""),
                        "tmdb_link": updated_parameters.get("tmdb_link", ""),
                    },
                    "mediainfo": updated_parameters.get("mediainfo", ""),
                    "source_params": updated_parameters.get("source_params", {}),
                    "title_components": updated_parameters.get("title_components", []),
                }

                # ä½¿ç”¨ParameterMapperé‡æ–°æ ‡å‡†åŒ–å‚æ•°
                from core.extractors.extractor import ParameterMapper

                mapper = ParameterMapper()

                # é‡æ–°æ ‡å‡†åŒ–å‚æ•°
                standardized_params = mapper.map_parameters(site_name, site_name, extracted_data)

            # ä»title_componentsä¸­æå–æ ‡é¢˜æ‹†è§£çš„å„é¡¹å‚æ•°
            title_components = updated_parameters.get("title_components", [])

            # [æ–°å¢] å¼€å§‹ï¼šæ ¹æ® title_components æ‹¼æ¥æ–°æ ‡é¢˜
            # 1. å°† title_components åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ï¼Œæ–¹ä¾¿åç»­æŸ¥æ‰¾
            title_params = {
                item["key"]: item["value"] for item in title_components if item.get("value")
            }

            # 2. ä» global_mappings.yaml è¯»å–æ‹¼æ¥é¡ºåº
            import yaml

            global_mappings_path = GLOBAL_MAPPINGS

            # é»˜è®¤é¡ºåºï¼ˆå¦‚æœè¯»å–é…ç½®å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
            order = [
                "ä¸»æ ‡é¢˜",
                "å­£é›†",
                "å¹´ä»½",
                "å‰§é›†çŠ¶æ€",
                "å‘å¸ƒç‰ˆæœ¬",
                "åˆ†è¾¨ç‡",
                "ç‰‡æºå¹³å°",
                "åª’ä»‹",
                "å¸§ç‡",
                "è§†é¢‘ç¼–ç ",
                "è§†é¢‘æ ¼å¼",
                "HDRæ ¼å¼",
                "è‰²æ·±",
                "éŸ³é¢‘ç¼–ç ",
            ]

            try:
                if os.path.exists(global_mappings_path):
                    with open(global_mappings_path, "r", encoding="utf-8") as f:
                        global_config = yaml.safe_load(f)
                        default_title_components = global_config.get(
                            "default_title_components", {}
                        )

                        if default_title_components:
                            # æŒ‰ç…§é…ç½®æ–‡ä»¶ä¸­çš„é¡ºåºæ„å»º order åˆ—è¡¨
                            order = []
                            for key, config in default_title_components.items():
                                if isinstance(config, dict) and "source_key" in config:
                                    order.append(config["source_key"])

                            logging.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–åˆ°æ ‡é¢˜æ‹¼æ¥é¡ºåº: {order}")
            except Exception as e:
                logging.warning(f"è¯»å– global_mappings.yaml å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é¡ºåº: {e}")
            title_parts = []
            for key in order:
                value = title_params.get(key)
                if value:
                    title_parts.append(
                        " ".join(map(str, value)) if isinstance(value, list) else str(value)
                    )

            raw_main_part = " ".join(filter(None, title_parts))
            main_part = re.sub(r"(?<!\d)\.(?!\d)", " ", raw_main_part)
            main_part = re.sub(r"\s+", " ", main_part).strip()
            release_group = title_params.get("åˆ¶ä½œç»„", "NOGROUP")
            if "N/A" in release_group:
                release_group = "NOGROUP"

            # å¯¹ç‰¹æ®Šåˆ¶ä½œç»„è¿›è¡Œå¤„ç†ï¼Œä¸éœ€è¦æ·»åŠ å‰ç¼€è¿å­—ç¬¦
            special_groups = ["MNHD-FRDS", "mUHD-FRDS"]
            if release_group in special_groups:
                preview_title = f"{main_part} {release_group}"
            else:
                preview_title = f"{main_part}-{release_group}"
            # [æ–°å¢] ç»“æŸï¼šæ ‡é¢˜æ‹¼æ¥å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ preview_title å˜é‡ä¸­

            # æ„é€ å®Œæ•´çš„å­˜å‚¨å‚æ•°
            final_parameters = {
                # [ä¿®æ”¹] å°†åŸæ¥çš„ title å€¼æ›¿æ¢ä¸ºæ–°ç”Ÿæˆçš„ preview_title
                "title": preview_title,
                "subtitle": updated_parameters.get("subtitle", ""),
                "imdb_link": updated_parameters.get("imdb_link", ""),
                "douban_link": updated_parameters.get("douban_link", ""),
                "tmdb_link": updated_parameters.get("tmdb_link", ""),
                "poster": updated_parameters.get("poster", ""),
                "screenshots": updated_parameters.get("screenshots", ""),
                "statement": updated_parameters.get("statement", ""),
                "body": updated_parameters.get("body", ""),
                "mediainfo": updated_parameters.get("mediainfo", ""),
                "type": standardized_params.get("type", ""),
                "medium": standardized_params.get("medium", ""),
                "video_codec": standardized_params.get("video_codec", ""),
                "audio_codec": standardized_params.get("audio_codec", ""),
                "resolution": standardized_params.get("resolution", ""),
                "team": standardized_params.get("team", ""),
                "source": standardized_params.get("source", ""),
                "tags": standardized_params.get("tags", []),
                "title_components": title_components,
                "standardized_params": standardized_params,
                "is_reviewed": True,  # æ ‡è®°ä¸ºå·²æ£€æŸ¥
                "final_publish_parameters": {
                    # [ä¿®æ”¹] é¢„è§ˆæ ‡é¢˜ä¹Ÿä½¿ç”¨æ–°ç”Ÿæˆçš„æ ‡é¢˜
                    "ä¸»æ ‡é¢˜ (é¢„è§ˆ)": preview_title,
                    "å‰¯æ ‡é¢˜": updated_parameters.get("subtitle", ""),
                    "IMDbé“¾æ¥": standardized_params.get("imdb_link", ""),
                    "è±†ç“£é“¾æ¥": standardized_params.get("douban_link", ""),
                    "TMDbé“¾æ¥": standardized_params.get("tmdb_link", ""),
                    "ç±»å‹": standardized_params.get("type", ""),
                    "åª’ä»‹": standardized_params.get("medium", ""),
                    "è§†é¢‘ç¼–ç ": standardized_params.get("video_codec", ""),
                    "éŸ³é¢‘ç¼–ç ": standardized_params.get("audio_codec", ""),
                    "åˆ†è¾¨ç‡": standardized_params.get("resolution", ""),
                    "åˆ¶ä½œç»„": standardized_params.get("team", ""),
                    "äº§åœ°": standardized_params.get("source", ""),
                    "æ ‡ç­¾": standardized_params.get("tags", []),
                },
                "complete_publish_params": {
                    "title_components": updated_parameters.get("title_components", []),
                    "subtitle": updated_parameters.get("subtitle", ""),
                    "imdb_link": standardized_params.get("imdb_link", ""),
                    "douban_link": standardized_params.get("douban_link", ""),
                    "tmdb_link": standardized_params.get("tmdb_link", ""),
                    "intro": {
                        "statement": updated_parameters.get("statement", ""),
                        "poster": updated_parameters.get("poster", ""),
                        "body": updated_parameters.get("body", ""),
                        "screenshots": updated_parameters.get("screenshots", ""),
                        "removed_ardtudeclarations": updated_parameters.get(
                            "removed_ardtudeclarations", []
                        ),
                        "imdb_link": updated_parameters.get("imdb_link", ""),
                        "douban_link": updated_parameters.get("douban_link", ""),
                    },
                    "mediainfo": updated_parameters.get("mediainfo", ""),
                    "source_params": updated_parameters.get("source_params", {}),
                    "standardized_params": standardized_params,
                },
                "raw_params_for_preview": {
                    # [ä¿®æ”¹] åŸå§‹é¢„è§ˆå‚æ•°ä¹Ÿä½¿ç”¨æ–°æ ‡é¢˜
                    "final_main_title": preview_title,
                    "subtitle": updated_parameters.get("subtitle", ""),
                    "imdb_link": standardized_params.get("imdb_link", ""),
                    "douban_link": standardized_params.get("douban_link", ""),
                    "tmdb_link": standardized_params.get("tmdb_link", ""),
                    "type": standardized_params.get("type", ""),
                    "medium": standardized_params.get("medium", ""),
                    "video_codec": standardized_params.get("video_codec", ""),
                    "audio_codec": standardized_params.get("audio_codec", ""),
                    "resolution": standardized_params.get("resolution", ""),
                    "release_group": standardized_params.get("team", ""),
                    "source": standardized_params.get("source", ""),
                    "tags": standardized_params.get("tags", []),
                },
            }

            # éœ€è¦å…ˆè·å–hashå€¼
            hash_value = seed_param_model.search_torrent_hash_by_torrentid(torrent_id, site_name)
            if hash_value:
                update_result = seed_param_model.update_parameters(hash_value, final_parameters)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°hashï¼Œå°è¯•æ’å…¥æ–°è®°å½•
                final_parameters["hash"] = f"manual_{torrent_id}_{site_name}"  # ä¸´æ—¶hash
                final_parameters["torrent_id"] = torrent_id
                final_parameters["site_name"] = site_name
                # ç¡®ä¿ä¼ é€’æ­£ç¡®çš„ torrent_id å’Œ site_name
                update_result = seed_param_model.save_parameters(
                    final_parameters["hash"], torrent_id, site_name, final_parameters
                )

            if update_result:
                logging.info(f"ç§å­å‚æ•°æ›´æ–°æˆåŠŸ: {torrent_id} from {site_name} ({site_name})")

                # ç”Ÿæˆåå‘æ˜ å°„è¡¨ï¼ˆä»æ ‡å‡†é”®åˆ°ä¸­æ–‡æ˜¾ç¤ºåç§°çš„æ˜ å°„ï¼‰
                reverse_mappings = generate_reverse_mappings()

                return jsonify(
                    {
                        "success": True,
                        "standardized_params": standardized_params,
                        "final_publish_parameters": final_parameters["final_publish_parameters"],
                        "complete_publish_params": final_parameters["complete_publish_params"],
                        "raw_params_for_preview": final_parameters["raw_params_for_preview"],
                        "reverse_mappings": reverse_mappings,
                        "message": "å‚æ•°æ›´æ–°å¹¶æ ‡å‡†åŒ–æˆåŠŸ",
                    }
                )
            else:
                logging.warning(f"ç§å­å‚æ•°æ›´æ–°å¤±è´¥: {torrent_id} from {site_name} ({site_name})")
                return jsonify({"success": False, "message": "å‚æ•°æ›´æ–°å¤±è´¥"}), 500

        except Exception as e:
            logging.error(f"æ›´æ–°ç§å­å‚æ•°å¤±è´¥: {e}", exc_info=True)
            return jsonify({"success": False, "message": f"æ›´æ–°å¤±è´¥: {str(e)}"}), 500

    except Exception as e:
        logging.error(f"update_db_seed_infoå‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


def _migrate_publish_impl(db_manager, data):
    """å‘å¸ƒåˆ°å•ä¸ªç«™ç‚¹çš„æ ¸å¿ƒé€»è¾‘ï¼ˆå¯è¢«æ‰¹é‡å‘å¸ƒå¤ç”¨ï¼‰ã€‚"""
    data = data or {}
    task_id = data.get("task_id")
    upload_data = copy.deepcopy(data.get("upload_data") or {})
    target_site_name = data.get("targetSite")
    source_site_name = data.get("sourceSite")

    if not task_id:
        return {"success": False, "logs": "é”™è¯¯ï¼šæ— æ•ˆæˆ–å·²è¿‡æœŸçš„ä»»åŠ¡IDã€‚", "url": None}, 400

    with MIGRATION_CACHE_LOCK:
        context = MIGRATION_CACHE.get(task_id)

    if not context:
        return {"success": False, "logs": "é”™è¯¯ï¼šæ— æ•ˆæˆ–å·²è¿‡æœŸçš„ä»»åŠ¡IDã€‚", "url": None}, 400

    if not target_site_name:
        return {"success": False, "logs": "é”™è¯¯ï¼šå¿…é¡»æä¾›ç›®æ ‡ç«™ç‚¹åç§°ã€‚", "url": None}, 400

    migrator = None  # ç¡®ä¿åœ¨ finally ä¸­å¯ç”¨

    try:
        # ğŸš« å‘å¸ƒå‰æ ‡ç­¾é™åˆ¶æ£€æŸ¥ï¼šç¦è½¬/é™è½¬/åˆ†é›†ç›´æ¥æ‹¦æˆª
        restricted_tag_map = {
            "ç¦è½¬": "tag.ç¦è½¬",
            "tag.ç¦è½¬": "tag.ç¦è½¬",
            "é™è½¬": "tag.é™è½¬",
            "tag.é™è½¬": "tag.é™è½¬",
            "åˆ†é›†": "tag.åˆ†é›†",
            "tag.åˆ†é›†": "tag.åˆ†é›†",
        }
        standardized_params = (upload_data or {}).get("standardized_params", {})
        raw_tags = (standardized_params.get("tags") or []) + (upload_data or {}).get("tags", [])
        restricted_tags = []
        for tag in raw_tags:
            mapped_tag = restricted_tag_map.get(tag)
            if mapped_tag and mapped_tag not in restricted_tags:
                restricted_tags.append(mapped_tag)

        if restricted_tags:
            return (
                {
                    "success": False,
                    "logs": f"ğŸš« å‘å¸ƒå‰æ ‡ç­¾é™åˆ¶: æ£€æµ‹åˆ°ç¦è½¬/é™è½¬/åˆ†é›†æ ‡ç­¾ {restricted_tags}",
                    "limit_reached": True,
                    "pre_check": True,
                    "url": None,
                },
                200,
            )

        target_info = db_manager.get_site_by_nickname(target_site_name)
        if not target_info:
            return (
                {
                    "success": False,
                    "logs": f"é”™è¯¯: ç›®æ ‡ç«™ç‚¹ '{target_site_name}' é…ç½®ä¸å®Œæ•´ã€‚",
                    "url": None,
                },
                404,
            )

        source_info = context["source_info"]
        original_torrent_path = context.get("original_torrent_path")
        torrent_dir = context.get("torrent_dir", "")  # è·å–ç§å­ç›®å½•

        # ä»ç¼“å­˜ä¸­è·å–æºç«™ç‚¹åç§°ï¼ˆå¦‚æœå‰ç«¯æ²¡æœ‰ä¼ é€’ï¼‰
        if not source_site_name:
            source_site_name = context.get("source_site_name", "")

        # ğŸš« å‘å¸ƒå‰é¢„æ£€æŸ¥å‘ç§é™åˆ¶ - åœ¨ä»»ä½•å‘å¸ƒé€»è¾‘ä¹‹å‰è¿›è¡Œ
        downloader_id = data.get("downloaderId") or data.get("downloader_id")
        if downloader_id:
            try:
                from .internal_guard import check_downloader_gate

                can_continue, limit_message = check_downloader_gate(downloader_id)

                if not can_continue:
                    return (
                        {
                            "success": False,
                            "logs": f"ğŸš« å‘å¸ƒå‰é¢„æ£€æŸ¥è§¦å‘é™åˆ¶: {limit_message}",
                            "limit_reached": True,
                            "pre_check": True,
                            "url": None,
                        },
                        200,
                    )
                else:
                    print(f"âœ… [å‘å¸ƒå‰é¢„æ£€æŸ¥] é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­å‘å¸ƒåˆ° {target_site_name}")
            except Exception as e:
                print(f"âš ï¸ [å‘å¸ƒå‰é¢„æ£€æŸ¥] æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: {e}")

        # åˆ›å»º TorrentMigrator å®ä¾‹ç”¨äºå‘å¸ƒ
        migrator = TorrentMigrator(
            source_info,
            target_info,
            search_term=context.get("source_torrent_id", ""),
            save_path=upload_data.get("save_path", "") or upload_data.get("savePath", ""),
            config_manager=config_manager,
            db_manager=db_manager,
        )

        # æ£€æŸ¥ç§å­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°ä¸‹è½½
        # [æ ¸å¿ƒä¿®æ”¹] ä¼˜å…ˆåœ¨ç»Ÿä¸€çš„ torrents ç›®å½•ä¸­æŸ¥æ‰¾
        source_torrent_id = context.get("source_torrent_id", "")
        source_site_code = source_info.get("site", (source_site_name or "").lower())

        if original_torrent_path is None or not os.path.exists(original_torrent_path):
            logging.info("åŸå§‹ç§å­æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œå¼€å§‹åœ¨ç»Ÿä¸€ç›®å½•ä¸­æŸ¥æ‰¾")

            from config import TEMP_DIR

            torrents_dir = os.path.join(TEMP_DIR, "torrents")

            # [æ–°å¢] é¦–å…ˆåœ¨ç»Ÿä¸€çš„ torrents ç›®å½•ä¸­æŸ¥æ‰¾ä»¥"ç«™ç‚¹-ID-"å¼€å¤´çš„ç§å­æ–‡ä»¶
            if os.path.exists(torrents_dir) and source_torrent_id:
                prefix = f"{source_site_code}-{source_torrent_id}-"
                logging.info(f"åœ¨ç»Ÿä¸€ç›®å½•ä¸­æŸ¥æ‰¾ç§å­æ–‡ä»¶ï¼Œå‰ç¼€: {prefix}")

                try:
                    for file in os.listdir(torrents_dir):
                        if file.startswith(prefix) and file.endswith(".torrent"):
                            original_torrent_path = os.path.join(torrents_dir, file)
                            torrent_dir = torrents_dir
                            logging.info(f"âœ… åœ¨ç»Ÿä¸€ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {file}")
                            break
                except Exception as e:
                    logging.warning(f"éå†ç»Ÿä¸€ç›®å½•æ—¶å‡ºé”™: {e}")

            # å¦‚æœåœ¨ç»Ÿä¸€ç›®å½•ä¸­æ²¡æ‰¾åˆ°ï¼Œå†æ£€æŸ¥æ—§æ ¼å¼ç›®å½•
            if (
                original_torrent_path is None or not os.path.exists(original_torrent_path)
            ) and source_torrent_id:
                logging.info("ç»Ÿä¸€ç›®å½•ä¸­æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥æ—§æ ¼å¼ç›®å½•")
                old_torrent_dir = os.path.join(TEMP_DIR, f"torrent_{source_torrent_id}")
                if os.path.exists(old_torrent_dir):
                    try:
                        for file in os.listdir(old_torrent_dir):
                            if file.endswith(".torrent"):
                                original_torrent_path = os.path.join(old_torrent_dir, file)
                                torrent_dir = old_torrent_dir
                                logging.info(
                                    f"åœ¨æ—§æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                )
                                break
                    except Exception as e:
                        logging.warning(f"æŸ¥æ‰¾æ—§æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                if original_torrent_path is None or not os.path.exists(original_torrent_path):
                    cached_torrent_dir = context.get("torrent_dir")
                    if cached_torrent_dir and os.path.exists(cached_torrent_dir):
                        try:
                            for file in os.listdir(cached_torrent_dir):
                                if file.endswith(".torrent"):
                                    original_torrent_path = os.path.join(cached_torrent_dir, file)
                                    torrent_dir = cached_torrent_dir
                                    logging.info(
                                        f"åœ¨æ–°æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                    )
                                    break
                        except Exception as e:
                            logging.warning(f"æŸ¥æ‰¾æ–°æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                if original_torrent_path is None or not os.path.exists(original_torrent_path):
                    try:
                        seed_name = get_seed_name(db_manager, source_torrent_id, source_site_name)
                        if seed_name:
                            safe_filename_base = re.sub(r'[<>:"/\\|?*]', "_", seed_name).strip()
                            seed_name_dir = os.path.join(TEMP_DIR, safe_filename_base)
                            if os.path.exists(seed_name_dir):
                                for file in os.listdir(seed_name_dir):
                                    if file.endswith(".torrent"):
                                        original_torrent_path = os.path.join(seed_name_dir, file)
                                        torrent_dir = seed_name_dir
                                        logging.info(
                                            f"åœ¨ç§å­åç§°ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                        )
                                        break
                    except Exception as e:
                        logging.warning(f"æŸ¥æ‰¾ç§å­åç§°ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")

            if original_torrent_path is None or not os.path.exists(original_torrent_path):
                # å¹¶å‘å‘å¸ƒæ—¶ï¼Œåªå…è®¸ä¸€ä¸ªçº¿ç¨‹è´Ÿè´£ä¸‹è½½/è¡¥é½åŸå§‹ .torrentï¼Œé¿å…åŒæ—¶å†™åŒä¸€ä¸ªæ–‡ä»¶å¯¼è‡´æŸå
                with MIGRATION_CACHE_LOCK:
                    torrent_file_lock = MIGRATION_TORRENT_FILE_LOCKS.setdefault(
                        task_id, threading.Lock()
                    )

                with torrent_file_lock:
                    # å¦ä¸€çº¿ç¨‹å¯èƒ½å·²ç»è¡¥é½/ä¸‹è½½æˆåŠŸ
                    with MIGRATION_CACHE_LOCK:
                        refreshed_context = MIGRATION_CACHE.get(task_id) or {}
                        refreshed_path = refreshed_context.get("original_torrent_path")
                        refreshed_dir = refreshed_context.get("torrent_dir")
                    if refreshed_path and os.path.exists(refreshed_path):
                        original_torrent_path = refreshed_path
                        if refreshed_dir:
                            torrent_dir = refreshed_dir
                    else:
                        logging.info("éœ€è¦é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶")
                        try:
                            import cloudscraper

                            session = requests.Session()
                            session.verify = False
                            scraper = cloudscraper.create_scraper(sess=session)

                            SOURCE_BASE_URL = source_info.get("base_url", "").rstrip("/")
                            if SOURCE_BASE_URL and not SOURCE_BASE_URL.startswith(
                                ("http://", "https://")
                            ):
                                SOURCE_BASE_URL = "https://" + SOURCE_BASE_URL
                            SOURCE_COOKIE = source_info.get("cookie", "")
                            source_torrent_id = context.get("source_torrent_id", "")

                            if SOURCE_BASE_URL and SOURCE_COOKIE and source_torrent_id:
                                response = scraper.get(
                                    f"{SOURCE_BASE_URL}/details.php",
                                    headers={
                                        "Cookie": SOURCE_COOKIE,
                                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                                    },
                                    params={"id": source_torrent_id, "hit": "1"},
                                    timeout=180,
                                )
                                response.raise_for_status()
                                response.encoding = "utf-8"

                                soup = BeautifulSoup(response.text, "html.parser")
                                download_link_tag = soup.select_one(
                                    f'a.index[href^="download.php?id={source_torrent_id}"]'
                                )

                                if not download_link_tag:
                                    logging.error("æœªæ‰¾åˆ°ç§å­ä¸‹è½½é“¾æ¥")
                                    return (
                                        {
                                            "success": False,
                                            "logs": "é”™è¯¯ï¼šæœªæ‰¾åˆ°ç§å­ä¸‹è½½é“¾æ¥ã€‚",
                                            "url": None,
                                        },
                                        500,
                                    )

                                torrent_response = scraper.get(
                                    f"{SOURCE_BASE_URL}/{download_link_tag['href']}",
                                    headers={
                                        "Cookie": SOURCE_COOKIE,
                                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                                    },
                                    timeout=180,
                                )
                                torrent_response.raise_for_status()

                                content_disposition = torrent_response.headers.get(
                                    "content-disposition"
                                )
                                torrent_filename = "unknown.torrent"
                                if content_disposition:
                                    filename_match = re.search(
                                        r'filename\*="?UTF-8\'\'([^"]+)"?',
                                        content_disposition,
                                        re.IGNORECASE,
                                    )
                                    if filename_match:
                                        torrent_filename = urllib.parse.unquote(
                                            filename_match.group(1), encoding="utf-8"
                                        )
                                    else:
                                        filename_match = re.search(
                                            r'filename="?([^"]+)"?', content_disposition
                                        )
                                        if filename_match:
                                            torrent_filename = urllib.parse.unquote(
                                                filename_match.group(1)
                                            )

                                torrent_dir = os.path.join(TEMP_DIR, "torrents")
                                os.makedirs(torrent_dir, exist_ok=True)
                                source_site_code = source_info.get(
                                    "site", (source_site_name or "").lower()
                                )

                                safe_filename = re.sub(r"[<>:\"/\\\\|?*]", "_", torrent_filename)
                                if len(safe_filename.encode("utf-8")) > 255:
                                    name, ext = os.path.splitext(safe_filename)
                                    max_len = 255 - len(ext.encode("utf-8"))
                                    safe_filename = (
                                        name.encode("utf-8")[:max_len].decode("utf-8", "ignore")
                                        + ext
                                    )

                                prefixed_filename = (
                                    f"{source_site_code}-{source_torrent_id}-{safe_filename}"
                                )
                                original_torrent_path = os.path.join(
                                    torrent_dir, prefixed_filename
                                )
                                tmp_torrent_path = (
                                    f"{original_torrent_path}.tmp-{uuid.uuid4().hex}"
                                )
                                try:
                                    with open(tmp_torrent_path, "wb") as f:
                                        f.write(torrent_response.content)
                                    os.replace(tmp_torrent_path, original_torrent_path)
                                finally:
                                    try:
                                        if os.path.exists(tmp_torrent_path):
                                            os.remove(tmp_torrent_path)
                                    except Exception:
                                        pass
                                logging.info(f"é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶æˆåŠŸ: {original_torrent_path}")
                            else:
                                logging.error("ç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œæ— æ³•é‡æ–°ä¸‹è½½ç§å­")
                                return (
                                    {
                                        "success": False,
                                        "logs": "é”™è¯¯ï¼šç¼ºå°‘å¿…è¦ä¿¡æ¯ï¼Œæ— æ³•é‡æ–°ä¸‹è½½ç§å­ã€‚",
                                        "url": None,
                                    },
                                    500,
                                )
                        except Exception as e:
                            logging.error(f"é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                            return (
                                {
                                    "success": False,
                                    "logs": f"é”™è¯¯ï¼šé‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}",
                                    "url": None,
                                },
                                500,
                            )

                    # ä¸‹è½½/è¡¥é½åæ›´æ–°ç¼“å­˜ï¼Œä¾›å…¶å®ƒçº¿ç¨‹å¤ç”¨
                    if original_torrent_path and os.path.exists(original_torrent_path):
                        with MIGRATION_CACHE_LOCK:
                            if task_id in MIGRATION_CACHE:
                                MIGRATION_CACHE[task_id][
                                    "original_torrent_path"
                                ] = original_torrent_path
                                if torrent_dir:
                                    MIGRATION_CACHE[task_id]["torrent_dir"] = torrent_dir

        if original_torrent_path and os.path.exists(original_torrent_path):
            with MIGRATION_CACHE_LOCK:
                if task_id in MIGRATION_CACHE:
                    MIGRATION_CACHE[task_id]["original_torrent_path"] = original_torrent_path
                    if torrent_dir:
                        MIGRATION_CACHE[task_id]["torrent_dir"] = torrent_dir

        if not original_torrent_path or not os.path.exists(original_torrent_path):
            raise Exception("åŸå§‹ç§å­æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨ã€‚")

        upload_data["torrent_dir"] = torrent_dir  # ç¡®ä¿ä¸Šä¼ å™¨èƒ½è·å–åˆ° torrent_dir
        result = migrator.publish_prepared_torrent(upload_data, original_torrent_path)

        # 3. å¦‚æœå‘å¸ƒæˆåŠŸï¼Œè‡ªåŠ¨æ·»åŠ åˆ°ä¸‹è½½å™¨
        if result.get("success") and result.get("url"):
            auto_add = data.get("auto_add_to_downloader", True)  # é»˜è®¤è‡ªåŠ¨æ·»åŠ 
            print(f"[ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒæˆåŠŸ, auto_add={auto_add}, url={result.get('url')}")

            if auto_add:
                config = config_manager.get()
                default_downloader = config.get("cross_seed", {}).get("default_downloader")

                downloader_id = data.get("downloaderId") or data.get("downloader_id")
                save_path = upload_data.get("save_path") or upload_data.get("savePath")
                print(
                    f"[ä¸‹è½½å™¨æ·»åŠ ] åˆå§‹å‚æ•°: downloader_id={downloader_id}, save_path={save_path}"
                )
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] é…ç½®çš„é»˜è®¤ä¸‹è½½å™¨: {default_downloader}")

                if default_downloader and default_downloader != "":
                    downloader_id = default_downloader
                    print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä½¿ç”¨é…ç½®çš„é»˜è®¤ä¸‹è½½å™¨: {downloader_id}")

                    if not save_path:
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] ç¼ºå°‘save_path,ä»æ•°æ®åº“è·å–æºç§å­çš„ä¿å­˜è·¯å¾„")
                        source_torrent_id = context.get("source_torrent_id")
                        if source_torrent_id and source_site_name:
                            torrent_name = get_seed_name(
                                db_manager, source_torrent_id, source_site_name
                            )
                            torrent_info = get_current_torrent_info(db_manager, torrent_name)
                            if torrent_info and torrent_info.get("save_path"):
                                save_path = torrent_info["save_path"]
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°ä¿å­˜è·¯å¾„: {save_path}")
                            else:
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¿å­˜è·¯å¾„")
                else:
                    print(f"[ä¸‹è½½å™¨æ·»åŠ ] é…ç½®ä¸ºä½¿ç”¨æºç§å­ä¸‹è½½å™¨,ä»æ•°æ®åº“æŸ¥è¯¢")
                    source_torrent_id = context.get("source_torrent_id")
                    if source_torrent_id and source_site_name:
                        torrent_name = get_seed_name(
                            db_manager, source_torrent_id, source_site_name
                        )
                        torrent_info = get_current_torrent_info(db_manager, torrent_name)
                        if torrent_info:
                            downloader_id = torrent_info.get("downloader_id")
                            if not save_path and torrent_info.get("save_path"):
                                save_path = torrent_info["save_path"]
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°ä¿å­˜è·¯å¾„: {save_path}")
                            print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°æºç§å­çš„ä¸‹è½½å™¨ID: {downloader_id}")
                        else:
                            print(f"[ä¸‹è½½å™¨æ·»åŠ ] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æºç§å­ä¿¡æ¯")

                    if not downloader_id:
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] æœªæ‰¾åˆ°æºç§å­çš„ä¸‹è½½å™¨ä¿¡æ¯")

                if save_path and downloader_id:
                    try:
                        print(
                            f"[ä¸‹è½½å™¨æ·»åŠ ] å‡†å¤‡åŒæ­¥æ·»åŠ åˆ°ä¸‹è½½å™¨: URL={result['url']}, Path={save_path}, DownloaderID={downloader_id}"
                        )
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] ç»“æœè¯¦æƒ…: {result}")
                        print(
                            f"[ä¸‹è½½å™¨æ·»åŠ ] ç›´æ¥ä¸‹è½½é“¾æ¥: {result.get('direct_download_url', 'None')}"
                        )

                        try:
                            from .internal_guard import check_downloader_gate

                            can_continue, limit_message = check_downloader_gate(downloader_id)

                            if not can_continue:
                                print(f"ğŸš« [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥è§¦å‘é™åˆ¶: {limit_message}")
                                result["auto_add_result"] = {
                                    "success": False,
                                    "message": limit_message,
                                    "sync": True,
                                    "downloader_id": None,
                                    "limit_reached": True,
                                    "pre_check": True,
                                }
                                return result, 200
                            else:
                                print(f"âœ… [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­æ·»åŠ ")
                        except Exception as e:
                            print(f"âš ï¸ [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: {e}")

                        success, message = add_torrent_to_downloader(
                            detail_page_url=result["url"],
                            save_path=save_path,
                            downloader_id=downloader_id,
                            db_manager=db_manager,
                            config_manager=config_manager,
                            direct_download_url=result.get("direct_download_url"),
                        )

                        limit_reached = success == "LIMIT_REACHED"

                        result["auto_add_result"] = {
                            "success": not limit_reached,
                            "message": message,
                            "sync": True,
                            "downloader_id": downloader_id if not limit_reached else None,
                            "limit_reached": limit_reached,
                        }

                    except Exception as e:
                        print(f"âŒ [ä¸‹è½½å™¨æ·»åŠ ] åŒæ­¥æ·»åŠ å¼‚å¸¸: {e}")
                        import traceback

                        traceback.print_exc()
                        result["auto_add_result"] = {
                            "success": False,
                            "message": f"æ·»åŠ åˆ°ä¸‹è½½å™¨å¤±è´¥: {str(e)}",
                        }
                else:
                    missing = []
                    if not save_path:
                        missing.append("save_path")
                    if not downloader_id:
                        missing.append("downloader_id")
                    print(f"âš ï¸ [ä¸‹è½½å™¨æ·»åŠ ] è·³è¿‡: ç¼ºå°‘å‚æ•° {', '.join(missing)}")
                    result["auto_add_result"] = {
                        "success": False,
                        "message": f"ç¼ºå°‘å¿…è¦å‚æ•°: {', '.join(missing)}",
                    }
            else:
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] auto_add=False, è·³è¿‡è‡ªåŠ¨æ·»åŠ ")

        # å¤„ç†æ‰¹é‡è½¬ç§è®°å½•ï¼ˆGo ç«¯æ‰¹é‡è½¬ç§è°ƒç”¨ /api/migrate/publish æ—¶ä¼šä¼  batch_idï¼‰
        batch_id = data.get("batch_id")  # Goç«¯ä¼ é€’çš„æ‰¹æ¬¡ID
        if batch_id:
            try:
                source_torrent_id = context.get("source_torrent_id")
                if source_torrent_id and source_site_name and target_site_name:
                    seed_title = "æœªçŸ¥æ ‡é¢˜"
                    try:
                        seed_param_model = SeedParameter(db_manager)
                        seed_parameters = seed_param_model.get_parameters(
                            source_torrent_id, source_site_name
                        )
                        if seed_parameters and seed_parameters.get("title"):
                            seed_title = seed_parameters.get("title")
                    except Exception:
                        pass

                    video_size_gb = data.get("video_size_gb")
                    progress = data.get("batch_progress")
                    status = "success" if result.get("success") else "failed"
                    success_url = result.get("url") if result.get("success") else None
                    error_detail = result.get("logs") if not result.get("success") else None
                    downloader_add_result = None
                    if result.get("auto_add_result"):
                        try:
                            downloader_add_result = json.dumps(
                                result.get("auto_add_result"), ensure_ascii=False
                            )
                        except Exception:
                            downloader_add_result = str(result.get("auto_add_result"))

                    source_site_for_record = data.get("nickname") or source_site_name

                    # å…¼å®¹ä¸åŒæ•°æ®åº“å‚æ•°å ä½ç¬¦ï¼ˆsqlite: ?, mysql/postgresql: %sï¼‰
                    try:
                        ph = db_manager.get_placeholder()
                    except Exception:
                        ph = "%s"
                    placeholders = ", ".join([ph] * 11)
                    insert_sql = f"""INSERT INTO batch_enhance_records
                        (batch_id, title, torrent_id, source_site, target_site, progress, video_size_gb, status, success_url, error_detail, downloader_add_result)
                        VALUES ({placeholders})"""

                    conn = None
                    cursor = None
                    try:
                        conn = db_manager._get_connection()
                        cursor = db_manager._get_cursor(conn)
                        cursor.execute(
                            insert_sql,
                            (
                                batch_id,
                                seed_title,
                                source_torrent_id,
                                source_site_for_record,
                                target_site_name,
                                progress,
                                video_size_gb,
                                status,
                                success_url,
                                error_detail,
                                downloader_add_result,
                            ),
                        )
                        conn.commit()
                    finally:
                        try:
                            if cursor:
                                cursor.close()
                        except Exception:
                            pass
                        try:
                            if conn:
                                conn.close()
                        except Exception:
                            pass
            except Exception:
                pass

        return result, 200

    except Exception as e:
        logging.error(f"migrate_publish to {target_site_name} å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return {"success": False, "logs": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}", "url": None}, 500
    finally:
        if migrator:
            migrator.cleanup()


@migrate_bp.route("/migrate/publish", methods=["POST"])
def migrate_publish():
    # æ–°å®ç°ï¼šå§”æ‰˜ç»™å¯å¤ç”¨çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¾¿äºæ‰¹é‡å¹¶å‘å‘å¸ƒä½¿ç”¨ã€‚
    result, status_code = _migrate_publish_impl(migrate_bp.db_manager, request.json)
    return jsonify(result), status_code

    db_manager = migrate_bp.db_manager
    data = request.json
    task_id, upload_data, target_site_name, source_site_name = (
        data.get("task_id"),
        data.get("upload_data"),
        data.get("targetSite"),
        data.get("sourceSite"),
    )

    if not task_id or task_id not in MIGRATION_CACHE:
        return jsonify({"success": False, "logs": "é”™è¯¯ï¼šæ— æ•ˆæˆ–å·²è¿‡æœŸçš„ä»»åŠ¡IDã€‚"}), 400

    if not target_site_name:
        return jsonify({"success": False, "logs": "é”™è¯¯ï¼šå¿…é¡»æä¾›ç›®æ ‡ç«™ç‚¹åç§°ã€‚"}), 400

    context = MIGRATION_CACHE[task_id]

    migrator = None  # ç¡®ä¿åœ¨ finally ä¸­å¯ç”¨

    try:
        # ğŸš« å‘å¸ƒå‰æ ‡ç­¾é™åˆ¶æ£€æŸ¥ï¼šç¦è½¬/é™è½¬/åˆ†é›†ç›´æ¥æ‹¦æˆª
        restricted_tag_map = {
            "ç¦è½¬": "tag.ç¦è½¬",
            "tag.ç¦è½¬": "tag.ç¦è½¬",
            "é™è½¬": "tag.é™è½¬",
            "tag.é™è½¬": "tag.é™è½¬",
            "åˆ†é›†": "tag.åˆ†é›†",
            "tag.åˆ†é›†": "tag.åˆ†é›†",
        }
        standardized_params = (upload_data or {}).get("standardized_params", {})
        raw_tags = (standardized_params.get("tags") or []) + (upload_data or {}).get("tags", [])
        restricted_tags = []
        for tag in raw_tags:
            mapped_tag = restricted_tag_map.get(tag)
            if mapped_tag and mapped_tag not in restricted_tags:
                restricted_tags.append(mapped_tag)

        if restricted_tags:
            return jsonify(
                {
                    "success": False,
                    "logs": f"ğŸš« å‘å¸ƒå‰æ ‡ç­¾é™åˆ¶: æ£€æµ‹åˆ°ç¦è½¬/é™è½¬/åˆ†é›†æ ‡ç­¾ {restricted_tags}",
                    "limit_reached": True,
                    "pre_check": True,
                }
            )

        target_info = db_manager.get_site_by_nickname(target_site_name)
        if not target_info:
            return (
                jsonify(
                    {"success": False, "logs": f"é”™è¯¯: ç›®æ ‡ç«™ç‚¹ '{target_site_name}' é…ç½®ä¸å®Œæ•´ã€‚"}
                ),
                404,
            )

        source_info = context["source_info"]
        original_torrent_path = context["original_torrent_path"]
        torrent_dir = context.get("torrent_dir", "")  # è·å–ç§å­ç›®å½•

        # ä»ç¼“å­˜ä¸­è·å–æºç«™ç‚¹åç§°ï¼ˆå¦‚æœå‰ç«¯æ²¡æœ‰ä¼ é€’ï¼‰
        if not source_site_name:
            source_site_name = context.get("source_site_name", "")

        # ğŸš« å‘å¸ƒå‰é¢„æ£€æŸ¥å‘ç§é™åˆ¶ - åœ¨ä»»ä½•å‘å¸ƒé€»è¾‘ä¹‹å‰è¿›è¡Œ
        downloader_id = data.get("downloaderId") or data.get("downloader_id")
        if downloader_id:
            try:
                from .internal_guard import check_downloader_gate

                can_continue, limit_message = check_downloader_gate(downloader_id)

                if not can_continue:
                    return jsonify(
                        {
                            "success": False,
                            "logs": f"ğŸš« å‘å¸ƒå‰é¢„æ£€æŸ¥è§¦å‘é™åˆ¶: {limit_message}",
                            "limit_reached": True,
                            "pre_check": True,
                        }
                    )
                else:
                    print(f"âœ… [å‘å¸ƒå‰é¢„æ£€æŸ¥] é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­å‘å¸ƒåˆ° {target_site_name}")
            except Exception as e:
                print(f"âš ï¸ [å‘å¸ƒå‰é¢„æ£€æŸ¥] æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: {e}")

        # åˆ›å»º TorrentMigrator å®ä¾‹ç”¨äºå‘å¸ƒ
        migrator = TorrentMigrator(
            source_info,
            target_info,
            search_term=context.get("source_torrent_id", ""),
            save_path=upload_data.get("save_path", "") or upload_data.get("savePath", ""),
            config_manager=config_manager,
            db_manager=db_manager,
        )

        # æ£€æŸ¥ç§å­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°ä¸‹è½½
        # [æ ¸å¿ƒä¿®æ”¹] ä¼˜å…ˆåœ¨ç»Ÿä¸€çš„ torrents ç›®å½•ä¸­æŸ¥æ‰¾
        source_torrent_id = context.get("source_torrent_id", "")
        source_site_code = source_info.get("site", source_site_name.lower())

        if original_torrent_path is None or not os.path.exists(original_torrent_path):
            logging.info("åŸå§‹ç§å­æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œå¼€å§‹åœ¨ç»Ÿä¸€ç›®å½•ä¸­æŸ¥æ‰¾")

            from config import TEMP_DIR

            torrents_dir = os.path.join(TEMP_DIR, "torrents")

            # [æ–°å¢] é¦–å…ˆåœ¨ç»Ÿä¸€çš„ torrents ç›®å½•ä¸­æŸ¥æ‰¾ä»¥"ç«™ç‚¹-ID-"å¼€å¤´çš„ç§å­æ–‡ä»¶
            if os.path.exists(torrents_dir) and source_torrent_id:
                prefix = f"{source_site_code}-{source_torrent_id}-"
                logging.info(f"åœ¨ç»Ÿä¸€ç›®å½•ä¸­æŸ¥æ‰¾ç§å­æ–‡ä»¶ï¼Œå‰ç¼€: {prefix}")

                try:
                    for file in os.listdir(torrents_dir):
                        if file.startswith(prefix) and file.endswith(".torrent"):
                            original_torrent_path = os.path.join(torrents_dir, file)
                            logging.info(f"âœ… åœ¨ç»Ÿä¸€ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {file}")
                            break
                except Exception as e:
                    logging.warning(f"éå†ç»Ÿä¸€ç›®å½•æ—¶å‡ºé”™: {e}")

            # å¦‚æœåœ¨ç»Ÿä¸€ç›®å½•ä¸­æ²¡æ‰¾åˆ°ï¼Œå†æ£€æŸ¥æ—§æ ¼å¼ç›®å½•
            if (
                original_torrent_path is None or not os.path.exists(original_torrent_path)
            ) and source_torrent_id:
                logging.info("ç»Ÿä¸€ç›®å½•ä¸­æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥æ—§æ ¼å¼ç›®å½•")
                # æ£€æŸ¥æ—§æ ¼å¼ç›®å½•
                old_torrent_dir = os.path.join(TEMP_DIR, f"torrent_{source_torrent_id}")
                if os.path.exists(old_torrent_dir):
                    try:
                        # æŸ¥æ‰¾old_torrent_dirä¸­çš„.torrentæ–‡ä»¶
                        for file in os.listdir(old_torrent_dir):
                            if file.endswith(".torrent"):
                                original_torrent_path = os.path.join(old_torrent_dir, file)
                                logging.info(
                                    f"åœ¨æ—§æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                )
                                break
                    except Exception as e:
                        logging.warning(f"æŸ¥æ‰¾æ—§æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                # å¦‚æœåœ¨æ—§æ ¼å¼ç›®å½•ä¸­æ²¡æ‰¾åˆ°ï¼Œæ£€æŸ¥ä»¥ç§å­åç§°å‘½åçš„ç›®å½•ï¼ˆæ–°æ ¼å¼ï¼‰
                if original_torrent_path is None or not os.path.exists(original_torrent_path):
                    # å°è¯•ä»ç¼“å­˜ä¸­è·å–ç§å­ç›®å½•è·¯å¾„
                    cached_torrent_dir = context.get("torrent_dir")
                    if cached_torrent_dir and os.path.exists(cached_torrent_dir):
                        try:
                            # æŸ¥æ‰¾cached_torrent_dirä¸­çš„.torrentæ–‡ä»¶
                            for file in os.listdir(cached_torrent_dir):
                                if file.endswith(".torrent"):
                                    original_torrent_path = os.path.join(cached_torrent_dir, file)
                                    logging.info(
                                        f"åœ¨æ–°æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                    )
                                    break
                        except Exception as e:
                            logging.warning(f"æŸ¥æ‰¾æ–°æ ¼å¼ä¸´æ—¶ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    else:
                        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰torrent_diræˆ–ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•é‡æ„è·¯å¾„
                        # ä½¿ç”¨ç§å­IDä»æ•°æ®åº“è·å–ç§å­ä¿¡æ¯ï¼Œç„¶åé‡å»ºç›®å½•è·¯å¾„
                        try:
                            from models.seed_parameter import SeedParameter
                            from flask import current_app

                            db_manager = current_app.config["DB_MANAGER"]
                            seed_param_model = SeedParameter(db_manager)

                            source_torrent_id = context.get("source_torrent_id", "")
                            source_site_name = context.get("source_site_name", "")

                            if source_torrent_id and source_site_name:
                                # ä»æ•°æ®åº“è·å–ç§å­å‚æ•°
                                parameters = seed_param_model.get_parameters(
                                    source_torrent_id, source_site_name
                                )
                                if parameters and parameters.get("title"):
                                    # é‡å»ºç§å­ç›®å½•è·¯å¾„
                                    from config import TEMP_DIR
                                    import re

                                    original_main_title = parameters.get("title", "")
                                    safe_filename_base = re.sub(
                                        r'[\\/*?:"<>|]', "_", original_main_title
                                    )[:150]
                                    reconstructed_torrent_dir = os.path.join(
                                        TEMP_DIR, safe_filename_base
                                    )

                                    if os.path.exists(reconstructed_torrent_dir):
                                        try:
                                            # æŸ¥æ‰¾reconstructed_torrent_dirä¸­çš„.torrentæ–‡ä»¶
                                            for file in os.listdir(reconstructed_torrent_dir):
                                                if file.endswith(".torrent"):
                                                    original_torrent_path = os.path.join(
                                                        reconstructed_torrent_dir, file
                                                    )
                                                    logging.info(
                                                        f"åœ¨é‡æ„çš„ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                                    )
                                                    break
                                        except Exception as e:
                                            logging.warning(f"æŸ¥æ‰¾é‡æ„ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                        except Exception as e:
                            logging.warning(f"å°è¯•é‡æ„ç§å­ç›®å½•è·¯å¾„æ—¶å‡ºé”™: {e}")

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œç›´æ¥åœ¨TEMP_DIRä¸­æŸ¥æ‰¾ä»¥ç§å­åç§°å‘½åçš„ç›®å½•
            if original_torrent_path is None or not os.path.exists(original_torrent_path):
                try:
                    # ä»æ•°æ®åº“è·å–ç§å­å‚æ•°æ¥ç¡®å®šç›®å½•å
                    from models.seed_parameter import SeedParameter
                    from flask import current_app

                    db_manager = current_app.config["DB_MANAGER"]
                    seed_param_model = SeedParameter(db_manager)

                    source_torrent_id = context.get("source_torrent_id", "")
                    source_site_name = context.get("source_site_name", "")

                    if source_torrent_id and source_site_name:
                        # ä»æ•°æ®åº“è·å–ç§å­å‚æ•°
                        parameters = seed_param_model.get_parameters(
                            source_torrent_id, source_site_name
                        )
                        if parameters and parameters.get("title"):
                            # é‡å»ºç§å­ç›®å½•è·¯å¾„
                            from config import TEMP_DIR
                            import re

                            original_main_title = parameters.get("title", "")
                            safe_filename_base = re.sub(r'[\\/*?:"<>|]', "_", original_main_title)[
                                :150
                            ]
                            seed_name_dir = os.path.join(TEMP_DIR, safe_filename_base)

                            # åœ¨è¯¥ç›®å½•ä¸­æŸ¥æ‰¾.torrentæ–‡ä»¶
                            if os.path.exists(seed_name_dir):
                                for file in os.listdir(seed_name_dir):
                                    if file.endswith(".torrent"):
                                        original_torrent_path = os.path.join(seed_name_dir, file)
                                        logging.info(
                                            f"åœ¨ç§å­åç§°ç›®å½•ä¸­æ‰¾åˆ°ç§å­æ–‡ä»¶: {original_torrent_path}"
                                        )
                                        break
                except Exception as e:
                    logging.warning(f"æŸ¥æ‰¾ç§å­åç§°ç›®å½•ä¸­çš„ç§å­æ–‡ä»¶æ—¶å‡ºé”™: {e}")

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç§å­æ–‡ä»¶ï¼Œåˆ™é‡æ–°ä¸‹è½½
            if original_torrent_path is None or not os.path.exists(original_torrent_path):
                logging.info("éœ€è¦é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶")
                # é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶
                try:
                    import cloudscraper
                    import re
                    from config import TEMP_DIR

                    # åˆå§‹åŒ–scraper
                    session = requests.Session()
                    session.verify = False
                    scraper = cloudscraper.create_scraper(sess=session)

                    # æ„é€ ä¸‹è½½é“¾æ¥
                    SOURCE_BASE_URL = source_info.get("base_url", "").rstrip("/")
                    # ç¡®ä¿URLæœ‰æ­£ç¡®çš„åè®®å‰ç¼€
                    if SOURCE_BASE_URL and not SOURCE_BASE_URL.startswith(("http://", "https://")):
                        SOURCE_BASE_URL = "https://" + SOURCE_BASE_URL
                    SOURCE_COOKIE = source_info.get("cookie", "")
                    source_torrent_id = context.get("source_torrent_id", "")

                    if SOURCE_BASE_URL and SOURCE_COOKIE and source_torrent_id:
                        # è·å–è¯¦æƒ…é¡µä»¥æ‰¾åˆ°ä¸‹è½½é“¾æ¥
                        response = scraper.get(
                            f"{SOURCE_BASE_URL}/details.php",
                            headers={
                                "Cookie": SOURCE_COOKIE,
                                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                            },
                            params={"id": source_torrent_id, "hit": "1"},
                            timeout=180,
                        )
                        response.raise_for_status()
                        response.encoding = "utf-8"

                        from bs4 import BeautifulSoup

                        soup = BeautifulSoup(response.text, "html.parser")
                        download_link_tag = soup.select_one(
                            f'a.index[href^="download.php?id={source_torrent_id}"]'
                        )

                        if download_link_tag:
                            # ä¸‹è½½ç§å­æ–‡ä»¶
                            torrent_response = scraper.get(
                                f"{SOURCE_BASE_URL}/{download_link_tag['href']}",
                                headers={
                                    "Cookie": SOURCE_COOKIE,
                                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                                },
                                timeout=180,
                            )
                            torrent_response.raise_for_status()

                            # ä»å“åº”å¤´ä¸­å°è¯•è·å–æ–‡ä»¶å
                            content_disposition = torrent_response.headers.get(
                                "content-disposition"
                            )
                            torrent_filename = "unknown.torrent"
                            if content_disposition:
                                # å°è¯•åŒ¹é…filename*ï¼ˆæ”¯æŒUTF-8ç¼–ç ï¼‰å’Œfilename
                                filename_match = re.search(
                                    r'filename\*="?UTF-8\'\'([^"]+)"?',
                                    content_disposition,
                                    re.IGNORECASE,
                                )
                                if filename_match:
                                    torrent_filename = filename_match.group(1)
                                    # URLè§£ç æ–‡ä»¶åï¼ˆUTF-8ç¼–ç ï¼‰
                                    torrent_filename = urllib.parse.unquote(
                                        torrent_filename, encoding="utf-8"
                                    )
                                else:
                                    # å°è¯•åŒ¹é…æ™®é€šçš„filename
                                    filename_match = re.search(
                                        r'filename="?([^"]+)"?', content_disposition
                                    )
                                    if filename_match:
                                        torrent_filename = filename_match.group(1)
                                        # URLè§£ç æ–‡ä»¶å
                                        torrent_filename = urllib.parse.unquote(torrent_filename)

                            # ä½¿ç”¨ç»Ÿä¸€çš„ç§å­ç›®å½•
                            from config import TEMP_DIR

                            torrent_dir = os.path.join(TEMP_DIR, "torrents")
                            os.makedirs(torrent_dir, exist_ok=True)

                            # è·å–ç«™ç‚¹ä»£ç ç”¨äºæ–‡ä»¶åå‰ç¼€
                            source_site_code = source_info.get("site", source_site_name.lower())

                            # ä¿å­˜ç§å­æ–‡ä»¶ï¼Œæ·»åŠ ç«™ç‚¹-ID-å‰ç¼€
                            try:
                                # å¯¹æ–‡ä»¶åè¿›è¡Œæ–‡ä»¶ç³»ç»Ÿå®‰å…¨çš„å¤„ç†
                                safe_filename = torrent_filename
                                # ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
                                safe_filename = re.sub(r'[<>:"/\\|?*]', "_", safe_filename)
                                # ç¡®ä¿æ–‡ä»¶åä¸è¶…è¿‡æ–‡ä»¶ç³»ç»Ÿé™åˆ¶
                                if len(safe_filename.encode("utf-8")) > 255:
                                    # å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªæ–­å¹¶ä¿æŒæ‰©å±•å
                                    name, ext = os.path.splitext(safe_filename)
                                    max_len = 255 - len(ext.encode("utf-8"))
                                    safe_filename = (
                                        name.encode("utf-8")[:max_len].decode("utf-8", "ignore")
                                        + ext
                                    )

                                # æ·»åŠ ç«™ç‚¹-ID-å‰ç¼€
                                prefixed_filename = (
                                    f"{source_site_code}-{source_torrent_id}-{safe_filename}"
                                )
                                original_torrent_path = os.path.join(
                                    torrent_dir, prefixed_filename
                                )
                                with open(original_torrent_path, "wb") as f:
                                    f.write(torrent_response.content)
                                logging.info(f"ç§å­æ–‡ä»¶å·²ä¿å­˜: {prefixed_filename}")
                            except OSError as e:
                                # å¦‚æœæ–‡ä»¶åæœ‰é—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤åç§°
                                logging.warning(f"ä½¿ç”¨åŸå§‹æ–‡ä»¶åä¿å­˜å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤åç§°")
                                prefixed_filename = (
                                    f"{source_site_code}-{source_torrent_id}-torrent.torrent"
                                )
                                original_torrent_path = os.path.join(
                                    torrent_dir, prefixed_filename
                                )
                                with open(original_torrent_path, "wb") as f:
                                    f.write(torrent_response.content)

                            logging.info(f"é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶æˆåŠŸ: {original_torrent_path}")
                        else:
                            logging.error("æœªæ‰¾åˆ°ç§å­ä¸‹è½½é“¾æ¥")
                            return (
                                jsonify({"success": False, "logs": "é”™è¯¯ï¼šæœªæ‰¾åˆ°ç§å­ä¸‹è½½é“¾æ¥ã€‚"}),
                                404,
                            )
                    else:
                        logging.error("ç¼ºå°‘å¿…è¦çš„æºç«™ç‚¹ä¿¡æ¯")
                        return (
                            jsonify({"success": False, "logs": "é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„æºç«™ç‚¹ä¿¡æ¯ã€‚"}),
                            400,
                        )
                except Exception as e:
                    logging.error(f"é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}")
                    return jsonify({"success": False, "logs": f"é‡æ–°ä¸‹è½½ç§å­æ–‡ä»¶å¤±è´¥: {e}"}), 500

        # 1. ç›´æ¥ä½¿ç”¨åŸå§‹ç§å­æ–‡ä»¶è·¯å¾„è¿›è¡Œå‘å¸ƒï¼ˆä¸å†ä¿®æ”¹ç§å­ï¼‰
        if not original_torrent_path or not os.path.exists(original_torrent_path):
            raise Exception("åŸå§‹ç§å­æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨ã€‚")

        # 2. å‘å¸ƒ (ä¼ é€’ torrent_dir ç»™ä¸Šä¼ å™¨)
        upload_data["torrent_dir"] = torrent_dir  # ç¡®ä¿ä¸Šä¼ å™¨èƒ½è·å–åˆ° torrent_dir
        result = migrator.publish_prepared_torrent(upload_data, original_torrent_path)

        # 3. å¦‚æœå‘å¸ƒæˆåŠŸï¼Œè‡ªåŠ¨æ·»åŠ åˆ°ä¸‹è½½å™¨
        if result.get("success") and result.get("url"):
            auto_add = data.get("auto_add_to_downloader", True)  # é»˜è®¤è‡ªåŠ¨æ·»åŠ 
            print(f"[ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒæˆåŠŸ, auto_add={auto_add}, url={result.get('url')}")

            if auto_add:
                # å…ˆè·å–é…ç½®çš„é»˜è®¤ä¸‹è½½å™¨
                config = config_manager.get()
                default_downloader = config.get("cross_seed", {}).get("default_downloader")

                # ä»è¯·æ±‚ä¸­è·å–ä¸‹è½½å™¨IDå’Œä¿å­˜è·¯å¾„
                downloader_id = data.get("downloaderId") or data.get("downloader_id")
                save_path = upload_data.get("save_path") or upload_data.get("savePath")
                print(
                    f"[ä¸‹è½½å™¨æ·»åŠ ] åˆå§‹å‚æ•°: downloader_id={downloader_id}, save_path={save_path}"
                )
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] é…ç½®çš„é»˜è®¤ä¸‹è½½å™¨: {default_downloader}")

                # åˆ¤æ–­é€»è¾‘:
                # 1. å¦‚æœé…ç½®äº†å…·ä½“çš„é»˜è®¤ä¸‹è½½å™¨(éç©ºéNone),ä½¿ç”¨å®ƒ
                # 2. å¦‚æœé…ç½®ä¸ºç©ºæˆ–"ä½¿ç”¨æºç§å­ä¸‹è½½å™¨",åˆ™ä»æ•°æ®åº“æŸ¥è¯¢æºç§å­çš„ä¸‹è½½å™¨
                # 3. æ— è®ºå“ªç§æƒ…å†µ,å¦‚æœç¼ºå°‘save_path,éƒ½å°è¯•ä»æ•°æ®åº“è·å–

                if default_downloader and default_downloader != "":
                    # é…ç½®äº†å…·ä½“çš„ä¸‹è½½å™¨,ä½¿ç”¨é…ç½®çš„ä¸‹è½½å™¨
                    downloader_id = default_downloader
                    print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä½¿ç”¨é…ç½®çš„é»˜è®¤ä¸‹è½½å™¨: {downloader_id}")

                    # å¦‚æœæ²¡æœ‰save_path,å°è¯•ä»æ•°æ®åº“è·å–æºç§å­çš„save_path
                    if not save_path:
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] ç¼ºå°‘save_path,ä»æ•°æ®åº“è·å–æºç§å­çš„ä¿å­˜è·¯å¾„")
                        source_torrent_id = context.get("source_torrent_id")
                        if source_torrent_id and source_site_name:
                            torrent_name = get_seed_name(
                                db_manager, source_torrent_id, source_site_name
                            )
                            torrent_info = get_current_torrent_info(db_manager, torrent_name)
                            if torrent_info and torrent_info.get("save_path"):
                                save_path = torrent_info["save_path"]
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°ä¿å­˜è·¯å¾„: {save_path}")
                            else:
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¿å­˜è·¯å¾„")
                else:
                    # é…ç½®ä¸º"ä½¿ç”¨æºç§å­æ‰€åœ¨çš„ä¸‹è½½å™¨"æˆ–æœªé…ç½®
                    # å°è¯•ä»æ•°æ®åº“æŸ¥è¯¢åŸå§‹ç§å­çš„ä¸‹è½½å™¨å’Œä¿å­˜è·¯å¾„
                    print(f"[ä¸‹è½½å™¨æ·»åŠ ] é…ç½®ä¸ºä½¿ç”¨æºç§å­ä¸‹è½½å™¨,ä»æ•°æ®åº“æŸ¥è¯¢")
                    source_torrent_id = context.get("source_torrent_id")
                    if source_torrent_id and source_site_name:
                        torrent_name = get_seed_name(
                            db_manager, source_torrent_id, source_site_name
                        )
                        torrent_info = get_current_torrent_info(db_manager, torrent_name)
                        if torrent_info:
                            downloader_id = torrent_info.get("downloader_id")
                            if not save_path and torrent_info.get("save_path"):
                                save_path = torrent_info["save_path"]
                                print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°ä¿å­˜è·¯å¾„: {save_path}")
                            print(f"[ä¸‹è½½å™¨æ·»åŠ ] ä»æ•°æ®åº“è·å–åˆ°æºç§å­çš„ä¸‹è½½å™¨ID: {downloader_id}")
                        else:
                            print(f"[ä¸‹è½½å™¨æ·»åŠ ] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æºç§å­ä¿¡æ¯")

                    if not downloader_id:
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] æœªæ‰¾åˆ°æºç§å­çš„ä¸‹è½½å™¨ä¿¡æ¯")

                # è°ƒç”¨æ·»åŠ åˆ°ä¸‹è½½å™¨
                if save_path and downloader_id:
                    try:
                        print(
                            f"[ä¸‹è½½å™¨æ·»åŠ ] å‡†å¤‡åŒæ­¥æ·»åŠ åˆ°ä¸‹è½½å™¨: URL={result['url']}, Path={save_path}, DownloaderID={downloader_id}"
                        )
                        print(f"[ä¸‹è½½å™¨æ·»åŠ ] ç»“æœè¯¦æƒ…: {result}")
                        print(
                            f"[ä¸‹è½½å™¨æ·»åŠ ] ç›´æ¥ä¸‹è½½é“¾æ¥: {result.get('direct_download_url', 'None')}"
                        )

                        # ğŸš« å‘å¸ƒå‰é¢„æ£€æŸ¥å‘ç§é™åˆ¶
                        try:
                            from .internal_guard import check_downloader_gate

                            can_continue, limit_message = check_downloader_gate(downloader_id)

                            if not can_continue:
                                print(f"ğŸš« [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥è§¦å‘é™åˆ¶: {limit_message}")
                                result["auto_add_result"] = {
                                    "success": False,
                                    "message": limit_message,
                                    "sync": True,
                                    "downloader_id": None,
                                    "limit_reached": True,
                                    "pre_check": True,
                                }
                                return jsonify(result)
                            else:
                                print(f"âœ… [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­æ·»åŠ ")
                        except Exception as e:
                            print(f"âš ï¸ [ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå‰é¢„æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ: {e}")

                        # åŒæ­¥è°ƒç”¨ add_torrent_to_downloader å‡½æ•°
                        success, message = add_torrent_to_downloader(
                            detail_page_url=result["url"],
                            save_path=save_path,
                            downloader_id=downloader_id,
                            db_manager=db_manager,
                            config_manager=config_manager,
                            direct_download_url=result.get("direct_download_url"),
                        )

                        # æ£€æŸ¥æ˜¯å¦è§¦å‘å‘ç§é™åˆ¶
                        limit_reached = success == "LIMIT_REACHED"

                        result["auto_add_result"] = {
                            "success": not limit_reached,  # é™åˆ¶è§¦å‘æ—¶è§†ä¸ºå¤±è´¥
                            "message": message,
                            "sync": True,
                            "downloader_id": downloader_id if not limit_reached else None,
                            "limit_reached": limit_reached,
                        }

                        if not limit_reached and success:
                            print(f"âœ… [ä¸‹è½½å™¨æ·»åŠ ] åŒæ­¥æ·»åŠ æˆåŠŸ: {message}")
                        elif limit_reached:
                            print(f"ğŸš« [ä¸‹è½½å™¨æ·»åŠ ] åŒæ­¥æ·»åŠ è¢«é™åˆ¶: {message}")
                        else:
                            print(f"âŒ [ä¸‹è½½å™¨æ·»åŠ ] åŒæ­¥æ·»åŠ å¤±è´¥: {message}")

                    except Exception as e:
                        print(f"âŒ [ä¸‹è½½å™¨æ·»åŠ ] åŒæ­¥æ·»åŠ å¼‚å¸¸: {e}")
                        import traceback

                        traceback.print_exc()
                        result["auto_add_result"] = {
                            "success": False,
                            "message": f"æ·»åŠ åˆ°ä¸‹è½½å™¨å¤±è´¥: {str(e)}",
                        }
                else:
                    missing = []
                    if not save_path:
                        missing.append("save_path")
                    if not downloader_id:
                        missing.append("downloader_id")
                    print(f"âš ï¸ [ä¸‹è½½å™¨æ·»åŠ ] è·³è¿‡: ç¼ºå°‘å‚æ•° {', '.join(missing)}")
                    result["auto_add_result"] = {
                        "success": False,
                        "message": f"ç¼ºå°‘å¿…è¦å‚æ•°: {', '.join(missing)}",
                    }
            else:
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] auto_add=False, è·³è¿‡è‡ªåŠ¨æ·»åŠ ")
        else:
            if not result.get("success"):
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒå¤±è´¥,è·³è¿‡ä¸‹è½½å™¨æ·»åŠ ")
            elif not result.get("url"):
                print(f"[ä¸‹è½½å™¨æ·»åŠ ] å‘å¸ƒæˆåŠŸä½†æœªè¿”å›URL,è·³è¿‡ä¸‹è½½å™¨æ·»åŠ ")

        # å¤„ç†æ‰¹é‡è½¬ç§è®°å½• - åˆ›å»º batch_enhance_records è¡¨è®°å½•
        batch_id = data.get("batch_id")  # Goç«¯ä¼ é€’çš„æ‰¹æ¬¡ID
        print(f"\n{'='*80}")
        print(f"[æ‰¹é‡è½¬ç§è®°å½•] æ£€æµ‹åˆ°batch_idå‚æ•°: {batch_id}")

        if batch_id:
            try:
                # ä» context ä¸­è·å–ç§å­ä¿¡æ¯
                source_torrent_id = context.get("source_torrent_id")
                print(
                    f"[æ‰¹é‡è½¬ç§è®°å½•] ç§å­ä¿¡æ¯: torrent_id={source_torrent_id}, source_site={source_site_name}, target_site={target_site_name}"
                )

                if source_torrent_id and source_site_name and target_site_name:
                    # [ä¿®å¤] ä»æ•°æ®åº“è·å–ç§å­æ ‡é¢˜
                    seed_title = "æœªçŸ¥æ ‡é¢˜"
                    try:
                        from models.seed_parameter import SeedParameter

                        seed_param_model = SeedParameter(db_manager)
                        seed_parameters = seed_param_model.get_parameters(
                            source_torrent_id, source_site_name
                        )
                        if seed_parameters and seed_parameters.get("title"):
                            seed_title = seed_parameters.get("title")
                            print(f"[æ‰¹é‡è½¬ç§è®°å½•] ä»æ•°æ®åº“è·å–åˆ°ç§å­æ ‡é¢˜: {seed_title}")
                        else:
                            print(f"[æ‰¹é‡è½¬ç§è®°å½•] âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç§å­æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    except Exception as e:
                        print(f"[æ‰¹é‡è½¬ç§è®°å½•] âš ï¸ æŸ¥è¯¢ç§å­æ ‡é¢˜å¤±è´¥: {e}")

                    conn = db_manager._get_connection()
                    cursor = db_manager._get_cursor(conn)

                    # å‡†å¤‡è®°å½•æ•°æ®
                    video_size_gb = data.get("video_size_gb")  # Goç«¯å¯èƒ½ä¼ é€’çš„è§†é¢‘å¤§å°
                    status = "success" if result.get("success") else "failed"
                    success_url = result.get("url") if result.get("success") else None
                    error_detail = result.get("logs") if not result.get("success") else None

                    print(
                        f"[æ‰¹é‡è½¬ç§è®°å½•] å‘å¸ƒç»“æœ: status={status}, success_url={success_url}, video_size_gb={video_size_gb}"
                    )

                    # ç”Ÿæˆä¸‹è½½å™¨æ·»åŠ ç»“æœæ–‡æœ¬
                    downloader_result = None
                    if "auto_add_result" in result:
                        auto_result = result["auto_add_result"]
                        print(f"[æ‰¹é‡è½¬ç§è®°å½•] ä¸‹è½½å™¨æ·»åŠ ç»“æœ: {auto_result}")
                        if auto_result["success"]:
                            downloader_result = f"æˆåŠŸ: {auto_result['message']}"
                        else:
                            downloader_result = f"å¤±è´¥: {auto_result['message']}"
                    else:
                        print(f"[æ‰¹é‡è½¬ç§è®°å½•] âš ï¸ æœªæ‰¾åˆ°auto_add_resultå­—æ®µ")
                        print(f"[æ‰¹é‡è½¬ç§è®°å½•] resultæ‰€æœ‰é”®: {list(result.keys())}")
                        print(f"[æ‰¹é‡è½¬ç§è®°å½•] resultå®Œæ•´å†…å®¹: {result}")

                    print(f"[æ‰¹é‡è½¬ç§è®°å½•] å‡†å¤‡å†™å…¥æ•°æ®åº“: downloader_result={downloader_result}")

                    # ç›´æ¥æ’å…¥è®°å½•(å‘å¸ƒçš„ç§å­ä¸ä¼šå…ˆè¢«Goç«¯æ’å…¥,åªæœ‰è¿‡æ»¤çš„ç§å­æ‰ä¼š)
                    if db_manager.db_type == "mysql":
                        insert_sql = """INSERT INTO batch_enhance_records
                                      (batch_id, title, torrent_id, source_site, target_site, progress, video_size_gb, status, success_url, error_detail, downloader_add_result)
                                      VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)"""
                    elif db_manager.db_type == "postgresql":
                        insert_sql = """INSERT INTO batch_enhance_records
                                      (batch_id, title, torrent_id, source_site, target_site, progress, video_size_gb, status, success_url, error_detail, downloader_add_result)
                                      VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)"""
                    else:  # sqlite
                        insert_sql = """INSERT INTO batch_enhance_records
                                      (batch_id, title, torrent_id, source_site, target_site, progress, video_size_gb, status, success_url, error_detail, downloader_add_result)
                                      VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"""

                    print(
                        f"[æ‰¹é‡è½¬ç§è®°å½•] æ‰§è¡ŒSQLæ’å…¥: batch_id={batch_id}, torrent_id={source_torrent_id}, title={seed_title}"
                    )
                    cursor.execute(
                        insert_sql,
                        (
                            batch_id,
                            seed_title,
                            source_torrent_id,
                            data.get("nickname", source_site_name),
                            target_site_name,
                            data.get("batch_progress"),
                            video_size_gb,
                            status,
                            success_url,
                            error_detail,
                            downloader_result,
                        ),
                    )
                    conn.commit()
                    cursor.close()
                    conn.close()

                    print(
                        f"âœ… [æ‰¹é‡è½¬ç§è®°å½•] æˆåŠŸå†™å…¥æ•°æ®åº“: {source_torrent_id} -> {target_site_name}"
                    )
                    print(f"   çŠ¶æ€: {status}")
                    print(f"   ä¸‹è½½å™¨ç»“æœ: {downloader_result}")
                else:
                    print(f"âŒ [æ‰¹é‡è½¬ç§è®°å½•] ç¼ºå°‘å¿…è¦çš„ç§å­ä¿¡æ¯:")
                    print(f"   torrent_id={source_torrent_id}")
                    print(f"   source_site={source_site_name}")
                    print(f"   target_site={target_site_name}")

            except Exception as e:
                print(f"âŒ [æ‰¹é‡è½¬ç§è®°å½•] è®°å½•æ—¶å‡ºé”™: {e}")
                import traceback

                traceback.print_exc()
        else:
            print(f"[æ‰¹é‡è½¬ç§è®°å½•] æœªæ£€æµ‹åˆ°batch_idå‚æ•°,è·³è¿‡è®°å½•åˆ°batch_enhance_recordsè¡¨")

        print(f"{'='*80}\n")

        return jsonify(result)

    except Exception as e:
        logging.error(f"migrate_publish to {target_site_name} å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "logs": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}", "url": None}), 500
    finally:
        if migrator:
            migrator.cleanup()
        # æ³¨æ„ï¼šæ­¤å¤„ä¸åˆ é™¤ MIGRATION_CACHE[task_id]ï¼Œå› ä¸ºå®ƒå¯èƒ½è¢«ç”¨äºå‘å¸ƒåˆ°å…¶ä»–ç«™ç‚¹ã€‚
        # å»ºè®®è®¾ç½®ä¸€ä¸ªç‹¬ç«‹çš„å®šæ—¶ä»»åŠ¡æ¥æ¸…ç†è¿‡æœŸçš„ç¼“å­˜ã€‚


# ===================================================================
#                    æ‰¹é‡å‘å¸ƒç§å­ API
# ===================================================================

BATCH_PUBLISH_TASKS = {}
BATCH_PUBLISH_LOCK = threading.Lock()
BATCH_PUBLISH_MAX_CONCURRENCY = 200
BATCH_PUBLISH_DEFAULT_CONCURRENCY = 5


def _batch_publish_emit_event(batch_id: str, payload: dict):
    stream = log_streamer.get_stream(batch_id)
    if not stream:
        stream = log_streamer.create_stream(batch_id)
    payload = payload or {}
    payload.setdefault("batch_id", batch_id)
    payload.setdefault("timestamp", time.time())
    try:
        stream.put(payload, block=False)
    except queue.Full:
        logging.warning(f"æ‰¹é‡å‘å¸ƒäº‹ä»¶é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæ¶ˆæ¯: {batch_id}")


def _batch_publish_get_public_task_state(batch_id: str) -> dict | None:
    with BATCH_PUBLISH_LOCK:
        task = BATCH_PUBLISH_TASKS.get(batch_id)
        if not task:
            return None

        # è¿”å›å¯åºåˆ—åŒ–çš„å…¬å…±å­—æ®µ
        return {
            "batch_id": batch_id,
            "task_id": task.get("task_id"),
            "total": task.get("total", 0),
            "processed": task.get("processed", 0),
            "success": task.get("success", 0),
            "failed": task.get("failed", 0),
            "isRunning": task.get("isRunning", False),
            "concurrency": task.get("concurrency", 1),
            "stop_reason": task.get("stop_reason"),
            "stop_message": task.get("stop_message"),
            "created_at": task.get("created_at"),
            "finished_at": task.get("finished_at"),
            "site_states": task.get("site_states", {}),
            "results": task.get("results", {}),
        }


def _process_publish_batch(
    *,
    batch_id: str,
    task_id: str,
    upload_data: dict,
    target_sites: list[str],
    source_site_name: str | None,
    downloader_id: str | None,
    auto_add_to_downloader: bool,
    concurrency: int,
    db_manager,
):
    # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
    with BATCH_PUBLISH_LOCK:
        task = BATCH_PUBLISH_TASKS.get(batch_id)
        if not task:
            return
        task["isRunning"] = True
        task["processed"] = 0
        task["success"] = 0
        task["failed"] = 0
        task["stop_reason"] = None
        task["stop_message"] = None
        task["site_states"] = {site: "queued" for site in target_sites}
        task["results"] = {}

    stop_event = threading.Event()
    state_lock = threading.Lock()

    _batch_publish_emit_event(
        batch_id,
        {
            "type": "batch_started",
            "total": len(target_sites),
            "concurrency": concurrency,
            "sites": target_sites,
        },
    )

    site_queue: queue.Queue[str] = queue.Queue()
    for site in target_sites:
        site_queue.put(site)

    def should_stop() -> bool:
        if stop_event.is_set():
            return True
        with BATCH_PUBLISH_LOCK:
            t = BATCH_PUBLISH_TASKS.get(batch_id)
            if not t:
                return True
            if t.get("cancel_requested"):
                stop_event.set()
                if not t.get("stop_reason"):
                    t["stop_reason"] = "cancelled"
                    t["stop_message"] = "ç”¨æˆ·å·²å–æ¶ˆæ‰¹é‡å‘å¸ƒ"
                return True
        return False

    def mark_stop(reason: str, message: str):
        with BATCH_PUBLISH_LOCK:
            t = BATCH_PUBLISH_TASKS.get(batch_id)
            if not t:
                return
            # åªè®°å½•ç¬¬ä¸€æ¬¡åœæ­¢åŸå› 
            if not t.get("stop_reason"):
                t["stop_reason"] = reason
                t["stop_message"] = message
        stop_event.set()

        _batch_publish_emit_event(
            batch_id,
            {"type": "batch_stopped", "reason": reason, "message": message},
        )

    def update_task_on_finish(site_name: str, result: dict):
        with BATCH_PUBLISH_LOCK:
            t = BATCH_PUBLISH_TASKS.get(batch_id)
            if not t:
                return
            t["results"][site_name] = result
            t["processed"] += 1
            if result.get("success"):
                t["success"] += 1
                t["site_states"][site_name] = "success"
            else:
                t["failed"] += 1
                t["site_states"][site_name] = "failed"

    def worker():
        while True:
            if should_stop():
                return
            try:
                site_name = site_queue.get_nowait()
            except queue.Empty:
                return

            if should_stop():
                return

            with state_lock:
                with BATCH_PUBLISH_LOCK:
                    t = BATCH_PUBLISH_TASKS.get(batch_id)
                    if t:
                        t["site_states"][site_name] = "running"

            _batch_publish_emit_event(batch_id, {"type": "site_started", "siteName": site_name})

            try:
                payload = {
                    "task_id": task_id,
                    "upload_data": upload_data,
                    "targetSite": site_name,
                    "sourceSite": source_site_name,
                    "downloaderId": downloader_id,
                    "auto_add_to_downloader": auto_add_to_downloader,
                }
                result, _status = _migrate_publish_impl(db_manager, payload)
            except Exception as e:
                result = {
                    "success": False,
                    "logs": f"æ‰¹é‡å‘å¸ƒå†…éƒ¨é”™è¯¯: {e}",
                    "url": None,
                }

            # æ ‡å‡†åŒ–å‰ç«¯éœ€è¦çš„å­—æ®µ
            result = result or {}
            result["siteName"] = site_name

            # æ£€æµ‹â€œå‘ç§é™åˆ¶â€å¹¶è§¦å‘åœæ­¢ï¼ˆåœæ­¢å–æ–°ç«™ç‚¹ï¼Œå·²åœ¨é£ä»»åŠ¡ä»ä¼šç»§ç»­ï¼‰
            auto_add_result = (
                (result.get("auto_add_result") or {}) if isinstance(result, dict) else {}
            )
            if auto_add_result.get("limit_reached"):
                mark_stop("limit_reached", auto_add_result.get("message", "å‘ç§é™åˆ¶è§¦å‘"))
            elif result.get("pre_check") and result.get("limit_reached"):
                mark_stop("pre_check_limit", result.get("logs", "å‘å¸ƒå‰é¢„æ£€æŸ¥è§¦å‘é™åˆ¶"))

            update_task_on_finish(site_name, result)

            public_state = _batch_publish_get_public_task_state(batch_id) or {}
            _batch_publish_emit_event(
                batch_id,
                {
                    "type": "site_finished",
                    "siteName": site_name,
                    "result": result,
                    "progress": public_state,
                },
            )

            site_queue.task_done()

    try:
        worker_count = max(1, min(concurrency, len(target_sites)))
        with ThreadPoolExecutor(max_workers=worker_count) as executor:
            futures = [executor.submit(worker) for _ in range(worker_count)]
            for f in futures:
                try:
                    f.result()
                except Exception as e:
                    logging.error(f"æ‰¹é‡å‘å¸ƒ worker å¼‚å¸¸: {e}", exc_info=True)
    finally:
        # æ ‡è®°å‰©ä½™ç«™ç‚¹ä¸º queuedï¼ˆç”¨äºå‰ç«¯å±•ç¤ºæš‚åœ/ç­‰å¾…ï¼‰
        with BATCH_PUBLISH_LOCK:
            t = BATCH_PUBLISH_TASKS.get(batch_id)
            if t:
                t["isRunning"] = False
                t["finished_at"] = time.time()

        final_state = _batch_publish_get_public_task_state(batch_id) or {"batch_id": batch_id}
        _batch_publish_emit_event(batch_id, {"type": "batch_finished", "summary": final_state})
        log_streamer.close_stream(batch_id)


@migrate_bp.route("/migrate/publish_batch/start", methods=["POST"])
def migrate_publish_batch_start():
    data = request.json or {}

    task_id = data.get("task_id")
    upload_data = data.get("upload_data") or {}
    target_sites = data.get("targetSites") or data.get("target_sites") or []
    source_site_name = data.get("sourceSite")
    downloader_id = data.get("downloaderId") or data.get("downloader_id")
    auto_add_to_downloader = bool(data.get("auto_add_to_downloader", True))

    concurrency = data.get("concurrency")
    if concurrency is None:
        cross_seed_cfg = config_manager.get().get("cross_seed", {}) or {}
        mode = data.get("concurrency_mode") or cross_seed_cfg.get(
            "publish_batch_concurrency_mode", "cpu"
        )
        manual_value = cross_seed_cfg.get(
            "publish_batch_concurrency_manual", BATCH_PUBLISH_DEFAULT_CONCURRENCY
        )
        cpu_threads = os.cpu_count() or 1
        cpu_threads = int(cpu_threads) if cpu_threads else 1

        if mode == "cpu":
            concurrency = cpu_threads * 2
        elif mode == "all":
            concurrency = len(target_sites) if isinstance(target_sites, list) else manual_value
        elif mode == "manual":
            concurrency = manual_value
        else:
            concurrency = BATCH_PUBLISH_DEFAULT_CONCURRENCY

    try:
        concurrency = int(concurrency)
    except Exception:
        concurrency = BATCH_PUBLISH_DEFAULT_CONCURRENCY
    concurrency = max(1, min(BATCH_PUBLISH_MAX_CONCURRENCY, concurrency))

    if not task_id:
        return jsonify({"success": False, "message": "ç¼ºå°‘ task_id å‚æ•°"}), 400

    with MIGRATION_CACHE_LOCK:
        if task_id not in MIGRATION_CACHE:
            return jsonify({"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}), 404

    if not isinstance(target_sites, list) or len(target_sites) == 0:
        return jsonify({"success": False, "message": "targetSites ä¸èƒ½ä¸ºç©º"}), 400

    batch_id = str(uuid.uuid4())

    with BATCH_PUBLISH_LOCK:
        BATCH_PUBLISH_TASKS[batch_id] = {
            "batch_id": batch_id,
            "task_id": task_id,
            "total": len(target_sites),
            "created_at": time.time(),
            "finished_at": None,
            "processed": 0,
            "success": 0,
            "failed": 0,
            "isRunning": True,
            "concurrency": concurrency,
            "site_states": {site: "queued" for site in target_sites},
            "results": {},
            "cancel_requested": False,
            "stop_reason": None,
            "stop_message": None,
        }

    # åˆ›å»º SSE é˜Ÿåˆ—
    log_streamer.create_stream(batch_id)

    threading.Thread(
        target=_process_publish_batch,
        kwargs={
            "batch_id": batch_id,
            "task_id": task_id,
            "upload_data": upload_data,
            "target_sites": target_sites,
            "source_site_name": source_site_name,
            "downloader_id": downloader_id,
            "auto_add_to_downloader": auto_add_to_downloader,
            "concurrency": concurrency,
            "db_manager": migrate_bp.db_manager,
        },
        daemon=True,
    ).start()

    return jsonify(
        {
            "success": True,
            "batch_id": batch_id,
            "progress": _batch_publish_get_public_task_state(batch_id),
        }
    )


@migrate_bp.route("/migrate/publish_batch/status/<batch_id>", methods=["GET"])
def migrate_publish_batch_status(batch_id):
    state = _batch_publish_get_public_task_state(batch_id)
    if not state:
        return jsonify({"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}), 404
    return jsonify({"success": True, "state": state})


@migrate_bp.route("/migrate/publish_batch/cancel/<batch_id>", methods=["POST"])
def migrate_publish_batch_cancel(batch_id):
    with BATCH_PUBLISH_LOCK:
        task = BATCH_PUBLISH_TASKS.get(batch_id)
        if not task:
            return jsonify({"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}), 404
        task["cancel_requested"] = True
        if not task.get("stop_reason"):
            task["stop_reason"] = "cancelled"
            task["stop_message"] = "ç”¨æˆ·å·²å–æ¶ˆæ‰¹é‡å‘å¸ƒ"

    _batch_publish_emit_event(batch_id, {"type": "batch_cancel_requested"})
    return jsonify({"success": True, "message": "å·²è¯·æ±‚å–æ¶ˆ"})


@migrate_bp.route("/migrate/publish_batch/stream/<batch_id>", methods=["GET"])
def migrate_publish_batch_stream(batch_id):
    """æ‰¹é‡å‘å¸ƒä»»åŠ¡ SSE äº‹ä»¶æµã€‚"""

    def generate():
        try:
            stream = log_streamer.get_stream(batch_id)
            if not stream:
                stream = log_streamer.create_stream(batch_id)

            yield f"data: {json.dumps({'type': 'connected', 'batch_id': batch_id})}\n\n"

            while True:
                try:
                    event = stream.get(timeout=1.0)
                    if event is None:
                        yield f"data: {json.dumps({'type': 'complete'})}\n\n"
                        break
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
                except Exception as queue_error:
                    if "Empty" in str(type(queue_error).__name__):
                        yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                    else:
                        logging.error(f"æ‰¹é‡å‘å¸ƒé˜Ÿåˆ—è¯»å–é”™è¯¯: {queue_error}")
                        break
        except Exception as e:
            logging.error(f"æ‰¹é‡å‘å¸ƒ SSE æµç”Ÿæˆé”™è¯¯: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return Response(
        stream_with_context(generate()),
        mimetype="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
            "Connection": "keep-alive",
        },
    )


@migrate_bp.route("/migrate_torrent", methods=["POST"])
def migrate_torrent():
    """æ‰§è¡Œä¸€æ­¥å¼ç§å­è¿ç§»ä»»åŠ¡ (ä¸æ¨èä½¿ç”¨)ã€‚"""
    db_manager = migrate_bp.db_manager
    data = request.json
    migrator = None
    source_site_name, target_site_name, search_term = (
        data.get("sourceSite"),
        data.get("targetSite"),
        data.get("searchTerm"),
    )

    if not all([source_site_name, target_site_name, search_term]):
        return jsonify({"success": False, "logs": "é”™è¯¯ï¼šæºç«™ç‚¹ã€ç›®æ ‡ç«™ç‚¹å’Œæœç´¢è¯ä¸èƒ½ä¸ºç©ºã€‚"}), 400
    if source_site_name == target_site_name:
        return jsonify({"success": False, "logs": "é”™è¯¯ï¼šæºç«™ç‚¹å’Œç›®æ ‡ç«™ç‚¹ä¸èƒ½ç›¸åŒã€‚"}), 400

    try:
        source_info = db_manager.get_site_by_nickname(source_site_name)
        target_info = db_manager.get_site_by_nickname(target_site_name)

        if not source_info or not source_info.get("cookie"):
            return (
                jsonify(
                    {
                        "success": False,
                        "logs": f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æºç«™ç‚¹ '{source_site_name}' æˆ–å…¶ç¼ºå°‘ Cookie é…ç½®ã€‚",
                    }
                ),
                404,
            )
        if not target_info or not target_info.get("cookie"):
            return (
                jsonify(
                    {
                        "success": False,
                        "logs": f"é”™è¯¯ï¼šæœªæ‰¾åˆ°ç›®æ ‡ç«™ç‚¹ '{target_site_name}' æˆ–å…¶ç¼ºå°‘ Cookie é…ç½®ã€‚",
                    }
                ),
                404,
            )

        migrator = TorrentMigrator(
            source_info,
            target_info,
            search_term,
            config_manager=config_manager,
            db_manager=db_manager,
        )
        if hasattr(migrator, "run"):
            result = migrator.run()
            return jsonify(result)
        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "logs": "é”™è¯¯ï¼šæ­¤æœåŠ¡å™¨ä¸æ”¯æŒä¸€æ­¥å¼è¿ç§»ï¼Œè¯·ä½¿ç”¨æ–°ç‰ˆè¿ç§»å·¥å…·ã€‚",
                    }
                ),
                501,
            )

    except Exception as e:
        logging.error(f"migrate_torrent å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "logs": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"}), 500
    finally:
        if migrator:
            migrator.cleanup()


@migrate_bp.route("/utils/parse_title", methods=["POST"])
def parse_title_utility():
    """æ¥æ”¶ä¸€ä¸ªæ ‡é¢˜å­—ç¬¦ä¸²ï¼Œè¿”å›è§£æåçš„å‚æ•°å­—å…¸ã€‚"""
    from utils.mediainfo_parser import (
        extract_hdr_info_from_mediainfo,
        extract_audio_info_from_mediainfo,
    )

    data = request.json
    title_to_parse = data.get("title")
    mediainfo = data.get("mediainfo", "")  # å¯é€‰çš„ mediaInfo å‚æ•°

    if not title_to_parse:
        return jsonify({"success": False, "error": "æ ‡é¢˜ä¸èƒ½ä¸ºç©ºã€‚"}), 400

    try:
        # ä» MediaInfo æå– HDR å’ŒéŸ³é¢‘ä¿¡æ¯
        mediainfo_hdr = None
        mediainfo_audio = None

        if mediainfo and mediainfo.strip():
            try:
                mediainfo_hdr = extract_hdr_info_from_mediainfo(mediainfo)
                mediainfo_audio = extract_audio_info_from_mediainfo(mediainfo)
                logging.info(f"ä» MediaInfo æå–åˆ° HDR ä¿¡æ¯: {mediainfo_hdr}")
                logging.info(f"ä» MediaInfo æå–åˆ°éŸ³é¢‘ä¿¡æ¯: {mediainfo_audio}")
            except Exception as e:
                logging.warning(f"ä» MediaInfo æå– HDR/éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")

        # ä¼ é€’ mediaInfo å‚æ•°ä»¥ä¾¿ä¿®æ­£ Blu-ray/BluRay æ ¼å¼ï¼Œä»¥åŠ HDR å’ŒéŸ³é¢‘ä¿¡æ¯
        parsed_components = upload_data_title(
            title_to_parse,
            mediaInfo=mediainfo,
            mediainfo_hdr=mediainfo_hdr,
            mediainfo_audio=mediainfo_audio,
        )

        if not parsed_components:
            return jsonify(
                {
                    "success": False,
                    "message": "æœªèƒ½ä»æ­¤æ ‡é¢˜ä¸­è§£æå‡ºæœ‰æ•ˆå‚æ•°ã€‚",
                    "components": {
                        "ä¸»æ ‡é¢˜": title_to_parse,
                        "æ— æ³•è¯†åˆ«": "è§£æå¤±è´¥",
                    },
                }
            )

        return jsonify({"success": True, "components": parsed_components})

    except Exception as e:
        logging.error(f"parse_title_utility å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"}), 500


@migrate_bp.route("/media/validate", methods=["POST"])
def validate_media():
    """æ¥æ”¶å‰ç«¯å‘é€çš„å¤±æ•ˆå›¾ç‰‡ä¿¡æ¯æˆ–ç®€ä»‹é‡æ–°è·å–è¯·æ±‚ã€‚"""
    data = request.json

    media_type = data.get("type")
    source_info = data.get("source_info")
    save_path = data.get("savePath")
    torrent_name = data.get("torrentName")
    downloader_id = data.get("downloaderId")  # è·å–ä¸‹è½½å™¨ID
    subtitle = source_info.get("subtitle") if source_info else ""
    imdb_link = source_info.get("imdb_link", "") if source_info else ""
    douban_link = source_info.get("douban_link", "") if source_info else ""
    tmdb_link = source_info.get("tmdb_link", "") if source_info else ""
    content_name = data.get("content_name") or (
        source_info.get("main_title") if source_info else ""
    )

    logging.info(
        f"æ”¶åˆ°åª’ä½“å¤„ç†è¯·æ±‚ - ç±»å‹: {media_type}, "
        f"æ¥æºä¿¡æ¯: {source_info}ï¼Œè§†é¢‘è·¯å¾„: {save_path}ï¼Œç§å­åç§°: {torrent_name}, ä¸‹è½½å™¨ID: {downloader_id}"
    )

    if media_type == "screenshot":
        screenshots = upload_data_screenshot(source_info, save_path, torrent_name, downloader_id)
        return jsonify({"success": True, "screenshots": screenshots}), 200
    elif media_type == "poster":
        # æµ·æŠ¥éªŒè¯å’Œè½¬å­˜å·²ç»åœ¨ upload_data_movie_info -> _parse_format_content ä¸­è‡ªåŠ¨å®Œæˆ
        (
            status,
            posters,
            description,
            extracted_imdb_link,
            extracted_douban_link,
            extracted_tmdb_link,
        ) = upload_data_movie_info(media_type, douban_link, imdb_link, tmdb_link, subtitle)
        if status:
            return (
                jsonify(
                    {
                        "success": True,
                        "posters": posters,
                        "extracted_imdb_link": extracted_imdb_link,
                        "extracted_douban_link": extracted_douban_link,
                        "extracted_tmdb_link": extracted_tmdb_link,
                    }
                ),
                200,
            )
        else:
            return jsonify({"success": False, "error": posters}), 400
    elif media_type == "intro":
        # å¤„ç†ç®€ä»‹é‡æ–°è·å–è¯·æ±‚
        (
            status,
            posters,
            description,
            extracted_imdb_link,
            extracted_douban_link,
            extracted_tmdb_link,
        ) = upload_data_movie_info(media_type, douban_link, imdb_link, tmdb_link, subtitle)

        if status:
            response_data = {
                "success": True,
                "intro": description,
                "extracted_imdb_link": extracted_imdb_link,
                "extracted_douban_link": extracted_douban_link,
                "extracted_tmdb_link": extracted_tmdb_link,
            }
            return jsonify(response_data), 200
        else:
            return jsonify({"success": False, "error": description}), 400
    elif media_type == "mediainfo":
        # å¤„ç†åª’ä½“ä¿¡æ¯é‡æ–°è·å–è¯·æ±‚
        from utils import upload_data_mediaInfo

        # è·å–å½“å‰çš„mediainfoï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        current_mediainfo = data.get("current_mediainfo", "")
        # è°ƒç”¨upload_data_mediaInfoå‡½æ•°é‡æ–°ç”Ÿæˆmediainfoï¼Œè®¾ç½®force_refresh=Trueå¼ºåˆ¶é‡æ–°è·å–
        new_mediainfo, _, _ = upload_data_mediaInfo(
            current_mediainfo,
            save_path,
            torrent_name=torrent_name,
            content_name=content_name,
            downloader_id=downloader_id,
            force_refresh=True,
        )  # å¼ºåˆ¶é‡æ–°è·å–
        if new_mediainfo:
            return jsonify({"success": True, "mediainfo": new_mediainfo}), 200
        else:
            return jsonify({"success": False, "error": "æ— æ³•ç”Ÿæˆåª’ä½“ä¿¡æ¯"}), 400
    else:
        return jsonify({"success": False, "error": f"ä¸æ”¯æŒçš„åª’ä½“ç±»å‹: {media_type}"}), 400


@migrate_bp.route("/migrate/get_downloader_info", methods=["POST"])
def get_downloader_info():
    """è·å–ç§å­çš„ä¸‹è½½å™¨ä¿¡æ¯ï¼ˆç”¨äºGoæœåŠ¡æŸ¥è¯¢ï¼‰"""
    db_manager = migrate_bp.db_manager
    data = request.json

    torrent_id = data.get("torrent_id")
    site_name = data.get("site_name")

    if not torrent_id or not site_name:
        return jsonify({"success": False, "message": "ç¼ºå°‘å¿…è¦å‚æ•°: torrent_id æˆ– site_name"}), 400

    try:
        torrent_name = get_seed_name(db_manager, torrent_id, site_name)
        torrent_info = get_current_torrent_info(db_manager, torrent_name)

        if torrent_info:
            return jsonify(
                {
                    "success": True,
                    "downloader_id": torrent_info.get("downloader_id"),
                    "save_path": torrent_info.get("save_path"),
                }
            )
        return jsonify({"success": False, "message": "æœªæ‰¾åˆ°è¯¥ç§å­ä¿¡æ¯"}), 404

    except Exception as e:
        logging.error(f"æŸ¥è¯¢ä¸‹è½½å™¨ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {str(e)}"}), 500


@migrate_bp.route("/migrate/add_to_downloader", methods=["POST"])
def migrate_add_to_downloader():
    """æ¥æ”¶å‘å¸ƒæˆåŠŸçš„ç§å­ä¿¡æ¯ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æŒ‡å®šçš„ä¸‹è½½å™¨ã€‚"""
    db_manager = migrate_bp.db_manager
    # config_manager åœ¨æ–‡ä»¶é¡¶éƒ¨å·²å¯¼å…¥ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
    data = request.json

    detail_page_url = data.get("url")
    save_path = data.get("savePath")
    downloader_path = data.get("downloaderPath")
    downloader_id = data.get("downloaderId")

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é»˜è®¤ä¸‹è½½å™¨
    use_default_downloader = data.get("useDefaultDownloader", False)

    # å¦‚æœéœ€è¦ä½¿ç”¨é»˜è®¤ä¸‹è½½å™¨ï¼Œä»é…ç½®ä¸­è·å–
    if use_default_downloader:
        config = config_manager.get()
        default_downloader_id = config.get("cross_seed", {}).get("default_downloader")
        # åªæœ‰å½“é»˜è®¤ä¸‹è½½å™¨IDä¸ä¸ºç©ºæ—¶æ‰ä½¿ç”¨é»˜è®¤ä¸‹è½½å™¨
        if default_downloader_id:
            downloader_id = default_downloader_id
            logging.info(f"ä½¿ç”¨é»˜è®¤ä¸‹è½½å™¨: {default_downloader_id}")
        # å¦‚æœ default_downloader_id ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨æºç§å­æ‰€åœ¨çš„ä¸‹è½½å™¨ï¼ˆä¿æŒ downloader_id ä¸å˜ï¼‰

    if not all([detail_page_url, save_path, downloader_id]):
        return (
            jsonify(
                {"success": False, "message": "é”™è¯¯ï¼šç¼ºå°‘å¿…è¦å‚æ•° (url, savePath, downloaderId)ã€‚"}
            ),
            400,
        )

    try:
        success, message = add_torrent_to_downloader(
            detail_page_url, save_path, downloader_id, db_manager, config_manager
        )

        # å¤„ç†å‘ç§é™åˆ¶çŠ¶æ€
        if success == "LIMIT_REACHED":
            return jsonify(
                {
                    "success": False,
                    "limit_reached": True,
                    "message": message,
                    "should_stop_batch": True,
                    "code": "SEEDING_LIMIT_EXCEEDED",
                }
            )

        return jsonify({"success": success, "message": message})
    except Exception as e:
        logging.error(f"add_to_downloader è·¯ç”±å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"}), 500


@migrate_bp.route("/sites/status", methods=["GET"])
def get_sites_status():
    """è·å–æ‰€æœ‰ç«™ç‚¹çš„è¯¦ç»†é…ç½®çŠ¶æ€ã€‚"""
    db_manager = migrate_bp.db_manager
    try:
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)

        # ä»æ•°æ®åº“æŸ¥è¯¢æ‰€æœ‰ç«™ç‚¹çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬è‹±æ–‡ç«™ç‚¹å
        cursor.execute(
            "SELECT nickname, site, cookie, passkey, migration FROM sites WHERE nickname IS NOT NULL AND nickname != ''"
        )
        sites_from_db = cursor.fetchall()

        sites_status = []
        for row_obj in sites_from_db:
            # [ä¿®å¤] å°† sqlite3.Row å¯¹è±¡è½¬æ¢ä¸ºæ ‡å‡†çš„ dictï¼Œä»¥æ”¯æŒ .get() æ–¹æ³•
            row = dict(row_obj)
            nickname = row.get("nickname")
            if not nickname:
                continue

            migration_status = row.get("migration", 0)

            site_info = {
                "name": nickname,
                "site": row.get("site"),  # æ·»åŠ è‹±æ–‡ç«™ç‚¹å
                "has_cookie": bool(row.get("cookie")),
                "has_passkey": bool(row.get("passkey")),
                "is_source": migration_status in [1, 3],
                "is_target": migration_status in [2, 3],
            }
            sites_status.append(site_info)

        return jsonify(sorted(sites_status, key=lambda x: x["name"].lower()))

    except Exception as e:
        logging.error(f"è·å–ç«™ç‚¹çŠ¶æ€åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500
    finally:
        if "conn" in locals() and conn:
            if "cursor" in locals() and cursor:
                cursor.close()
            conn.close()


# ç§»é™¤äº†ä¸downloader_queueç›¸å…³çš„APIè·¯ç”±ï¼Œå› ä¸ºç°åœ¨ä½¿ç”¨åŒæ­¥æ–¹å¼


@migrate_bp.route("/migrate/update_preview_data", methods=["POST"])
def update_preview_data():
    """æ›´æ–°é¢„è§ˆæ•°æ®"""
    data = request.json
    task_id = data.get("task_id")
    updated_data = data.get("updated_data")

    if not task_id:
        return jsonify({"success": False, "message": "é”™è¯¯ï¼šæ— æ•ˆæˆ–å·²è¿‡æœŸçš„ä»»åŠ¡IDã€‚"}), 400

    if not updated_data:
        return jsonify({"success": False, "message": "é”™è¯¯ï¼šç¼ºå°‘æ›´æ–°æ•°æ®ã€‚"}), 400

    try:
        # è·å–ç¼“å­˜ä¸­çš„ä¸Šä¸‹æ–‡
        with MIGRATION_CACHE_LOCK:
            context = MIGRATION_CACHE.get(task_id)
        if not context:
            return jsonify({"success": False, "message": "é”™è¯¯ï¼šæ— æ•ˆæˆ–å·²è¿‡æœŸçš„ä»»åŠ¡IDã€‚"}), 400
        source_info = context["source_info"]
        original_torrent_path = context["original_torrent_path"]

        # æ›´æ–° review_data ä¸­çš„ç›¸å…³å­—æ®µ
        review_data = context["review_data"].copy()
        review_data["original_main_title"] = updated_data.get(
            "original_main_title", review_data.get("original_main_title", "")
        )
        review_data["title_components"] = updated_data.get(
            "title_components", review_data.get("title_components", [])
        )
        review_data["subtitle"] = updated_data.get("subtitle", review_data.get("subtitle", ""))
        review_data["imdb_link"] = updated_data.get("imdb_link", review_data.get("imdb_link", ""))
        review_data["intro"] = updated_data.get("intro", review_data.get("intro", {}))
        review_data["mediainfo"] = updated_data.get("mediainfo", review_data.get("mediainfo", ""))
        review_data["source_params"] = updated_data.get(
            "source_params", review_data.get("source_params", {})
        )

        # é‡æ–°æå–äº§åœ°ä¿¡æ¯
        full_description_text = (
            f"{review_data['intro'].get('statement', '')}\n{review_data['intro'].get('body', '')}"
        )
        origin_info = extract_origin_from_description(full_description_text)
        if origin_info and "source_params" in review_data:
            review_data["source_params"]["äº§åœ°"] = origin_info

        # é‡æ–°ç”Ÿæˆé¢„è§ˆå‚æ•°
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦é‡æ–°æ„å»ºå®Œæ•´çš„å‘å¸ƒå‚æ•°é¢„è§ˆ
        try:
            # 1. é‡æ–°è§£ææ ‡é¢˜ç»„ä»¶
            title_components = review_data.get("title_components", [])
            if not title_components:
                # ä¼ å…¥mediainfoä»¥ä¾¿ä¿®æ­£Blu-ray/BluRayæ ¼å¼
                mediainfo = review_data.get("mediainfo", "")
                title_components = upload_data_title(
                    review_data["original_main_title"], mediaInfo=mediainfo
                )

            # 2. é‡æ–°æ„å»ºæ ‡é¢˜å‚æ•°å­—å…¸
            title_params = {
                item["key"]: item["value"] for item in title_components if item.get("value")
            }

            # 3. å¦‚æœåˆ†è¾¨ç‡ä¸ºç©ºï¼Œå°è¯•ä»MediaInfoä¸­æå–åˆ†è¾¨ç‡
            resolution_from_title = title_params.get("åˆ†è¾¨ç‡")
            if not resolution_from_title or resolution_from_title == "N/A":
                resolution_from_mediainfo = extract_resolution_from_mediainfo(
                    review_data["mediainfo"]
                )
                if resolution_from_mediainfo:
                    # æ›´æ–°æ ‡é¢˜å‚æ•°ä¸­çš„åˆ†è¾¨ç‡
                    title_params["åˆ†è¾¨ç‡"] = resolution_from_mediainfo
                    # åŒæ—¶æ›´æ–°title_componentsä¸­çš„åˆ†è¾¨ç‡é¡¹
                    for component in title_components:
                        if component["key"] == "åˆ†è¾¨ç‡":
                            component["value"] = resolution_from_mediainfo
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†è¾¨ç‡é¡¹ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„
                        title_components.append(
                            {"key": "åˆ†è¾¨ç‡", "value": resolution_from_mediainfo}
                        )

            # 3. é‡æ–°æ‹¼æ¥ä¸»æ ‡é¢˜
            order = [
                "ä¸»æ ‡é¢˜",
                "å­£é›†",
                "å¹´ä»½",
                "å‰§é›†çŠ¶æ€",
                "å‘å¸ƒç‰ˆæœ¬",
                "åˆ†è¾¨ç‡",
                "ç‰‡æºå¹³å°",
                "åª’ä»‹",
                "å¸§ç‡",
                "HDRæ ¼å¼",
                "è§†é¢‘ç¼–ç ",
                "è§†é¢‘æ ¼å¼",
                "è‰²æ·±",
                "éŸ³é¢‘ç¼–ç ",
            ]
            title_parts = []
            for key in order:
                value = title_params.get(key)
                if value:
                    title_parts.append(
                        " ".join(map(str, value)) if isinstance(value, list) else str(value)
                    )

            raw_main_part = " ".join(filter(None, title_parts))
            main_part = re.sub(r"(?<!\d)\.(?!\d)", " ", raw_main_part)
            main_part = re.sub(r"\s+", " ", main_part).strip()
            release_group = title_params.get("åˆ¶ä½œç»„", "NOGROUP")
            if "N/A" in release_group:
                release_group = "NOGROUP"

            # å¯¹ç‰¹æ®Šåˆ¶ä½œç»„è¿›è¡Œå¤„ç†ï¼Œä¸éœ€è¦æ·»åŠ å‰ç¼€è¿å­—ç¬¦
            special_groups = ["MNHD-FRDS", "mUHD-FRDS"]
            if release_group in special_groups:
                preview_title = f"{main_part} {release_group}"
            else:
                preview_title = f"{main_part}-{release_group}"

            # 4. é‡æ–°ç»„åˆç®€ä»‹
            full_description = (
                f"{review_data['intro'].get('statement', '')}\n"
                f"{review_data['intro'].get('poster', '')}\n"
                f"{review_data['intro'].get('body', '')}\n"
                f"{review_data['intro'].get('screenshots', '')}"
            )

            # 5. é‡æ–°æ”¶é›†æ ‡ç­¾
            source_tags = set(review_data["source_params"].get("æ ‡ç­¾") or [])
            mediainfo_tags = set(extract_tags_from_mediainfo(review_data["mediainfo"]))
            all_tags = sorted(list(source_tags.union(mediainfo_tags)))

            # 6. é‡æ–°ç»„è£…é¢„è§ˆå­—å…¸
            final_publish_parameters = {
                "ä¸»æ ‡é¢˜ (é¢„è§ˆ)": preview_title,
                "å‰¯æ ‡é¢˜": review_data["subtitle"],
                "IMDbé“¾æ¥": review_data["imdb_link"],
                "ç±»å‹": review_data["source_params"].get("ç±»å‹", "N/A"),
                "åª’ä»‹": title_params.get("åª’ä»‹", "N/A"),
                "è§†é¢‘ç¼–ç ": title_params.get("è§†é¢‘ç¼–ç ", "N/A"),
                "éŸ³é¢‘ç¼–ç ": title_params.get("éŸ³é¢‘ç¼–ç ", "N/A"),
                "åˆ†è¾¨ç‡": title_params.get("åˆ†è¾¨ç‡", "N/A"),
                "åˆ¶ä½œç»„": title_params.get("åˆ¶ä½œç»„", "N/A"),
                "äº§åœ°": review_data["source_params"].get("äº§åœ°", "N/A"),
                "æ ‡ç­¾ (ç»¼åˆ)": all_tags,
            }

            # ä½¿ç”¨æ–°çš„Extractorå’ŒParameterMapperæ¥å¤„ç†å‚æ•°æ˜ å°„
            source_site_name = context.get("source_site_name", "")

            # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„HTML soupå¯¹è±¡ç”¨äºæå–å™¨
            # ç”±äºæˆ‘ä»¬å·²ç»æœ‰æå–çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„soupå¯¹è±¡
            from bs4 import BeautifulSoup

            mock_html = (
                f"<html><body><h1 id='top'>{review_data.get('title', '')}</h1></body></html>"
            )
            mock_soup = BeautifulSoup(mock_html, "html.parser")

            # åˆå§‹åŒ–æå–å™¨
            from core.extractors.extractor import Extractor, ParameterMapper

            extractor = Extractor()
            mapper = ParameterMapper()

            # åˆ›å»ºæå–æ•°æ®ç»“æ„ï¼Œæ¨¡æ‹Ÿä»ç½‘é¡µæå–çš„æ•°æ®
            extracted_data = {
                "title": review_data.get("title", ""),
                "subtitle": review_data.get("subtitle", ""),
                "intro": review_data.get("intro", {}),
                "mediainfo": review_data.get("mediainfo", ""),
                "source_params": review_data.get("source_params", {}),
                "title_components": title_components,
            }

            # ä½¿ç”¨ParameterMapperæ˜ å°„å‚æ•°
            standardized_params = mapper.map_parameters(source_site_name, "", extracted_data)

            # ä¿å­˜å‚æ•°åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
            import os

            tmp_dir = "data/tmp"
            os.makedirs(tmp_dir, exist_ok=True)

            # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°åˆ°æ–‡ä»¶
            with open(os.path.join(tmp_dir, "2.txt"), "w", encoding="utf-8") as f:
                f.write(f"æºç«™ç‚¹åç§°: {source_site_name}\n")
                f.write("æœ€ç»ˆæ ‡å‡†åŒ–å‚æ•°ï¼ˆä½¿ç”¨æ–°æ˜ å°„ç³»ç»Ÿï¼‰:\n")
                for key, value in standardized_params.items():
                    f.write(f"{key}: {value}\n")
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                f.write(f"\nè°ƒè¯•ä¿¡æ¯:\n")
                f.write(f"video_codecå€¼: {standardized_params.get('video_codec', 'æœªæ‰¾åˆ°')}\n")
                f.write(f"codecå€¼: {standardized_params.get('codec', 'æœªæ‰¾åˆ°')}\n")

            # ç”¨äºé¢„è§ˆæ˜¾ç¤ºæ ‡å‡†åŒ–é”®å¯¹åº”çš„å†…å®¹
            preview_video_codec = standardized_params.get("video_codec", "video.other")
            preview_audio_codec = standardized_params.get("audio_codec", "audio.other")
            preview_medium = standardized_params.get("medium", "medium.other")
            preview_resolution = standardized_params.get("resolution", "resolution.other")
            preview_team = standardized_params.get("team", "team.other")
            preview_type = standardized_params.get("type", "category.other")
            preview_source = standardized_params.get("source", "N/A")

            raw_params_for_preview = {
                "final_main_title": preview_title,
                "subtitle": review_data["subtitle"],
                "imdb_link": review_data["imdb_link"],
                "type": preview_type,
                "medium": preview_medium,
                "video_codec": preview_video_codec,
                "audio_codec": preview_audio_codec,
                "resolution": preview_resolution,
                "release_group": preview_team,
                "source": preview_source,
                "tags": list(all_tags),
            }

            # æ›´æ–° review_data ä¸­çš„é¢„è§ˆå‚æ•°
            review_data["final_publish_parameters"] = final_publish_parameters
            review_data["raw_params_for_preview"] = raw_params_for_preview

            # æ›´æ–°ç¼“å­˜ä¸­çš„ review_data
            with MIGRATION_CACHE_LOCK:
                if task_id in MIGRATION_CACHE:
                    MIGRATION_CACHE[task_id]["review_data"] = review_data

            return jsonify({"success": True, "data": review_data, "message": "é¢„è§ˆæ•°æ®æ›´æ–°æˆåŠŸ"})
        except Exception as e:
            logging.error(f"é‡æ–°ç”Ÿæˆé¢„è§ˆæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return jsonify({"success": False, "message": f"é‡æ–°ç”Ÿæˆé¢„è§ˆæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}"}), 500

    except Exception as e:
        logging.error(f"update_preview_data å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"}), 500


# ===================================================================
#                    æ‰¹é‡è·å–ç§å­æ•°æ® API
# ===================================================================

# å­˜å‚¨æ‰¹é‡ä»»åŠ¡çš„è¿›åº¦ä¿¡æ¯
BATCH_FETCH_TASKS = {}


@migrate_bp.route("/migrate/get_aggregated_torrents", methods=["POST"])
def get_aggregated_torrents():
    """è·å–æŒ‰åç§°èšåˆçš„ç§å­åˆ—è¡¨ï¼ˆç”¨äºæ‰¹é‡è·å–æ•°æ®ï¼‰"""
    try:
        db_manager = migrate_bp.db_manager
        data = request.json

        # è·å–åˆ†é¡µå‚æ•°
        page = data.get("page", 1)
        page_size = data.get("pageSize", 50)

        # è·å–ç­›é€‰æ¡ä»¶
        name_search = data.get("nameSearch", "").lower()
        path_filters = data.get("pathFilters", [])
        state_filters = data.get("stateFilters", [])
        downloader_filters = data.get("downloaderFilters", [])

        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)

        # è·å–æ‰€æœ‰ç«™ç‚¹é…ç½®ä¿¡æ¯
        cursor.execute("SELECT nickname, migration FROM sites")
        site_configs = {row["nickname"]: dict(row) for row in cursor.fetchall()}

        # æŸ¥è¯¢æ‰€æœ‰ç§å­æ•°æ®
        if db_manager.db_type == "postgresql":
            cursor.execute(
                'SELECT hash, name, save_path, size, progress, state, sites, "group", details, downloader_id FROM torrents WHERE state != %s',
                ("ä¸å­˜åœ¨",),
            )
        else:
            cursor.execute(
                "SELECT hash, name, save_path, size, progress, state, sites, `group`, details, downloader_id FROM torrents WHERE state != %s",
                ("ä¸å­˜åœ¨",),
            )
        torrents_raw = [dict(row) for row in cursor.fetchall()]

        # æŸ¥è¯¢ seed_parameters è¡¨ä¸­å·²å­˜åœ¨çš„ç§å­åç§°
        # å»é™¤ .torrent åç¼€è¿›è¡ŒåŒ¹é…
        cursor.execute(
            "SELECT DISTINCT name FROM seed_parameters WHERE name IS NOT NULL AND name != ''"
        )
        existing_seed_names = set(row["name"] for row in cursor.fetchall())
        logging.info(f"seed_parameters è¡¨ä¸­å·²æœ‰ {len(existing_seed_names)} ä¸ªç§å­è®°å½•")

        # æŒ‰åç§°èšåˆç§å­
        from collections import defaultdict

        agg_torrents = defaultdict(
            lambda: {
                "name": "",
                "save_path": "",
                "size": 0,
                "progress": 0,
                "state": set(),
                "sites": defaultdict(dict),
                "downloader_ids": [],
            }
        )

        for t in torrents_raw:
            torrent_key = t["name"]
            agg = agg_torrents[torrent_key]
            if not agg["name"]:
                agg.update(
                    {
                        "name": t["name"],
                        "save_path": t.get("save_path", ""),
                        "size": t.get("size", 0),
                    }
                )
            downloader_id = t.get("downloader_id")
            if downloader_id and downloader_id not in agg["downloader_ids"]:
                agg["downloader_ids"].append(downloader_id)
            agg["progress"] = max(agg.get("progress", 0), t.get("progress", 0))
            agg["state"].add(t.get("state", "N/A"))
            if t.get("sites"):
                site_name = t.get("sites")
                agg["sites"][site_name]["comment"] = t.get("details")
                agg["sites"][site_name]["state"] = t.get("state", "N/A")
                agg["sites"][site_name]["migration"] = site_configs.get(site_name, {}).get(
                    "migration", 0
                )

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶åº”ç”¨ç­›é€‰
        filtered_list = []
        for name, data in agg_torrents.items():
            # æ’é™¤å·²åœ¨ seed_parameters è¡¨ä¸­å­˜åœ¨çš„ç§å­
            # å»é™¤ .torrent åç¼€è¿›è¡ŒåŒ¹é…
            name_without_ext = name
            if name_without_ext.lower().endswith(".torrent"):
                name_without_ext = name_without_ext[:-8]

            if name_without_ext in existing_seed_names:
                logging.debug(f"æ’é™¤å·²å­˜åœ¨çš„ç§å­: {name}")
                continue

            # åç§°æœç´¢ç­›é€‰
            if name_search and name_search not in name.lower():
                continue

            # è·¯å¾„ç­›é€‰
            if path_filters and data["save_path"] not in path_filters:
                continue

            # çŠ¶æ€ç­›é€‰
            state_str = ", ".join(sorted(list(data["state"])))
            if state_filters and state_str not in state_filters:
                continue

            # ä¸‹è½½å™¨ç­›é€‰
            if downloader_filters:
                if not any(did in downloader_filters for did in data.get("downloader_ids", [])):
                    continue

            data.update({"state": state_str, "sites": dict(data["sites"])})  # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
            filtered_list.append(data)

        # è®¡ç®—æ€»æ•°
        total = len(filtered_list)

        # åº”ç”¨åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_list = filtered_list[start_idx:end_idx]

        print(
            f"åˆ†é¡µå‚æ•°: page={page}, page_size={page_size}, total={total}, start_idx={start_idx}, end_idx={end_idx}, paginated_count={len(paginated_list)}"
        )

        cursor.close()
        conn.close()

        return jsonify({"success": True, "data": paginated_list, "total": total})

    except Exception as e:
        logging.error(f"get_aggregated_torrents å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@migrate_bp.route("/migrate/batch_fetch_seed_data", methods=["POST"])
def batch_fetch_seed_data():
    """æ‰¹é‡è·å–ç§å­æ•°æ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    try:
        db_manager = migrate_bp.db_manager
        config_manager = migrate_bp.config_manager
        data = request.json

        torrent_names = data.get("torrentNames", [])
        # ä»é…ç½®ä¸­è¯»å–æºç«™ç‚¹ä¼˜å…ˆçº§
        config = config_manager.get()
        source_sites_priority = config.get("source_priority", [])

        if not torrent_names:
            return jsonify({"success": False, "message": "é”™è¯¯ï¼šç§å­åç§°åˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400

        if not source_sites_priority:
            return (
                jsonify({"success": False, "message": "é”™è¯¯ï¼šè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®æºç«™ç‚¹ä¼˜å…ˆçº§"}),
                400,
            )

        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())

        # åˆå§‹åŒ–ä»»åŠ¡è¿›åº¦
        BATCH_FETCH_TASKS[task_id] = {
            "total": len(torrent_names),
            "processed": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0,
            "isRunning": True,
            "results": [],
        }

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæ‰¹é‡è·å–
        from threading import Thread

        thread = Thread(
            target=_process_batch_fetch,
            args=(task_id, torrent_names, source_sites_priority, db_manager, config_manager),
        )
        thread.daemon = True
        thread.start()

        return jsonify({"success": True, "task_id": task_id, "message": "æ‰¹é‡è·å–ä»»åŠ¡å·²å¯åŠ¨"})

    except Exception as e:
        logging.error(f"batch_fetch_seed_data å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


def _process_batch_fetch(task_id, torrent_names, source_sites_priority, db_manager, config_manager):
    """åå°å¤„ç†æ‰¹é‡è·å–ä»»åŠ¡"""
    import time
    import re

    # è®°å½•æ¯ä¸ªç«™ç‚¹çš„æœ€åè¯·æ±‚æ—¶é—´ï¼Œç”¨äºæ§åˆ¶è¯·æ±‚é—´éš”
    site_last_request_time = {}
    # é»˜è®¤è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
    REQUEST_INTERVAL = 5

    try:
        def extract_torrent_id(comment: str):
            if not comment:
                return None
            id_match = re.search(r"id=(\d+)", comment)
            if id_match:
                return id_match.group(1)
            stripped = comment.strip()
            if re.fullmatch(r"\d+", stripped):
                return stripped
            return None

        # é¢„å¤„ç†ï¼šæ‰¹é‡ä½¿ç”¨ IYUU æŸ¥è¯¢ç¼ºå¤±çš„ä¼˜å…ˆçº§æºç«™ç‚¹ï¼ˆé¿å…é€ä¸ªè¯·æ±‚ï¼‰
        iyuu_batch_names = set()
        iyuu_batch_done = False

        config = config_manager.get() if config_manager else {}
        iyuu_settings = config.get("iyuu_settings", {})
        path_filter_enabled = iyuu_settings.get("path_filter_enabled", False)
        selected_paths = set(iyuu_settings.get("selected_paths", []) or [])

        torrents_for_iyuu = {}
        for torrent_name in torrent_names:
            if task_id not in BATCH_FETCH_TASKS:
                logging.warning(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆ")
                break

            conn = None
            cursor = None
            try:
                conn = db_manager._get_connection()
                cursor = db_manager._get_cursor(conn)

                if db_manager.db_type == "sqlite":
                    cursor.execute(
                        "SELECT hash, name, save_path, size, sites, details, downloader_id FROM torrents WHERE name = ? AND state != ?",
                        (torrent_name, "ä¸å­˜åœ¨"),
                    )
                else:  # postgresql or mysql
                    cursor.execute(
                        "SELECT hash, name, save_path, size, sites, details, downloader_id FROM torrents WHERE name = %s AND state != %s",
                        (torrent_name, "ä¸å­˜åœ¨"),
                    )

                torrents = [dict(row) for row in cursor.fetchall()]
                if not torrents:
                    continue

                # å¦‚æœå¯ç”¨äº†è·¯å¾„è¿‡æ»¤ä¸”å½“å‰ç§å­ä¸åœ¨é€‰ä¸­è·¯å¾„å†…ï¼Œä¿æŒä¸å•ç§å­æŸ¥è¯¢ä¸€è‡´ï¼šè·³è¿‡ IYUU
                if path_filter_enabled and selected_paths:
                    save_path = torrents[0].get("save_path", "")
                    if save_path and save_path not in selected_paths:
                        continue

                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å¯ç”¨çš„ä¼˜å…ˆçº§æºç«™ç‚¹
                priority_source_found = False
                for priority_site in source_sites_priority:
                    source_info = db_manager.get_site_by_nickname(priority_site)
                    if not source_info or not source_info.get("cookie"):
                        continue
                    if source_info.get("migration", 0) not in [1, 3]:
                        continue

                    for torrent in torrents:
                        if torrent.get("sites") != priority_site:
                            continue
                        torrent_id = extract_torrent_id(torrent.get("details", ""))
                        if torrent_id:
                            priority_source_found = True
                            break

                    if priority_source_found:
                        break

                if priority_source_found:
                    continue

                torrent_size = torrents[0].get("size", 0) or 0
                # ä¸å•ç§å­ IYUU æŸ¥è¯¢ä¿æŒä¸€è‡´ï¼šä»…å¯¹å¤§äº 200MB çš„ç§å­æ‰§è¡Œ IYUU æŸ¥è¯¢
                if torrent_size <= 207374182:
                    continue

                iyuu_batch_names.add(torrent_name)
                torrents_for_iyuu[torrent_name] = torrents

            except Exception as e:
                logging.error(f"é¢„å¤„ç† {torrent_name} çš„ IYUU æ‰¹é‡æŸ¥è¯¢æ¡ä»¶å¤±è´¥: {e}", exc_info=True)
            finally:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()

        if iyuu_batch_names and task_id in BATCH_FETCH_TASKS:
            try:
                from core.iyuu import IYUUThread, log_iyuu_message

                log_iyuu_message(
                    f"æ‰¹é‡é¢„æŸ¥è¯¢ï¼š{len(iyuu_batch_names)} ä¸ªç§å­ç»„å°†ä½¿ç”¨ IYUU è§£æä¼˜å…ˆçº§æºç«™ç‚¹",
                    "INFO",
                )

                iyuu_worker = IYUUThread(db_manager, config_manager)
                configured_sites = iyuu_worker._get_configured_sites()

                agg_torrents = {}
                all_torrents = {}
                for name in torrent_names:
                    if name not in iyuu_batch_names:
                        continue
                    torrent_rows = torrents_for_iyuu.get(name, [])
                    torrent_list = [
                        {
                            "hash": t.get("hash"),
                            "sites": t.get("sites"),
                            "size": t.get("size", 0),
                            "save_path": t.get("save_path", ""),
                        }
                        for t in torrent_rows
                    ]
                    all_torrents[name] = torrent_list
                    agg_torrents[name] = torrent_list

                iyuu_worker._perform_iyuu_search(
                    agg_torrents,
                    configured_sites,
                    all_torrents,
                    force_query=False,
                )
                iyuu_batch_done = True
            except Exception as e:
                logging.error(f"æ‰¹é‡ IYUU æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
                iyuu_batch_done = False

        for torrent_name in torrent_names:
            if task_id not in BATCH_FETCH_TASKS:
                logging.warning(f"ä»»åŠ¡ {task_id} å·²è¢«å–æ¶ˆ")
                break

            try:
                # æŸ¥è¯¢è¯¥åç§°çš„æ‰€æœ‰ç§å­è®°å½•
                conn = db_manager._get_connection()
                cursor = db_manager._get_cursor(conn)

                if db_manager.db_type == "sqlite":
                    cursor.execute(
                        "SELECT hash, name, save_path, size, sites, details, downloader_id FROM torrents WHERE name = ? AND state != ?",
                        (torrent_name, "ä¸å­˜åœ¨"),
                    )
                else:  # postgresql or mysql
                    cursor.execute(
                        "SELECT hash, name, save_path, size, sites, details, downloader_id FROM torrents WHERE name = %s AND state != %s",
                        (torrent_name, "ä¸å­˜åœ¨"),
                    )

                torrents = [dict(row) for row in cursor.fetchall()]
                cursor.close()
                conn.close()

                if not torrents:
                    BATCH_FETCH_TASKS[task_id]["results"].append(
                        {"name": torrent_name, "status": "skipped", "reason": "æœªæ‰¾åˆ°ç§å­è®°å½•"}
                    )
                    BATCH_FETCH_TASKS[task_id]["skipped"] += 1
                    BATCH_FETCH_TASKS[task_id]["processed"] += 1
                    continue

                # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾å¯ç”¨çš„æºç«™ç‚¹
                source_found = None
                for priority_site in source_sites_priority:
                    # è·å–ç«™ç‚¹ä¿¡æ¯
                    source_info = db_manager.get_site_by_nickname(priority_site)
                    if not source_info or not source_info.get("cookie"):
                        continue

                    # æ£€æŸ¥è¯¥ç«™ç‚¹çš„migrationçŠ¶æ€
                    if source_info.get("migration", 0) not in [1, 3]:
                        continue

                    # æŸ¥æ‰¾è¯¥ç«™ç‚¹çš„ç§å­è®°å½•
                    for torrent in torrents:
                        if torrent.get("sites") == priority_site:
                            # æå–ç§å­ID
                            comment = torrent.get("details", "")
                            torrent_id = None

                            torrent_id = extract_torrent_id(comment)

                            if torrent_id:
                                source_found = {
                                    "site": priority_site,
                                    "site_info": source_info,
                                    "torrent_id": torrent_id,
                                    "torrent": torrent,
                                }
                                break

                    if source_found:
                        break

                # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœä¼˜å…ˆçº§ç«™ç‚¹éƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ IYUU æŸ¥è¯¢ï¼ˆæ‰¹é‡é¢„æŸ¥è¯¢å·²è¦†ç›–å¤šæ•°æƒ…å†µï¼‰
                if not source_found and (not iyuu_batch_done or torrent_name not in iyuu_batch_names):
                    try:
                        from core.iyuu import IYUUThread, iyuu_thread

                        iyuu_worker = None
                        if iyuu_thread and iyuu_thread.is_alive():
                            iyuu_worker = iyuu_thread
                        else:
                            iyuu_worker = IYUUThread(db_manager, config_manager)

                        if iyuu_worker:
                            # è·å–ç§å­å¤§å°ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªç§å­çš„å¤§å°ï¼Œå› ä¸ºåŒåç§å­å¤§å°åº”è¯¥ç›¸åŒï¼‰
                            torrent_size = 0
                            if torrents:
                                torrent_size = torrents[0].get("size", 0)

                            logging.info(
                                f"ä¼˜å…ˆçº§ç«™ç‚¹æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨ IYUU æŸ¥è¯¢: {torrent_name} (å¤§å°: {torrent_size} å­—èŠ‚)"
                            )

                            # æ‰§è¡Œ IYUU æŸ¥è¯¢
                            result_stats = iyuu_worker._process_single_torrent(
                                torrent_name, torrent_size
                            )

                            if result_stats and result_stats.get("total_found", 0) > 0:
                                logging.info(
                                    f"IYUU æŸ¥è¯¢æ‰¾åˆ° {result_stats['total_found']} æ¡è®°å½•ï¼Œé‡æ–°æŸ¥è¯¢æ•°æ®åº“"
                                )

                                # é‡æ–°æŸ¥è¯¢æ•°æ®åº“ï¼Œè·å–æ›´æ–°åçš„ç§å­è®°å½•
                                conn = db_manager._get_connection()
                                cursor = db_manager._get_cursor(conn)

                                if db_manager.db_type == "sqlite":
                                    cursor.execute(
                                        "SELECT hash, name, save_path, sites, details, downloader_id FROM torrents WHERE name = ? AND state != ?",
                                        (torrent_name, "ä¸å­˜åœ¨"),
                                    )
                                else:  # postgresql or mysql
                                    cursor.execute(
                                        "SELECT hash, name, save_path, sites, details, downloader_id FROM torrents WHERE name = %s AND state != %s",
                                        (torrent_name, "ä¸å­˜åœ¨"),
                                    )

                                updated_torrents = [dict(row) for row in cursor.fetchall()]
                                cursor.close()
                                conn.close()

                                if updated_torrents:
                                    torrents = updated_torrents
                                    logging.info(f"IYUU æŸ¥è¯¢åé‡æ–°æ£€æŸ¥ä¼˜å…ˆçº§ç«™ç‚¹")

                                    # é‡æ–°æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾å¯ç”¨çš„æºç«™ç‚¹
                                    for priority_site in source_sites_priority:
                                        # è·å–ç«™ç‚¹ä¿¡æ¯
                                        source_info = db_manager.get_site_by_nickname(
                                            priority_site
                                        )
                                        if not source_info or not source_info.get("cookie"):
                                            continue

                                        # æ£€æŸ¥è¯¥ç«™ç‚¹çš„migrationçŠ¶æ€
                                        if source_info.get("migration", 0) not in [1, 3]:
                                            continue

                                        # æŸ¥æ‰¾è¯¥ç«™ç‚¹çš„ç§å­è®°å½•ï¼ˆåœ¨æ›´æ–°åçš„torrentsä¸­ï¼‰
                                        for torrent in torrents:
                                            if torrent.get("sites") != priority_site:
                                                continue
                                            comment = torrent.get("details", "")
                                            torrent_id = extract_torrent_id(comment)
                                            if torrent_id:
                                                    source_found = {
                                                        "site": priority_site,
                                                        "site_info": source_info,
                                                        "torrent_id": torrent_id,
                                                        "torrent": torrent,
                                                    }
                                                    logging.info(
                                                        f"IYUU æŸ¥è¯¢ååœ¨ä¼˜å…ˆçº§ç«™ç‚¹ä¸­æ‰¾åˆ°: {priority_site}"
                                                    )
                                                    break

                                        if source_found:
                                            break
                                else:
                                    logging.info(f"IYUU æŸ¥è¯¢æœªæ‰¾åˆ°æ–°çš„ç§å­è®°å½•")
                    except Exception as e:
                        logging.error(f"IYUU æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)

                # ç¬¬ä¸‰é˜¶æ®µï¼šå¦‚æœ IYUU æŸ¥è¯¢åè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œåœ¨å…¶ä»–å­˜åœ¨çš„æºç«™ç‚¹ä¸­æŸ¥æ‰¾
                if not source_found:
                    # è·å–æ‰€æœ‰å·²å­˜åœ¨çš„ç«™ç‚¹åç§°ï¼ˆæ’é™¤å·²ç»åœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­çš„ï¼‰
                    existing_sites = set()
                    for torrent in torrents:
                        site_name = torrent.get("sites")
                        if site_name and site_name not in source_sites_priority:
                            existing_sites.add(site_name)

                    # åœ¨è¿™äº›å…¶ä»–ç«™ç‚¹ä¸­æŸ¥æ‰¾å¯ç”¨çš„æºç«™ç‚¹
                    for site_name in existing_sites:
                        # è·å–ç«™ç‚¹ä¿¡æ¯
                        source_info = db_manager.get_site_by_nickname(site_name)
                        if not source_info or not source_info.get("cookie"):
                            continue
                        # æ£€æŸ¥è¯¥ç«™ç‚¹çš„migrationçŠ¶æ€
                        if source_info.get("migration", 0) not in [1, 3]:
                            continue
                        # æŸ¥æ‰¾è¯¥ç«™ç‚¹çš„ç§å­è®°å½•
                        for torrent in torrents:
                            if torrent.get("sites") == site_name:
                                # æå–ç§å­ID
                                comment = torrent.get("details", "")
                                torrent_id = None
                                if comment:
                                    # å°è¯•ä»commentä¸­æå–ID
                                    import re

                                    id_match = re.search(r"id=(\d+)", comment)
                                    if id_match:
                                        torrent_id = id_match.group(1)
                                    elif re.match(r"^\d+$", comment.strip()):
                                        torrent_id = comment.strip()
                                if torrent_id:
                                    source_found = {
                                        "site": site_name,
                                        "site_info": source_info,
                                        "torrent_id": torrent_id,
                                        "torrent": torrent,
                                    }
                                    break
                        if source_found:
                            break

                # æ–°å¢ï¼šè‡ªåŠ¨é‡è¯•å’Œç«™ç‚¹åˆ‡æ¢é€»è¾‘
                # å®ç°ç«™ç‚¹è‡ªåŠ¨é‡è¯•å’Œæ™ºèƒ½åˆ‡æ¢åŠŸèƒ½
                max_retry_per_site = 2  # æ¯ä¸ªç«™ç‚¹æœ€å¤šé‡è¯•2æ¬¡
                fetch_success = False
                final_source = None
                attempted_sites_details = []

                # æ’é™¤"æˆ‘å ¡"å’Œ"OurBits"ç«™ç‚¹
                excluded_sites = {"æˆ‘å ¡", "OurBits"}

                # æ„å»ºæ‰€æœ‰å¯ç”¨ç«™ç‚¹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
                all_available_sites = []

                # 1. é¦–å…ˆæŒ‰é…ç½®çš„ä¼˜å…ˆçº§é¡ºåºæ·»åŠ ä¼˜å…ˆçº§ç«™ç‚¹
                for priority_site in source_sites_priority:
                    # è·³è¿‡è¢«æ’é™¤çš„ç«™ç‚¹
                    if priority_site in excluded_sites:
                        continue

                    source_info = db_manager.get_site_by_nickname(priority_site)
                    if not source_info or not source_info.get("cookie"):
                        continue
                    if source_info.get("migration", 0) not in [1, 3]:
                        continue

                    # æŸ¥æ‰¾è¯¥ä¼˜å…ˆçº§ç«™ç‚¹çš„ç§å­è®°å½•
                    for torrent in torrents:
                        if torrent.get("sites") == priority_site:
                            comment = torrent.get("details", "")
                            torrent_id = None
                            if comment:
                                import re

                                id_match = re.search(r"id=(\d+)", comment)
                                if id_match:
                                    torrent_id = id_match.group(1)
                                elif re.match(r"^\d+$", comment.strip()):
                                    torrent_id = comment.strip()

                            if torrent_id:
                                all_available_sites.append(
                                    {
                                        "site_name": priority_site,
                                        "site_info": source_info,
                                        "torrent_id": torrent_id,
                                        "torrent": torrent,
                                        "priority": "configured",
                                    }
                                )
                                logging.info(f"âœ“ æ·»åŠ ä¼˜å…ˆçº§ç«™ç‚¹: {priority_site}")
                                break

                # 2. ç„¶åæ·»åŠ å…¶ä»–å¯ç”¨ç«™ç‚¹ä½œä¸ºåå¤‡
                # è·å–æ‰€æœ‰åœ¨torrentsä¸­æœ‰è®°å½•çš„ç«™ç‚¹ï¼ˆæ’é™¤å·²åœ¨ä¼˜å…ˆçº§ä¸­çš„ï¼‰
                priority_site_names = set(source_sites_priority)
                site_name_map = {}
                for torrent in torrents:
                    site_name = torrent.get("sites")
                    # è·³è¿‡è¢«æ’é™¤çš„ç«™ç‚¹
                    if site_name in excluded_sites:
                        continue
                    if site_name and site_name not in priority_site_names:
                        site_name_map[site_name] = torrent

                # æŒ‰è¿ç§»çŠ¶æ€æ’åºåå¤‡ç«™ç‚¹ï¼ˆä»…å…è®¸å¯ä½œä¸ºæºçš„ç«™ç‚¹ï¼‰
                sorted_sites = []
                for site_name, torrent in site_name_map.items():
                    # è·³è¿‡è¢«æ’é™¤çš„ç«™ç‚¹
                    if site_name in excluded_sites:
                        continue

                    source_info = db_manager.get_site_by_nickname(site_name)
                    if not source_info or not source_info.get("cookie"):
                        continue
                    migration_status = source_info.get("migration", 0)
                    if migration_status not in [1, 3]:
                        continue
                    sorted_sites.append((site_name, torrent, source_info, 2))

                # æŒ‰ä¼˜å…ˆçº§é™åºæ’åº
                sorted_sites.sort(key=lambda x: x[3], reverse=True)

                # 3. å°†åå¤‡ç«™ç‚¹æ·»åŠ åˆ°å¯ç”¨ç«™ç‚¹åˆ—è¡¨
                for site_name, torrent, source_info, _ in sorted_sites:
                    # æå–ç§å­ID
                    comment = torrent.get("details", "")
                    torrent_id = None
                    if comment:
                        import re

                        id_match = re.search(r"id=(\d+)", comment)
                        if id_match:
                            torrent_id = id_match.group(1)
                        elif re.match(r"^\d+$", comment.strip()):
                            torrent_id = comment.strip()

                    if torrent_id:
                        all_available_sites.append(
                            {
                                "site_name": site_name,
                                "site_info": source_info,
                                "torrent_id": torrent_id,
                                "torrent": torrent,
                                "priority": "fallback",
                            }
                        )
                        logging.info(f"  æ·»åŠ åå¤‡ç«™ç‚¹: {site_name}")

                logging.info(
                    f"ä¸º {torrent_name} æ„å»ºå¯ç”¨ç«™ç‚¹åˆ—è¡¨å®Œæˆï¼Œå…± {len(all_available_sites)} ä¸ªç«™ç‚¹"
                )

                # éå†æ‰€æœ‰å¯ç”¨ç«™ç‚¹è¿›è¡Œå°è¯•
                for site_attempt in all_available_sites:
                    for attempt in range(1, max_retry_per_site + 1):
                        if fetch_success:
                            break

                        try:
                            site_name = site_attempt["site_name"]

                            # æ£€æŸ¥ç«™ç‚¹è¯·æ±‚é—´éš”ï¼ˆæ‰¹é‡æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
                            if not os.getenv("BATCH_MODE") == "true":
                                if site_name in site_last_request_time:
                                    elapsed = time.time() - site_last_request_time[site_name]
                                    if elapsed < REQUEST_INTERVAL:
                                        wait_time = REQUEST_INTERVAL - elapsed
                                        logging.info(
                                            f"â° ç«™ç‚¹ {site_name} è¯·æ±‚é—´éš”æ§åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’"
                                        )
                                        time.sleep(wait_time)

                            site_last_request_time[site_name] = time.time()

                            if attempt > 1:
                                logging.info(f"ğŸ”„ ç«™ç‚¹ {site_name} ç¬¬{attempt}æ¬¡é‡è¯•")
                            else:
                                priority_indicator = (
                                    "â­" if site_attempt.get("priority") == "configured" else "ğŸ“‹"
                                )
                                logging.info(
                                    f"{priority_indicator} æ­£åœ¨ä»ç«™ç‚¹ {site_name} è·å– {torrent_name}"
                                )

                            migrator = None
                            try:
                                # åˆå§‹åŒ–TorrentMigrator
                                migrator = TorrentMigrator(
                                    source_site_info=site_attempt["site_info"],
                                    target_site_info=None,
                                    search_term=site_attempt["torrent_id"],
                                    save_path=site_attempt["torrent"].get("save_path", ""),
                                    torrent_name=torrent_name,
                                    downloader_id=site_attempt["torrent"].get("downloader_id"),
                                    config_manager=config_manager,
                                    db_manager=db_manager,
                                )

                                # å°è¯•è·å–æ•°æ®
                                result = migrator.prepare_review_data()
                            finally:
                                if migrator:
                                    # æ‰¹é‡æŠ“å–åªéœ€è¦æ—¥å¿—/å‚æ•°ï¼Œä¸å¼ºåˆ¶æ¸…ç†å·²ä¸‹è½½çš„ç§å­æ–‡ä»¶
                                    migrator.cleanup(remove_temp_files=False)

                            if "review_data" in result:
                                # æˆåŠŸè·å–
                                final_source = site_attempt
                                fetch_success = True
                                logging.info(f"âœ… ä»ç«™ç‚¹ {site_name} æˆåŠŸè·å– {torrent_name}")
                                break
                            else:
                                # è·å–å¤±è´¥ï¼Œè®°å½•é”™è¯¯
                                error_detail = result.get("logs", "æœªçŸ¥é”™è¯¯")

                                # è®°å½•å°è¯•è¿‡çš„ç«™ç‚¹
                                if site_name not in attempted_sites_details:
                                    attempted_sites_details.append(site_name)

                                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                                should_retry = False
                                if attempt < max_retry_per_site:
                                    # å¯¹äºç½‘ç»œç›¸å…³é”™è¯¯å’Œç§å­é“¾æ¥æŸ¥æ‰¾é”™è¯¯ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
                                    if (
                                        "è¿æ¥" in error_detail.lower()
                                        or "timeout" in error_detail.lower()
                                        or "ç½‘ç»œ" in error_detail.lower()
                                        or "placeholder" in error_detail.lower()
                                        or "429" in error_detail
                                        or "502" in error_detail
                                        or "503" in error_detail
                                        or "504" in error_detail
                                        or "æœªæ‰¾åˆ°ç§å­ä¸‹è½½é“¾æ¥" in error_detail
                                    ):  # æ–°å¢ï¼šç§å­ä¸‹è½½é“¾æ¥æœªæ‰¾åˆ°é”™è¯¯

                                        wait_time = REQUEST_INTERVAL * (
                                            2 ** (attempt - 1)
                                        )  # æŒ‡æ•°é€€é¿
                                        logging.warning(
                                            f"âš ï¸ ç«™ç‚¹ {site_name} ç¬¬{attempt}æ¬¡å¤±è´¥ ({error_detail})ï¼Œ{wait_time}ç§’åé‡è¯•"
                                        )
                                        time.sleep(wait_time)
                                        should_retry = True
                                    else:
                                        logging.warning(
                                            f"âŒ ç«™ç‚¹ {site_name} è·å–å¤±è´¥ï¼ˆéé‡è¯•é”™è¯¯ï¼‰: {error_detail}"
                                        )

                                if not should_retry:
                                    logging.info(f"â­ï¸ ç«™ç‚¹ {site_name} è·å–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«™ç‚¹")
                                    break

                        except Exception as attempt_error:
                            error_msg = str(attempt_error)
                            logging.error(
                                f"ç«™ç‚¹ {site_attempt['site_name']} ç¬¬{attempt}æ¬¡å°è¯•å¼‚å¸¸: {error_msg}"
                            )

                            # è®°å½•å°è¯•è¿‡çš„ç«™ç‚¹
                            if site_attempt["site_name"] not in attempted_sites_details:
                                attempted_sites_details.append(site_attempt["site_name"])

                            # å¯¹äºç½‘ç»œå¼‚å¸¸ï¼Œå¦‚æœè¿˜æ²¡åˆ°é‡è¯•ä¸Šé™åˆ™é‡è¯•
                            if attempt < max_retry_per_site and (
                                "è¿æ¥" in error_msg.lower() or "timeout" in error_msg.lower()
                            ):
                                wait_time = REQUEST_INTERVAL * (2 ** (attempt - 1))
                                logging.warning(
                                    f"âš ï¸ ç«™ç‚¹ {site_attempt['site_name']} ç¬¬{attempt}æ¬¡å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•"
                                )
                                time.sleep(wait_time)
                            else:
                                logging.info(
                                    f"â­ï¸ ç«™ç‚¹ {site_attempt['site_name']} å¼‚å¸¸ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«™ç‚¹"
                                )
                                break

                    if fetch_success:
                        break  # æˆåŠŸè·å–ï¼Œé€€å‡ºç«™ç‚¹å¾ªç¯

                # å¤„ç†æœ€ç»ˆç»“æœ
                if fetch_success and final_source:
                    BATCH_FETCH_TASKS[task_id]["results"].append(
                        {
                            "name": torrent_name,
                            "status": "success",
                            "source_site": final_source["site_name"],
                            "attempted_sites": len(attempted_sites_details),
                            "retries": max_retry_per_site,
                        }
                    )
                    BATCH_FETCH_TASKS[task_id]["success"] += 1
                    logging.info(
                        f"ğŸ“Š {torrent_name} æ‰¹é‡è·å–æˆåŠŸ (å°è¯•äº†{len(attempted_sites_details)}ä¸ªç«™ç‚¹ï¼Œæ¥è‡ª{final_source['site_name']})"
                    )
                else:
                    failure_reason = f"åœ¨{len(attempted_sites_details)}ä¸ªç«™ç‚¹å…¨éƒ¨å°è¯•å¤±è´¥"
                    if attempted_sites_details:
                        failure_reason += f" (å°è¯•ç«™ç‚¹: {', '.join(attempted_sites_details)})"

                    BATCH_FETCH_TASKS[task_id]["results"].append(
                        {
                            "name": torrent_name,
                            "status": "failed",
                            "reason": failure_reason,
                            "attempted_sites": len(attempted_sites_details),
                        }
                    )
                    BATCH_FETCH_TASKS[task_id]["failed"] += 1
                    logging.error(f"âŒ {torrent_name} æ‰¹é‡è·å–å¤±è´¥: {failure_reason}")

                BATCH_FETCH_TASKS[task_id]["processed"] += 1

            except Exception as e:
                BATCH_FETCH_TASKS[task_id]["results"].append(
                    {"name": torrent_name, "status": "failed", "reason": str(e)}
                )
                BATCH_FETCH_TASKS[task_id]["failed"] += 1
                BATCH_FETCH_TASKS[task_id]["processed"] += 1
                logging.error(f"å¤„ç†ç§å­ {torrent_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

        # æ ‡è®°ä»»åŠ¡å®Œæˆ
        if task_id in BATCH_FETCH_TASKS:
            BATCH_FETCH_TASKS[task_id]["isRunning"] = False
            logging.info(f"æ‰¹é‡è·å–ä»»åŠ¡ {task_id} å®Œæˆ")

    except Exception as e:
        logging.error(f"æ‰¹é‡è·å–ä»»åŠ¡ {task_id} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        if task_id in BATCH_FETCH_TASKS:
            BATCH_FETCH_TASKS[task_id]["isRunning"] = False


@migrate_bp.route("/migrate/batch_fetch_progress", methods=["GET"])
def batch_fetch_progress():
    """è·å–æ‰¹é‡è·å–ä»»åŠ¡çš„è¿›åº¦"""
    try:
        task_id = request.args.get("task_id")

        if not task_id:
            return jsonify({"success": False, "message": "ç¼ºå°‘task_idå‚æ•°"}), 400

        if task_id not in BATCH_FETCH_TASKS:
            return jsonify({"success": False, "message": "ä»»åŠ¡ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"}), 404

        progress = BATCH_FETCH_TASKS[task_id]

        return jsonify({"success": True, "progress": progress})

    except Exception as e:
        logging.error(f"batch_fetch_progress å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


# ===================================================================
#                    å®æ—¶æ—¥å¿—æµ API (SSE)
# ===================================================================


@migrate_bp.route("/migrate/logs/stream/<task_id>", methods=["GET"])
def stream_logs(task_id):
    """å®æ—¶æ¨é€ä»»åŠ¡æ—¥å¿—æµ (Server-Sent Events)

    å‰ç«¯é€šè¿‡ EventSource è¿æ¥æ­¤ç«¯ç‚¹ï¼Œæ¥æ”¶å®æ—¶æ—¥å¿—äº‹ä»¶
    æ¯ä¸ªäº‹ä»¶åŒ…å«ï¼šstepï¼ˆæ­¥éª¤åï¼‰ã€messageï¼ˆæ¶ˆæ¯ï¼‰ã€statusï¼ˆçŠ¶æ€ï¼‰ç­‰ä¿¡æ¯
    """

    def generate():
        """ç”Ÿæˆ SSE äº‹ä»¶æµ"""
        try:
            # è·å–æˆ–åˆ›å»ºæ—¥å¿—æµ
            stream = log_streamer.get_stream(task_id)
            if not stream:
                # å¦‚æœæµä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
                stream = log_streamer.create_stream(task_id)
                logging.info(f"ä¸ºä»»åŠ¡ {task_id} åˆ›å»ºæ–°çš„æ—¥å¿—æµ")

            # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯
            yield f"data: {json.dumps({'type': 'connected', 'task_id': task_id})}\n\n"

            # æŒç»­ä»é˜Ÿåˆ—è¯»å–æ—¥å¿—äº‹ä»¶
            while True:
                try:
                    # ç­‰å¾…æ–°çš„æ—¥å¿—äº‹ä»¶ï¼ˆè¶…æ—¶1ç§’ï¼‰
                    event = stream.get(timeout=1.0)

                    # None è¡¨ç¤ºæµç»“æŸ
                    if event is None:
                        yield f"data: {json.dumps({'type': 'complete'})}\n\n"
                        logging.info(f"ä»»åŠ¡ {task_id} æ—¥å¿—æµç»“æŸ")
                        break

                    # å‘é€æ—¥å¿—äº‹ä»¶
                    event["type"] = "log"
                    yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"

                except Exception as queue_error:
                    # é˜Ÿåˆ—è¶…æ—¶æˆ–å…¶ä»–é”™è¯¯ï¼Œå‘é€å¿ƒè·³ä¿æŒè¿æ¥
                    if "Empty" in str(type(queue_error).__name__):
                        # å‘é€å¿ƒè·³
                        yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                    else:
                        logging.error(f"é˜Ÿåˆ—è¯»å–é”™è¯¯: {queue_error}")
                        break

        except Exception as e:
            logging.error(f"SSEæµç”Ÿæˆé”™è¯¯: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    # è¿”å› SSE å“åº”
    return Response(
        stream_with_context(generate()),
        mimetype="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
            "Connection": "keep-alive",
        },
    )


@migrate_bp.route("/migrate/bdinfo_status/<seed_id>")
def get_bdinfo_status(seed_id):
    """è·å– BDInfo å¤„ç†çŠ¶æ€"""
    try:
        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        # ä»æ•°æ®åº“æŸ¥è¯¢åŸºæœ¬ä¿¡æ¯
        conn = migrate_bp.db_manager._get_connection()
        cursor = migrate_bp.db_manager._get_cursor(conn)

        # seed_id æ ¼å¼ä¸º "hash_torrentId_siteName"ï¼Œéœ€è¦è§£æ
        if "_" in seed_id:
            # è§£æå¤åˆ seed_id
            parts = seed_id.split("_")
            if len(parts) >= 3:
                # æœ€åä¸€ä¸ªéƒ¨åˆ†æ˜¯ site_nameï¼Œä¸­é—´æ˜¯ torrent_idï¼Œå‰é¢æ˜¯ hash
                site_name_val = parts[-1]
                torrent_id_val = parts[-2]
                hash_val = "_".join(parts[:-2])  # hash å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿

                # ä½¿ç”¨å¤åˆä¸»é”®æŸ¥è¯¢
                if migrate_bp.db_manager.db_type == "sqlite":
                    cursor.execute(
                        """
                        SELECT mediainfo_status, bdinfo_task_id, bdinfo_started_at, 
                               bdinfo_completed_at, mediainfo, bdinfo_error 
                        FROM seed_parameters 
                        WHERE hash = ? AND torrent_id = ? AND site_name = ?
                    """,
                        (hash_val, torrent_id_val, site_name_val),
                    )
                else:
                    cursor.execute(
                        """
                        SELECT mediainfo_status, bdinfo_task_id, bdinfo_started_at, 
                               bdinfo_completed_at, mediainfo, bdinfo_error 
                        FROM seed_parameters 
                        WHERE hash = %s AND torrent_id = %s AND site_name = %s
                    """,
                        (hash_val, torrent_id_val, site_name_val),
                    )
            else:
                # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œå°è¯•ä½¿ç”¨ CONCAT æŸ¥è¯¢
                if migrate_bp.db_manager.db_type == "sqlite":
                    cursor.execute(
                        """
                        SELECT mediainfo_status, bdinfo_task_id, bdinfo_started_at, 
                               bdinfo_completed_at, mediainfo, bdinfo_error 
                        FROM seed_parameters 
                        WHERE hash || '_' || torrent_id || '_' || site_name = ?
                    """,
                        (seed_id,),
                    )
                else:
                    cursor.execute(
                        """
                        SELECT mediainfo_status, bdinfo_task_id, bdinfo_started_at, 
                               bdinfo_completed_at, mediainfo, bdinfo_error 
                        FROM seed_parameters 
                        WHERE CONCAT(hash, '_', torrent_id, '_', site_name) = %s
                    """,
                        (seed_id,),
                    )
        else:
            # å¦‚æœæ²¡æœ‰ä¸‹åˆ’çº¿ï¼Œè¯´æ˜æ ¼å¼ä¸å¯¹ï¼Œè¿”å›é”™è¯¯
            return jsonify({"error": f"æ— æ•ˆçš„ç§å­IDæ ¼å¼: {seed_id}"}), 400

        result = cursor.fetchone()
        cursor.close()
        conn.close()

        if not result:
            return jsonify({"error": "ç§å­æ•°æ®ä¸å­˜åœ¨"}), 404

        # å¦‚æœæœ‰ä»»åŠ¡IDï¼Œä»ä»»åŠ¡ç®¡ç†å™¨è·å–è¯¦ç»†çŠ¶æ€
        task_status = None
        progress_info = None
        if result["bdinfo_task_id"]:
            bdinfo_manager = get_bdinfo_manager()
            task_status = bdinfo_manager.get_task_status(result["bdinfo_task_id"])

            # å¦‚æœä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­ï¼Œè·å–è¿›åº¦ä¿¡æ¯
            if task_status and task_status.get("status") in ["processing_bdinfo", "processing"]:
                progress_info = {
                    "progress_percent": task_status.get("progress_percent", 0.0),
                    "current_file": task_status.get("current_file", ""),
                    "elapsed_time": task_status.get("elapsed_time", ""),
                    "remaining_time": task_status.get("remaining_time", ""),
                }

        # åˆ¤æ–­æ˜¯å¦ä¸ºBDInfoå†…å®¹
        is_bdinfo = False
        if result["mediainfo"]:
            from utils.mediainfo import validate_media_info_format

            _, is_bdinfo, _, _, _, _ = validate_media_info_format(result["mediainfo"])

        response_data = {
            "seed_id": seed_id,
            "mediainfo_status": result["mediainfo_status"],
            "bdinfo_task_id": result["bdinfo_task_id"],
            "bdinfo_started_at": result["bdinfo_started_at"],
            "bdinfo_completed_at": result["bdinfo_completed_at"],
            "bdinfo_error": result["bdinfo_error"],
            "mediainfo": (
                result["mediainfo"] if result["mediainfo_status"] == "completed" else None
            ),
            "is_bdinfo": is_bdinfo,
            "task_status": task_status,
        }

        # æ·»åŠ è¿›åº¦ä¿¡æ¯
        if progress_info:
            response_data["progress_info"] = progress_info

        return jsonify(response_data)

    except Exception as e:
        logging.error(f"è·å– BDInfo çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


@migrate_bp.route("/migrate/refresh_bdinfo/<seed_id>", methods=["POST"])
def refresh_bdinfo(seed_id):
    """æ‰‹åŠ¨è§¦å‘ BDInfo é‡æ–°è·å–"""
    try:
        db_manager = migrate_bp.db_manager
        torrent_name = None

        # seed_id ç»Ÿä¸€ä½¿ç”¨å¤åˆæ ¼å¼ï¼šhash_torrentId_siteName
        if "_" not in seed_id:
            return jsonify({"error": f"æ— æ•ˆçš„ç§å­IDæ ¼å¼: {seed_id}"}), 400

        try:
            conn = db_manager._get_connection()
            cursor = db_manager._get_cursor(conn)

            parts = seed_id.split("_")
            if len(parts) >= 3:
                site_name_val = parts[-1]
                torrent_id_val = parts[-2]
                hash_val = "_".join(parts[:-2])

                if db_manager.db_type == "sqlite":
                    cursor.execute(
                        "SELECT name FROM seed_parameters WHERE hash = ? AND torrent_id = ? AND site_name = ?",
                        (hash_val, torrent_id_val, site_name_val),
                    )
                else:
                    cursor.execute(
                        "SELECT name FROM seed_parameters WHERE hash = %s AND torrent_id = %s AND site_name = %s",
                        (hash_val, torrent_id_val, site_name_val),
                    )
            else:
                # å…œåº•ï¼šç”¨æ‹¼æ¥å­—æ®µåŒ¹é…ï¼ˆç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œï¼‰
                ph = db_manager.get_placeholder()
                if db_manager.db_type == "sqlite":
                    cursor.execute(
                        f"SELECT name FROM seed_parameters WHERE hash || '_' || torrent_id || '_' || site_name = {ph}",
                        (seed_id,),
                    )
                else:
                    cursor.execute(
                        f"SELECT name FROM seed_parameters WHERE CONCAT(hash, '_', torrent_id, '_', site_name) = {ph}",
                        (seed_id,),
                    )

            row = cursor.fetchone()
            if row:
                torrent_name = row["name"] if isinstance(row, dict) else row[0]

        except Exception as e:
            logging.warning(f"é€šè¿‡å¤åˆseed_idæŸ¥è¯¢nameå¤±è´¥: {e}")
        finally:
            try:
                cursor.close()
                conn.close()
            except Exception:
                pass

        torrent_info = get_current_torrent_info(db_manager, torrent_name)
        if not torrent_info or not torrent_info.get("save_path"):
            return jsonify({"error": "æ— æ³•è·å–ä¿å­˜è·¯å¾„"}), 404

        # è°ƒç”¨åˆ·æ–°å‡½æ•°
        from utils.mediainfo import refresh_bdinfo_for_seed

        refresh_result = refresh_bdinfo_for_seed(seed_id, torrent_info["save_path"], priority=1)

        if refresh_result["success"]:
            return jsonify(refresh_result)
        else:
            return jsonify(refresh_result), 500

    except Exception as e:
        logging.error(f"åˆ·æ–° BDInfo å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


@migrate_bp.route("/migrate/bdinfo_tasks")
def get_bdinfo_tasks():
    """è·å–æ‰€æœ‰ BDInfo ä»»åŠ¡çŠ¶æ€ï¼ˆç®¡ç†å‘˜æ¥å£ï¼‰"""
    try:
        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        bdinfo_manager = get_bdinfo_manager()
        tasks = bdinfo_manager.get_all_tasks()
        stats = bdinfo_manager.get_stats()

        return jsonify({"tasks": tasks, "stats": stats})

    except Exception as e:
        logging.error(f"è·å– BDInfo ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


@migrate_bp.route("/migrate/refresh_mediainfo_async", methods=["POST"])
def refresh_mediainfo_async():
    """å¼‚æ­¥ç‰ˆæœ¬çš„ MediaInfo åˆ·æ–°æ¥å£"""
    try:
        data = request.json
        current_mediainfo = data.get("current_mediainfo", "")
        seed_id = data.get("seed_id")
        save_path = data.get("save_path")
        content_name = data.get("content_name")
        downloader_id = data.get("downloader_id")
        torrent_name = data.get("torrent_name")
        force_refresh = data.get("force_refresh", True)
        priority = data.get("priority", 1)  # é»˜è®¤é«˜ä¼˜å…ˆçº§

        if not seed_id or not save_path or seed_id == "" or seed_id == None:
            return (
                jsonify({"success": False, "message": "ç¼ºå°‘å¿…è¦å‚æ•°: seed_id æˆ– save_path"}),
                400,
            )

        # è°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬çš„ MediaInfo å¤„ç†å‡½æ•°
        from utils.mediainfo import upload_data_mediaInfo_async

        # è§£æ seed_id è·å–å¤åˆä¸»é”®ç»„ä»¶
        hash_value = torrent_id = site_name = None
        if "_" in seed_id:
            parts = seed_id.split("_")
            if len(parts) >= 3:
                site_name = parts[-1]
                torrent_id = parts[-2]
                hash_value = "_".join(parts[:-2])

        # è·å–ç«™ç‚¹ä¸­æ–‡åï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        nickname = data.get("nickname")  # ä»è¯·æ±‚ä¸­è·å–ç«™ç‚¹ä¸­æ–‡å

        new_mediainfo, is_mediainfo, is_bdinfo, bdinfo_info = upload_data_mediaInfo_async(
            mediaInfo=current_mediainfo,
            save_path=save_path,
            seed_id=seed_id,
            content_name=content_name,
            downloader_id=downloader_id,
            torrent_name=torrent_name,
            force_refresh=force_refresh,
            priority=priority,
            # æ–°å¢å‚æ•°ï¼šé¢„å†™å…¥æ‰€éœ€çš„åŸºæœ¬ä¿¡æ¯
            hash_value=hash_value,
            torrent_id=torrent_id,
            site_name=site_name,
            nickname=nickname,
        )

        # å³ä½¿ MediaInfo æå–å¤±è´¥ï¼Œå¦‚æœ BDInfo ä»»åŠ¡å·²æ·»åŠ ï¼Œä¹Ÿè¿”å›æˆåŠŸ
        if bdinfo_info["bdinfo_status"] == "processing" and bdinfo_info["bdinfo_task_id"]:
            response_data = {
                "success": True,
                "mediainfo": new_mediainfo or "",
                "is_mediainfo": is_mediainfo,
                "is_bdinfo": is_bdinfo,
                "bdinfo_async": bdinfo_info,
                "message": "BDInfo æ­£åœ¨åå°å¤„ç†ä¸­",
            }
            return jsonify(response_data), 200
        elif new_mediainfo:
            response_data = {
                "success": True,
                "mediainfo": new_mediainfo,
                "is_mediainfo": is_mediainfo,
                "is_bdinfo": is_bdinfo,
                "bdinfo_async": bdinfo_info,
                "message": "MediaInfo æ›´æ–°å®Œæˆ",
            }
            return jsonify(response_data), 200
        else:
            return jsonify({"success": False, "message": "MediaInfo æå–å¤±è´¥"}), 500

    except Exception as e:
        logging.error(f"å¼‚æ­¥ MediaInfo åˆ·æ–°å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@migrate_bp.route("/migrate/bdinfo_records", methods=["GET"])
def get_bdinfo_records():
    """è·å–BDInfoå¤„ç†è®°å½•"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        status_filter = request.args.get("status_filter", "")
        page = int(request.args.get("page", 1))
        page_size = int(request.args.get("pageSize", 20))

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = ["bdinfo_task_id IS NOT NULL"]
        params = []

        # æ·»åŠ çŠ¶æ€ç­›é€‰
        if status_filter:
            if status_filter == "processing":
                where_conditions.append("mediainfo_status IN ('processing_bdinfo', 'processing')")
            elif status_filter == "completed":
                where_conditions.append("mediainfo_status = 'completed'")
            elif status_filter == "failed":
                where_conditions.append(
                    "(mediainfo_status = 'failed' OR bdinfo_error IS NOT NULL)"
                )

        where_clause = " AND ".join(where_conditions)

        # è·å–æ•°æ®åº“ç®¡ç†å™¨
        db_manager = migrate_bp.db_manager
        conn = db_manager._get_connection()
        cursor = db_manager._get_cursor(conn)

        # è·å–æ€»æ•°
        count_sql = f"SELECT COUNT(*) as total FROM seed_parameters WHERE {where_clause}"
        cursor.execute(count_sql, params)
        total = cursor.fetchone()["total"]

        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size

        # è·å–è®°å½•
        if db_manager.db_type == "sqlite":
            records_sql = f"""
                SELECT 
                    sp.hash || '_' || sp.torrent_id || '_' || sp.site_name as seed_id,
                    sp.title,
                    sp.site_name,
                    COALESCE(s.nickname, sp.site_name) as nickname,
                    sp.mediainfo_status,
                    sp.bdinfo_task_id,
                    sp.bdinfo_started_at,
                    sp.bdinfo_completed_at,
                    sp.bdinfo_error,
                    sp.mediainfo,
                    sp.updated_at
                FROM seed_parameters sp
                LEFT JOIN sites s ON sp.site_name = s.site
                WHERE {where_clause}
                ORDER BY sp.bdinfo_started_at DESC
                LIMIT ? OFFSET ?
            """
            params.extend([page_size, offset])
        else:  # postgresql or mysql
            records_sql = f"""
                SELECT 
                    CONCAT(sp.hash, '_', sp.torrent_id, '_', sp.site_name) as seed_id,
                    sp.title,
                    sp.site_name,
                    COALESCE(s.nickname, sp.site_name) as nickname,
                    sp.mediainfo_status,
                    sp.bdinfo_task_id,
                    sp.bdinfo_started_at,
                    sp.bdinfo_completed_at,
                    sp.bdinfo_error,
                    sp.mediainfo,
                    sp.updated_at
                FROM seed_parameters sp
                LEFT JOIN sites s ON sp.site_name = s.site
                WHERE {where_clause}
                ORDER BY sp.bdinfo_started_at DESC
                LIMIT %s OFFSET %s
            """
            params.extend([page_size, offset])

        cursor.execute(records_sql, params)
        records = []

        for row in cursor.fetchall():
            # åˆ¤æ–­æ˜¯å¦ä¸ºBDInfoå†…å®¹
            is_bdinfo = False
            if row["mediainfo"]:
                from utils.mediainfo import validate_media_info_format

                _, is_bdinfo, _, _, _, _ = validate_media_info_format(row["mediainfo"])

            records.append(
                {
                    "seed_id": row["seed_id"],
                    "title": row["title"] or "æœªçŸ¥æ ‡é¢˜",
                    "site_name": row["site_name"] or "æœªçŸ¥ç«™ç‚¹",
                    "nickname": row["nickname"] or row["site_name"] or "æœªçŸ¥ç«™ç‚¹",
                    "mediainfo_status": row["mediainfo_status"] or "unknown",
                    "bdinfo_task_id": row["bdinfo_task_id"],
                    "bdinfo_started_at": row["bdinfo_started_at"],
                    "bdinfo_completed_at": row["bdinfo_completed_at"],
                    "bdinfo_error": row["bdinfo_error"],
                    "mediainfo": row["mediainfo"],
                    "is_bdinfo": is_bdinfo,
                }
            )

        cursor.close()
        conn.close()

        return jsonify(
            {"success": True, "data": records, "total": total, "page": page, "pageSize": page_size}
        )

    except Exception as e:
        logging.error(f"è·å–BDInfoè®°å½•å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}"}), 500


@migrate_bp.route("/migrate/bdinfo_sse/<seed_id>")
def bdinfo_sse(seed_id):
    """BDInfoè¿›åº¦æ›´æ–°çš„SSEç«¯ç‚¹"""
    try:
        # ç”Ÿæˆå”¯ä¸€çš„è¿æ¥ID
        connection_id = str(uuid.uuid4())

        # å¯¼å…¥SSEå“åº”ç”Ÿæˆå™¨
        from utils.sse_manager import generate_sse_response

        # è¿”å›SSEå“åº”æµ
        return generate_sse_response(connection_id, seed_id)

    except Exception as e:
        logging.error(f"åˆ›å»ºSSEè¿æ¥å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


@migrate_bp.route("/migrate/bdinfo/progress", methods=["POST"])
def bdinfo_progress_callback():
    """æ¥æ”¶è¿œç¨‹BDInfoè¿›åº¦å›ä¼ """
    try:
        data = request.json
        task_id = data.get("task_id")
        progress_percent = data.get("progress_percent", 0)
        current_file = data.get("current_file", "")
        elapsed_time = data.get("elapsed_time", "")
        remaining_time = data.get("remaining_time", "")

        if not task_id:
            return jsonify({"success": False, "message": "ç¼ºå°‘ task_id å‚æ•°"}), 400

        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        bdinfo_manager = get_bdinfo_manager()

        # ä½¿ç”¨æ–°çš„å›è°ƒå¤„ç†æ–¹æ³•
        progress_data = {
            "progress_percent": progress_percent,
            "current_file": current_file,
            "elapsed_time": elapsed_time,
            "remaining_time": remaining_time,
        }

        success = bdinfo_manager.handle_remote_progress_callback(task_id, progress_data)

        if not success:
            return jsonify({"success": False, "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"}), 404

        return jsonify({"success": True, "message": "è¿›åº¦æ›´æ–°æˆåŠŸ"})

    except Exception as e:
        logging.error(f"å¤„ç†BDInfoè¿›åº¦å›ä¼ å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@migrate_bp.route("/migrate/bdinfo/complete", methods=["POST"])
def bdinfo_complete_callback():
    """æ¥æ”¶è¿œç¨‹BDInfoå®Œæˆå›ä¼ """
    try:
        data = request.json
        task_id = data.get("task_id")
        success = data.get("success", False)
        bdinfo_content = data.get("bdinfo", "")
        error_message = data.get("error_message", "")

        if not task_id:
            return jsonify({"success": False, "message": "ç¼ºå°‘ task_id å‚æ•°"}), 400

        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        bdinfo_manager = get_bdinfo_manager()

        # ä½¿ç”¨æ–°çš„å®Œæˆå›è°ƒå¤„ç†æ–¹æ³•
        callback_success = bdinfo_manager.handle_remote_completion_callback(
            task_id, success, bdinfo_content, error_message
        )

        if not callback_success:
            return jsonify({"success": False, "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"}), 404

        return jsonify({"success": True, "message": "å®ŒæˆçŠ¶æ€æ›´æ–°æˆåŠŸ"})

    except Exception as e:
        logging.error(f"å¤„ç†BDInfoå®Œæˆå›ä¼ å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "message": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500


@migrate_bp.route("/migrate/cleanup_bdinfo_process", methods=["POST"])
def cleanup_bdinfo_process():
    """æ¸…ç† BDInfo æ®‹ç•™è¿›ç¨‹"""
    try:
        data = request.json
        seed_id = data.get("seed_id")

        if not seed_id:
            return jsonify({"error": "ç¼ºå°‘ seed_id å‚æ•°"}), 400

        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        bdinfo_manager = get_bdinfo_manager()

        # æŸ¥æ‰¾å¹¶æ¸…ç†å¯¹åº”çš„ä»»åŠ¡
        cleaned = False
        with bdinfo_manager.lock:
            for task_id, task in bdinfo_manager.tasks.items():
                if task.seed_id == seed_id and task.status == "processing_bdinfo":
                    bdinfo_manager._cleanup_process(task)
                    cleaned = True
                    break

        if cleaned:
            return jsonify({"success": True, "message": "å·²æ¸…ç†æ®‹ç•™è¿›ç¨‹"})
        else:
            return jsonify({"success": True, "message": "æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„è¿›ç¨‹"})

    except Exception as e:
        logging.error(f"æ¸…ç† BDInfo è¿›ç¨‹å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500


@migrate_bp.route("/migrate/restart_bdinfo", methods=["POST"])
def restart_bdinfo():
    """é‡å¯å¡æ­»çš„ BDInfo ä»»åŠ¡"""
    try:
        data = request.json
        seed_id = data.get("seed_id")

        if not seed_id:
            return jsonify({"error": "ç¼ºå°‘ seed_id å‚æ•°"}), 400

        from core.bdinfo.bdinfo_manager import get_bdinfo_manager

        bdinfo_manager = get_bdinfo_manager()

        # è·å–ç§å­æ•°æ®
        conn = migrate_bp.db_manager._get_connection()
        cursor = migrate_bp.db_manager._get_cursor(conn)

        # è§£æå¤åˆ seed_idï¼Œä½¿ç”¨å®Œæ•´å¤åˆä¸»é”®æŸ¥è¯¢ä»¥ç¡®ä¿å‡†ç¡®æ€§
        hash_val = torrent_id = site_name = None
        result = None
        if "_" in seed_id:
            parts = seed_id.split("_")
            if len(parts) >= 3:
                # æœ€åä¸€ä¸ªéƒ¨åˆ†æ˜¯ site_nameï¼Œä¸­é—´æ˜¯ torrent_idï¼Œå‰é¢æ˜¯ hash
                site_name = parts[-1]
                torrent_id = parts[-2]
                hash_val = "_".join(parts[:-2])

                # ä½¿ç”¨å®Œæ•´å¤åˆä¸»é”®æŸ¥è¯¢ç§å­åç§°
                if migrate_bp.db_manager.db_type == "sqlite":
                    cursor.execute(
                        "SELECT name FROM seed_parameters WHERE hash = ? AND torrent_id = ? AND site_name = ?",
                        (hash_val, torrent_id, site_name),
                    )
                else:
                    cursor.execute(
                        "SELECT name FROM seed_parameters WHERE hash = %s AND torrent_id = %s AND site_name = %s",
                        (hash_val, torrent_id, site_name),
                    )
                result = cursor.fetchone()
            else:
                # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œå°è¯•ä½¿ç”¨ CONCAT æŸ¥è¯¢
                if migrate_bp.db_manager.db_type == "sqlite":
                    cursor.execute(
                        "SELECT hash, torrent_id, site_name, name FROM seed_parameters WHERE hash || '_' || torrent_id || '_' || site_name = ?",
                        (seed_id,),
                    )
                else:
                    cursor.execute(
                        "SELECT hash, torrent_id, site_name, name FROM seed_parameters WHERE CONCAT(hash, '_', torrent_id, '_', site_name) = %s",
                        (seed_id,),
                    )
                result = cursor.fetchone()
        else:
            return jsonify({"error": "æ— æ•ˆçš„ seed_id æ ¼å¼"}), 400

        cursor.close()
        conn.close()

        if not result:
            return jsonify({"error": "ç§å­æ•°æ®ä¸å­˜åœ¨"}), 404

        if isinstance(result, dict):
            if result.get("hash"):
                hash_val = result.get("hash")
                torrent_id = result.get("torrent_id")
                site_name = result.get("site_name")
            torrent_name = result.get("name")
        else:
            if len(result) >= 4:
                hash_val = result[0]
                torrent_id = result[1]
                site_name = result[2]
                torrent_name = result[3]
            else:
                torrent_name = result[0] if len(result) > 0 else None

        torrent_info = get_current_torrent_info(migrate_bp.db_manager, torrent_name)
        save_path = torrent_info.get("save_path") if torrent_info else None
        downloader_id = torrent_info.get("downloader_id") if torrent_info else None
        if not torrent_name and torrent_info:
            torrent_name = torrent_info.get("name")

        if not save_path:
            return jsonify({"error": "æ— æ³•è·å–ä¿å­˜è·¯å¾„"}), 404

        # 1. æ¸…ç†å¯èƒ½çš„æ®‹ç•™è¿›ç¨‹
        bdinfo_manager.cleanup_orphaned_process(seed_id)

        # 2. é‡ç½®æ•°æ®åº“çŠ¶æ€
        bdinfo_manager.reset_task_status(seed_id)

        # 3. æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœæœ‰torrent_nameï¼‰
        actual_save_path = save_path
        if torrent_name and save_path:
            # æ£€æŸ¥save_pathæ˜¯å¦å·²ç»åŒ…å«äº†torrent_name
            if not save_path.endswith(torrent_name):
                actual_save_path = os.path.join(save_path, torrent_name)
                logging.info(
                    f"é‡å¯BDInfoä»»åŠ¡æ„å»ºå®Œæ•´è·¯å¾„: {save_path} + {torrent_name} -> {actual_save_path}"
                )

        # 4. åº”ç”¨è·¯å¾„æ˜ å°„ï¼ˆå¦‚æœæœ‰ä¸‹è½½å™¨IDï¼‰
        if downloader_id:
            try:
                from utils.mediainfo import translate_path

                mapped_path = translate_path(downloader_id, actual_save_path)
                if mapped_path != actual_save_path:
                    logging.info(
                        f"é‡å¯BDInfoä»»åŠ¡åº”ç”¨è·¯å¾„æ˜ å°„: {actual_save_path} -> {mapped_path}"
                    )
                    actual_save_path = mapped_path
            except Exception as e:
                logging.warning(f"è·¯å¾„æ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è·¯å¾„: {e}")
        # 5. é‡æ–°æ·»åŠ ä»»åŠ¡
        task_id = bdinfo_manager.add_task(
            seed_id=seed_id,
            save_path=actual_save_path,
            priority=1,
            downloader_id=downloader_id,  # é«˜ä¼˜å…ˆçº§ï¼Œä¼ é€’ä¸‹è½½å™¨IDï¼ˆå¯èƒ½ä¸ºNoneï¼‰
        )

        return jsonify({"success": True, "task_id": task_id, "message": "BDInfo ä»»åŠ¡å·²é‡å¯"})

    except Exception as e:
        logging.error(f"é‡å¯ BDInfo ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return jsonify({"error": str(e)}), 500
